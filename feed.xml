<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-03-31T18:39:04-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">Galerkin neural networks for approximating PDEs</title><link href="http://localhost:4000/blog/2023/galerkinNNs/" rel="alternate" type="text/html" title="Galerkin neural networks for approximating PDEs" /><published>2023-03-22T11:12:00-04:00</published><updated>2023-03-22T11:12:00-04:00</updated><id>http://localhost:4000/blog/2023/galerkinNNs</id><content type="html" xml:base="http://localhost:4000/blog/2023/galerkinNNs/"><![CDATA[<h1 id="1-introduction"><strong>1. Introduction</strong></h1>

<p>Neural networks have seen much success in computer vision over the last decade. Only more recently has there been an increase in interest in using them for tasks that have traditionally been in the purview of computational scientists and engineers. The one task I’d like to discuss today is approximating solutions of PDEs. To my knowledge, the first use of neural nets for this task was by Dissanayake and Phan-Thien in <a href="https://doi.org/10.1002/cnm.1640100303">[1]</a>. They proposed learning solutions by minimizing the interior and boundary residuals associated with the PDE measured in the \(L^{2}\) norm.</p>

<p>As computational power and optimization techniques have improved, neural networks have only grown more in popularity. For solving the forward PDE problem, most works in the literature today are similar to the approach in <a href="https://doi.org/10.1002/cnm.1640100303">[1]</a>. It is also worth noting that neural networks offer clear advantages for high-dimensional PDEs and problems for which one would like to incorporate measured data alongside a physics-based model, see for instance <a href="https://www.brown.edu/research/projects/crunch/sites/brown.edu.research.projects.crunch/files/uploads/Physics-informed%20neural%20networks_A%20deep%20learning%20framwork%20fir%20solving%20forward%20and%20inverse%20probelms%20involving%20nonlinear%20partial%20differential%20equations.pdf">[2]</a>.</p>

<p>However, in practice there are a number of issues that arise when using neural networks to approximate PDEs. The most glaring is that while neural networks are <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">universal approximators</a>, their associated objective functions are highly nonconvex and one often observes that the approximation error stagnates after a certain point irrespective of how many layers or neurons are added to the network. This is in sharp contrast with traditional methods such as finite elements which have theoretical guarantees on the error that are realized in practice.</p>

<p>Our goal when developing Galerkin Neural Networks was to have an approach that allows for rigorous control of the approximation error. Leveraging techniques from functional analysis, we train a sequence of shallow networks which learn <a href="https://en.wikipedia.org/wiki/Riesz_representation_theorem">Riesz representers</a> of weak residuals of the PDE. These Riesz representers form a finite-dimensional subspace from which to approximate the PDE. The result is a method that I believe stands apart quite distinctly from other approaches in the literature. The full analysis and results may be found in my paper <a href="https://arxiv.org/abs/2105.14094">[3]</a> (co-authored with my advisor Mark Ainsworth), but this post serves as an accessible introduction with accompanying code.</p>

<h1 id="2-feedforward-neural-networks"><strong>2. Feedforward Neural Networks</strong></h1>

<p>Mathematically, a feedforward neural network is best described as a linear combination of parameterized nonlinear functions. To be precise:</p>

<p><strong>Definition 1.</strong> <em>Given an input \(x \in \mathbb{R}^{d}\), the <strong>feedforward neural network with \(\ell\) hidden layers and widths \(n_{0}, \dots, n_{\ell}\)</strong> is described by the sequence of linear transformations</em></p>

\[\begin{align}
  T_{i}(x) = \begin{cases} x\cdot W^{(i)} + b^{(i)}, &amp;i=1,\dots,\ell-1\\
  x\cdot c, &amp;i=\ell
  \end{cases} \notag
\end{align}\]

<p><em>and the function \(u_{NN} : \mathbb{R}^{d} \to \mathbb{R}\) given by</em></p>

\[\begin{align}
  u_{NN}(x;\theta) := T_{\ell} \circ (\sigma \circ T_{\ell-1} \circ \dots \sigma \circ T_{i} \circ \dots \sigma \circ T_{1})(x),
\end{align}\]

<p><em>where \(W^{(i)} \in \mathbb{R}^{n_{i-1}\times n_{i}}\), \(b^{(i)} \in \mathbb{R}^{1\times n_{i}}\), \(c \in \mathbb{R}^{n_{\ell}}\), and \(\sigma : \mathbb{R} \to \mathbb{R}\).</em></p>

<p>In the standard nomenclature, \(W^{(i)}\) and \(b^{(i)}\) are the <strong>weights</strong> and <strong>biases</strong>, respectively, of the \(\ell\)th hidden layer; \(c\) are the linear or <strong>activation coefficients</strong>; and \(\sigma\) is the <strong>activation function</strong>. Often \(\theta\) is used to denote all of the network parameters \((W^{(i)}, b^{(i)})\) and \(c\).</p>

<div class="container">
  <div class="row">
    <div class="col">
    </div>
    <div class="col-9">
      <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gnn/featured-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gnn/featured-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gnn/featured-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/gnn/featured.png" class="img rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

      <div class="caption">
        <b>Figure 1:</b> Visualization of a feedforward neural network with 4 hidden layers and 8 neurons per layer. The lines between nodes (neurons) represent the weights of the linear transformation connecting one layer to the next.
      </div> 
    </div>
    <div class="col">
    </div>
  </div>
</div>

<p>As an example, for \(\ell=1\) we can write \(u_{NN}\) as</p>

\[u_{NN}(x;\theta) = \sum_{i=1}^{n} c_{i}\sigma(x\cdot W^{(1)}_{i} + b^{(1)}_{i})\]

<p>which allows us to more easily see that this is really just a linear combination of nonlinear functions. Some common choices of \(\sigma\) are the ReLU function \(t \mapsto \max\{0,t\}\), the hyperbolic tangent function, and the sigmoid function \(t \mapsto 1/(1+\exp(-t))\). For our work, we only use the hyperbolic tangent function.</p>

<p>The goal of a neural network is to determine, or “learn”, the parameters \(\theta\) such that \(u_{NN}(\cdot;\theta)\) is a good approximation to some given target function \(f\). For instance, suppose we simply want to fit a feedforward neural network to \(f\). A common approach is to compute</p>

\[\begin{align}
\theta^{*} = \text{arg min}_{\theta} ||f-u_{NN}(\cdot;\theta)||_{L^{2}}^{2} \label{eq:objective fn}
\end{align}\]

<p>in order to learn the optimal parameters \(\theta^{*}\). A problem such as \eqref{eq:objective fn} is typically approximated using a first-order method, such as <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">stochastic gradient descent</a>, or a second-order method, such as <a href="https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm">BFGS</a>. We’ll discuss some particular optimization methods for training neural networks later in this post.</p>

<h2 id="ii-approximation-of-pdes-via-neural-networks"><strong>II. Approximation of PDEs via Neural Networks</strong></h2>

<p>In <a href="https://doi.org/10.1002/cnm.1640100303">[1]</a>, the authors consider a PDE of the form</p>

\[\begin{align}
  \begin{cases}
    \mathcal{L}[u] = f, &amp;\text{in}\;\Omega\\
    \mathcal{B}[u] = g, &amp;\text{on}\;\partial\Omega,
  \end{cases} \notag
\end{align}\]

<p>where \(\mathcal{L}\) and \(\mathcal{B}\) are differential operators. They propose the minimization problem</p>

\[\begin{align}
  \min_{\theta} ||\mathcal{L}[u_{NN}(\cdot;\theta)] - f||_{L^{2}(\Omega)}^{2} + ||\mathcal{B}[u_{NN}(\cdot;\theta)] - g||_{L^{2}(\partial\Omega)}^{2} \label{eq:dissanayake objective}
\end{align}\]

<p>in which one seeks to minimize both the interior and boundary residuals evaluated at a discrete set of points in the domain. A similar approach is followed in <a href="https://www.brown.edu/research/projects/crunch/sites/brown.edu.research.projects.crunch/files/uploads/Physics-informed%20neural%20networks_A%20deep%20learning%20framwork%20fir%20solving%20forward%20and%20inverse%20probelms%20involving%20nonlinear%20partial%20differential%20equations.pdf">[2]</a>.</p>

<p>There are a few things worth noting before we continue.</p>
<ol>
  <li>
    <p>The optimization problem \eqref{eq:dissanayake objective} only makes sense if \(f \in L^{2}(\Omega)\) and \(g \in L^{2}(\partial\Omega)\). There are many instances where we might have \(f \in H^{-1}(\Omega)\), for example \(f = \delta_{0}\) where \(\delta_{0}\) is the Dirac-Delta distribution. In this case, a pointwise interpretation of the residuals does not make sense.</p>
  </li>
  <li>
    <p>Despite neural networks having excellent approximation power in theory, their approximation error often stagnates after a certain point and increasing network width and/or depth does not typically solve this issue. This stagnation can be due to stalling in local minima, which has been written about in the context of function approximation here <a href="https://epubs.siam.org/doi/pdf/10.1137/20M1353010?casa_token=nNt0YvwU13AAAAAA:ly4stGiO85ibuuoN0wP2fRlC5vXBOk2iqCfOd0qF_q2nvK0Hrat6ZQH58P7WA1VMxmg0HZl9ifRz">[4]</a>, but in general, one can’t expect to specify a target approximation error – say, maybe we want our \(L^{2}\) error to satisfy \(\|f - u_{NN}(\cdot;\theta)\|_{L^{2}} &lt; \mathcal{O}(10^{-4})\) – and expect to to be able to achieve this. This is in contrast to methods such as finite elements where, provided a stable discretization is chosen, machine precision error can be reached even if getting there is computationally infeasible.</p>
  </li>
</ol>

<p><strong>Example 1.</strong> To demonstrate the second point above, we attempt to learn the solution to the differential equation \(-u''(x) = f(x)\) with homogeneous Dirichlet boundary condition (i.e. \(u(0) = u(1) = 0\)). The true solution is taken to be</p>

\[u(x) = \sin(2\pi x) + \sin(4\pi x) + \sin(6\pi x)\]

<p>and \(f\) is chosen to satisfy this true solution. The loss history over each epoch is provided in Figure 2 alongside a table of parameters for the implementation.</p>

<div class="container">
  <div class="row">
    <div class="col-6">
      <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gnn/loss_ex1-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gnn/loss_ex1-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gnn/loss_ex1-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/gnn/loss_ex1.png" class="img rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-6">
      <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gnn/L2_ex1-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gnn/L2_ex1-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gnn/L2_ex1-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/gnn/L2_ex1.png" class="img rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
  </div>
</div>

<p>\(\small
\begin{array}{c|c|c|c|c|c|c|c}
\hline
  \textbf{Parameter} &amp; \text{depth} &amp; \text{width} &amp; \text{activation} &amp; \text{optimizer} &amp; \text{learn rate} &amp; \text{training set} &amp; \text{batch size} \\ 
\hline
   \textbf{Value} &amp; \text{2} &amp; \text{150, 150} &amp; \tanh &amp; \text{Adam} &amp; 10^{-5} &amp; \text{20000} &amp; \text{50} \\
\hline
\end{array}\)</p>
<div class="caption">
  <b>Figure 2:</b> Loss history for example 1 (top). Parameters for the neural network and training (bottom).
</div>

<p>We can repeat this example with deeper or wider networks and find that while initially, such architectures might yield an increase in accuracy, they eventually won’t decrease the approximation error further. In fact, haphazardly using very deep networks can yield worse results due to difficulty in training.</p>

<p>Before I talk about Galerkin neural networks and how they attempt to skirt around this issue, we’ll need to (briefly) cover variational formulations and weak solutions of PDEs.</p>

<h1 id="3-overview-of-variational-formulations-for-pdes"><strong>3. Overview of Variational Formulations for PDEs</strong></h1>

<p>We’ll start out with a motivating example here. Let \(X\) be a general Hilbert space (we’ll be more concrete in a second, but \(X\) is just a space of functions with some restrictions on their behavior). Let’s multiply the equation from Example 1 (\(-u''(x) = f(x)\)) by a function \(v \in X\) and integrate both sides over the interval \((0,1)\):</p>

\[-\int_{0}^{1} u''(x)v(x)\;dx = \int_{0}^{1} f(x)v(x)\;dx.\]

<p>The integral on the left-hand side can be integrated by parts to obtain</p>

\[\begin{align}
  \int_{0}^{1} u'(x)v'(x)\;dx - u'(1)v(1) + u'(0)v(0) = \int_{0}^{1} f(x)v(x)\;dx.\label{eq:weak form ex1}
\end{align}\]

<p>The Hilbert space \(X\) is typically chosen according to the highest order of derivatives appearing in \eqref{eq:weak form ex1} as well as the boundary conditions for the equation. In this case, we’ll work with the Sobolev space \(H^{1}_{0}\):</p>

\[H^{1}_{0}(\Omega) := \left\{ v \;:\; \int_{\Omega} |D^{\alpha}v|^{2}\;dx &lt; \infty, \;|\alpha| \leqslant 1, \;v|_{\partial\Omega} = 0\right\}.\]

<p>The space \(H^{1}_{0}\) consists of function whose derivatives up to first-order are square integrable and whose value on the boundary of the domain is zero. We note that if \(v \in H^{1}_{0}\), then the boundary terms in \eqref{eq:weak form ex1} will vanish and we’re left with</p>

\[\int_{0}^{1} u'(x)v'(x)\;dx = \int_{0}^{1} f(x)v(x)\;dx.\]

<p>The \(H^{1}_{0}\) variational or weak formulation of Example 1 is to seek a solution \(u \in H^{1}_{0}(0,1)\) such that
\(\begin{align}
  \int_{0}^{1} u'(x)v'(x)\;dx = \int_{0}^{1} f(x)v(x)\;dx \;\;\;\forall v \in H^{1}_{0}(0,1).\label{eq:poisson1d weak}
\end{align}\)</p>

<p>If you haven’t taken a course on PDEs before \eqref{eq:poisson1d weak} will probably be quite new to you, so I want to point out a few key observations.</p>

<ol>
  <li>
    <p>Any \(u\) which satisfies \(-u''(x) = f(x)\) for \(x \in (0,1)\) and \(u(0)=u(1)=0\) will automatically satisfy \eqref{eq:poisson1d weak}. This is clear by following the derivation steps above. So what does this mean? If the data \(f\) is smooth enough, it means that the solution of the original equation and the solution of \eqref{eq:poisson1d weak} coincide.</p>
  </li>
  <li>
    <p>\eqref{eq:poisson1d weak} only involves first derivatives of \(u\) and \(v\) while our original equation involved second derivatives. Since requiring the existence of second derivatives is a stronger condition, we can surmise that solutions of \eqref{eq:poisson1d weak} are “weaker” than solutions of the original equation, hence where the term <em>weak solution</em> comes from. For most cases, when the data \(f\) is smooth enough (and in two dimensions and higher, when the domain is regular enough), this weakening of the required smoothness of the solutions is irrelevant. However, there will be genuine cases that occur in practice, especially in solid and fluid mechanics, where solutions contain discontinuities, boundary layers, and sharp gradients which reduce regularity for which weak formulations such as \eqref{eq:poisson1d weak} are necessary. We’ll touch more on this later.</p>
  </li>
  <li>
    <p>Rather than requiring an equation to be satisfied for every value of \(x\), \eqref{eq:poisson1d weak} only considers <em>averaged</em> quantities. That is, as long as the integrals in \eqref{eq:poisson1d weak} are finite, its solution is well-defined. This is in contrast to the original equation (whose solutions we’ll refer to as <em>classic solutions</em>) which requires \(u''(x)\) to be defined for all \(x\) in the domain in order for a solution to make sense.</p>
  </li>
</ol>

<p>Even when it’s not strictly necessary to use a weak/variational formulation like \eqref{eq:poisson1d weak}, numerical methods such as finite elements work with the variational formulation since its approximation is usually equivalent to solving a sparse linear system of equations. As well, there are a number of mathematical tools that allow for proving nice error estimates when working with the weak formulation.</p>

<h2 id="i-approximating-weak-solutions"><strong>I. Approximating Weak Solutions</strong></h2>

<p>With all that out of the way, how do we actually solve a variational equation like \eqref{eq:poisson1d weak}? To generalize things a bit more nicely, let’s consider an abstract variational problem on a Hilbert space \(X\):</p>

\[\begin{align}
  u \in X \;:\; a(u,v) = L(v) \;\;\;\forall v \in X.\label{eq:variational}
\end{align}\]

<p>Here, \(a : X \times X \to \mathbb{R}\) is a bilinear operator on \(X\) and \(L : X \to \mathbb{R}\) is a linear operator on \(X\). In the previous example, we would have \(a(u,v) := \int_{0}^{1} u'(x)v'(x)\;dx\) and \(L(v) := \int_{0}^{1} f(x)v(x)\;dx\). Typically, \eqref{eq:variational} can’t be solved exactly and we’ll have to seek an approximate solution.</p>

<p><strong>Definition (Galerkin method).</strong> Given a finite-dimensional subspace \(X_{h} \subset X\), the Galerkin method seeks an approximate solution to \eqref{eq:variational} which is an orthogonal projection (with respect to the bilinear operator \(a(\cdot,\cdot)\)) of the true solution \(u\) onto \(X_{h}\). This is equivalent to seeking a solution \(u_{h} \in X_{h}\) such that</p>

\[\begin{align}
  a(u_{h},v) = L(v) \;\;\;\forall v \in X_{h}.\label{eq:galerkin}
\end{align}\]

<p>What does the Galerkin method actually entail in practice? If \(X_{h}\) is a finite-dimensional space of \(X\), then it is spanned by a (finite) set of basis functions which we’ll denote by \(\{\varphi_{i}\}_{i=1}^{n}\). In other words, we can write \(X_{h} = \text{span}\{\varphi_{i}\}_{i=1}^{n}\). This means we can write \(u_{h}\) as \(u_{h}(x) = \sum_{j=1}^{n} c_{j}\varphi_{j}(x)\) for some coefficients \(c_{j}\), i.e. \(u_{h}\) is simply a linear combination of the basis functions of \(X_{h}\) and solving \eqref{eq:galerkin} is equivalent to finding the linear coefficients \(c_{j}\) so that \(u_{h}\) satisfies \eqref{eq:galerkin}.</p>

<p>In order for \(u_{h}\) to solve \eqref{eq:galerkin}, we must have \(a(u_{h},v) = L(v)\) for each \(v \in X_{h}\). Since there are finitely many basis functions in \(X_{h}\), this is then equivalent to the system of equations</p>

\[\begin{align}
  a(\sum_{j=1}^{n} c_{j}\varphi_{j}, \varphi_{1}) &amp;= L(\varphi_{1})\notag\\
  a(\sum_{j=1}^{n} c_{j}\varphi_{j}, \varphi_{2}) &amp;= L(\varphi_{2})\notag\\
  &amp;\vdots\notag\\
  a(\sum_{j=1}^{n} c_{j}\varphi_{j}, \varphi_{n}) &amp;= L(\varphi_{n}).\label{eq:system1}
\end{align}\]

<p>To solve for \(c\), we note that \(a\) is linear in the first argument so \eqref{eq:system1} can be written as the linear system</p>

\[\begin{align}
  \begin{bmatrix}
    a(\varphi_{1}, \varphi_{1}) &amp; a(\varphi_{1}, \varphi_{2}) &amp; \cdots &amp; a(\varphi_{1}, \varphi_{n})\\
    a(\varphi_{2}, \varphi_{1}) &amp; a(\varphi_{2}, \varphi_{2}) &amp; \cdots &amp; a(\varphi_{2}, \varphi_{n})\\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
    a(\varphi_{n}, \varphi_{1}) &amp; a(\varphi_{n}, \varphi_{2}) &amp; \cdots &amp; a(\varphi_{n}, \varphi_{n})
  \end{bmatrix} \begin{bmatrix}
    c_{1}\\
    c_{2}\\
    \vdots\\
    c_{n}
  \end{bmatrix} = \begin{bmatrix}
    L(\varphi_{1})\\
    L(\varphi_{2})\\
    \vdots\\
    L(\varphi_{n})
  \end{bmatrix}.\label{eq:discrete}
\end{align}\]

<p>Notably, the Galerkin method yields a solution which is the “best approximation” to \(u\) from the space \(X_{h}\), a fact which was proved by Jean Céa in his 1964 dissertation.</p>

<p><strong>Theorem (Céa’s Lemma).</strong> <em>Let \(X\) be a Hilbert space with norm \(||\cdot||_{X}\). Let \(a : X \times X \to \mathbb{R}\) ve a bilinear operator which is both continuous and coercive in \(X\), i.e.</em></p>
<ol>
  <li>
    <p><em>There is a constant \(M &gt; 0\) such that \(a(u,v) \leqslant M\|u\|_{X}\|v\|_{X}\) for all \(u,v \in X\) (continuity),</em></p>
  </li>
  <li>
    <p><em>There is a constant \(\alpha &gt; 0\) such that \(\alpha \|v\|_{X}^{2} \leqslant a(v,v)\) for all \(v \in X\) (coercivity).</em></p>
  </li>
</ol>

<p><em>Then \(u_{h}\) defined by \eqref{eq:galerkin} satisfies the estimate</em></p>

\[\begin{align}
  \|u-u_{h}\|_{X} \leqslant \frac{M}{\alpha} \inf_{v \in X_{h}} \|u-v\|_{X} \label{eq:cea}.
\end{align}\]

<p>The continuity and coercivity conditions relate to the well-posedness of \eqref{eq:variational}. They are sufficient conditions for \eqref{eq:variational} to have a unique solution. Céa’s Lemma is useful because \eqref{cea} allows us to leverage approximation properties of the space \(X_{h}\) to derive error estimates for the PDE approximation \(u_{h}\). What exactly do we mean by this? Let’s demonstrate with an example.</p>

<p>In finite element methods, one takes \(X_{h}\) to be the space of piecewise continuous polynomial functions. In one dimension, the basis for piecewise linears are the <a href="https://en.wikipedia.org/wiki/Triangular_function">hat functions</a>. The approximation error of any continuous function by such functions may be quantified by the size of the support of each hat function (i.e. the so-called mesh size), which in turns tells exactly how fast the error decreases with respect to the number of degrees of freedom in the problem. For example, let’s consider the domain \(\Omega = (0,1)\) which as subdivided into equal subintervals of width \(h\). Given a function \(f \in H^{1}(0,1)\) and its interpolation \(\Pi_{h}f\) onto the space of piecewise continuous linear functions over our subintervals (\(\Pi_{h}\) is an interpolation operator here which takes functions in \(H^{1}\) and interpolates them to piecewise continuous linear functions on subintervals of width \(h\); the precise definition is not important for this discussion), we have the well-known error estimate</p>

\[\begin{align}
  \||f-\Pi_{h}f\||_{H^{1}} \leqslant Ch\||f''\||_{L^{2}}.\label{eq:linear interp}
\end{align}\]

<p>The right-hand side of \eqref{eq:linear interp} tells us that if we reduce the width \(h\) of our subintervals by a factor of, say 2 (i.e. we use more piecewise continuous linears to approximate \(f\)), we should expect the error \(\|f-\Pi_{h}f\|_{H^{1}}\) to also reduce by a factor of 2. Estimates like \eqref{eq:linear interp} also tell us that, if we use the same space of piecewise continuous linears to try and approximate the solution of \eqref{eq:variational}, we should expect the approximation error \(\|u-u_{h}\|_{H^{1}}\) to converge in the same manner as \eqref{eq:linear interp} thanks to Céa’s Lemma:</p>

\[\begin{align}
  \|u-u_{h}\|_{H^{1}} &amp;\leqslant \frac{M}{\alpha} \inf_{v \in X_{h}} \|u-v\|_{H^{1}}\notag\\
  &amp;\leqslant \frac{M}{\alpha}\cdot \|u-\Pi_{h}u\|_{H^{1}}\label{eq:piecewise linear ex}\\
  &amp;\leqslant C\frac{M}{\alpha}h ||u''||_{L^{2}}\notag.
\end{align}\]

<p>Note that we get from the first line to the second line of \eqref{eq:piecewise linear ex} by picking \(v = \Pi_{h}u \in X_{h}\), the interpolation of our true solution onto the space of piecewise continuous linears. The infimum of $|u-v|_{H^{1}}$ over all choices of \(v \in X_{h}\) is always bounded from above by \(\|u-v\|_{H^{1}}\) for any <em>specific</em> choice of \(v\)!</p>

<p>With Galerkin neural networks, which we’re now finally in a position to introduce, we will utilize the Galerkin method and Céa’s Lemma to develop a framework for deep learning of PDEs which retains many of the nice approximation properties of the Galerkin method and is naturally adaptive to the PDE under consideration.</p>

<h1 id="4-galerkin-neural-network-framework"><strong>4. Galerkin Neural Network Framework</strong></h1>

<p>I’m sorry for how long the introductory sections of this post were, but I figured that if you’re a mathematician interested in learning more about Galerkin neural networks, you either already read through the published paper <a href="https://arxiv.org/abs/2105.14094">[3]</a> or are able to read through it without much difficulty. This blog post is really geared towards CS folks and others who might not be familiar with a lot of the mathematical machinery presented thus far and are looking for a more informal introduction than is presented in <a href="https://arxiv.org/abs/2105.14094">[3]</a>.</p>

<p><strong>Main Idea.</strong> <em>Galerkin neural networks seeks to approximate the solution \(u\) of \eqref{eq:variational} by projecting it onto a finite-dimensional subspace whose basis functions are outputs of a sequence of shallow neural networks. That is, we approximate \(u\) with</em></p>

\[\begin{align}
  u_{NN}(x;\theta) \approx \sum_{i=1}^{m} \alpha_{i} \varphi_{i}^{NN}(x;\theta^{(i)}).
\end{align}\]

<p>With this idea in mind, it all boils down to how to generate the basis functions \(\{\varphi_{i}\}_{i=1}^{m}\), since once we have the basis functions in hand, we simply follow \eqref{eq:discrete} to obtain the coefficients \(\alpha\).</p>

<p>Figure 3 provides a visualization of the overall idea. You’ll notice I’ve labeled the basis function coming from the smallest network as the “coarse scale” basis function while the one coming from the widest network is labeled a “fine scale” basis function. This has to do with how we can interpret the basis functions and I’ll discuss this more once we’ve actually covered how to generate the basis functions.</p>

<div class="container">
  <div class="row">
    <div class="col">
    </div>
    <div class="col-10">
      <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gnn/gnn-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gnn/gnn-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gnn/gnn-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/gnn/gnn.png" class="img rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

      <div class="caption">
        <b>Figure 3:</b> Visualization of a the Galerkin neural network framework.
      </div> 
    </div>
    <div class="col">
    </div>
  </div>
</div>

<p>And the way that we’re going to generate these basis functions has to do with learning Riesz representations of something called the <strong>weak residual.</strong> Let’s suppose we start off with some initial approximation \(u_{0} \in X\) to the true solution \(u\). I want to emphasize here that \(u_{0}\) can be <em>any</em> element of \(X\). In our paper, we always made the choice \(u_{0} = 0\) in the examples, which is just to say that we don’t have to have any prior knowledge of the solution at all to pick \(u_{0}\).</p>

<p><strong>Definition.</strong> Given \(u_{0} \in X\), the weak residual of \(u_{0}\) with respect to \eqref{eq:variational} is the linear functional \(r(u_{0}) : X \to \mathbb{R}\) given by</p>

\[\begin{align}
  \langle r(u_{0}),v \rangle = L(v) - a(u_{0},v).\label{eq:weak residual}
\end{align}\]

<p>The notation \(\langle r(u_{0}),v \rangle\) denotes the evaluation of \(r(u_{0})\) at \(v\). The quantity \eqref{eq:weak residual} will be zero by definition if \(u_{0}\) is the true solution. Otherwise, it is just a nonzero number.</p>

<h2 id="i-riesz-representation-of-weak-residuals"><strong>I. Riesz Representation of Weak Residuals</strong></h2>

<p>We note that \(L(v) = a(u,v)\) by \eqref{eq:variational}, so we can rewrite \eqref{eq:weak residual} as</p>

\[\langle r(u_{0}),v \rangle = a(u,v) - a(u_{0},v) = a(u-u_{0},v). \label{eq:weak residual riesz}\]

<p>Rewriting it in this way allows us to make some observations on \(r(u_{0})\).</p>

<p><strong>Theorem (Riesz representation).</strong> <em>Let \(X\) be a Hilbert space with norm \(\|\cdot\|_{X}\) and inner product \((\cdot,\cdot)_{X}\) and let \(L : X \to \mathbb{R}\) be a bounded linear functional on \(X\). There exists an element \(\varphi_{L} \in X\) such that</em></p>

\[\langle L,v \rangle = (\varphi_{L},v)_{X}\]

<p><em>for all \(v \in X\). Moreover, we have \(\|L\|_{*} = \|\varphi_{L}\|_{X}\), where \(\|\cdot\|_{*}\) denotes the operator norm.</em></p>

<p>The Riesz representation theorem says that in a Hilbert space, linear functionals can be “identified” with elements of the Hilbert space itself. Namely, if we want to evaluate a linear functional \(L\) for some argument \(v\), this is equivalent to taking the inner product of \(v\) with some element \(\varphi_{L} \in X\). This kernel \(\varphi_{L}\) is known as the <strong>Riesz representation</strong> of \(L\).</p>

<p>This suggests that the Riesz representation of the functional \(r(u_{0})\) is the error \(u-u_{0}\), at least when we consider the norm on \(X\) to be defined by \(\|\cdot\|_{X} := a(\cdot,\cdot)^{1/2}\) (this definition of \(\|\cdot\|_{X}\) means that \((\cdot,\cdot)_{X} := a(\cdot,\cdot)\)). <em>We can define the norm and inner product on \(X\) in this way if \(a\) is symmetric and positive-definite</em>, so for now we’re going to assuming that \(a\) is symmetric positive-definite. The norm that’s induced by the operator \(a\) has a special name: the energy norm. It’s standard to use triple bars to denote this norm: \(\vert\vert\vert \cdot \vert\vert\vert := a(\cdot,\cdot)^{1/2}\).</p>

<p>Thus, we can explicitly say what the Riesz representation of the weak residual is, but who cares? Why is this important? Let’s introduce the variable \(\varphi\) to denote the <em>normalized</em> Riesz representation of \(r(u_{0})\), i.e. \(\varphi = (u-u_{0})/\vert\vert\vert u-u_{0} \vert\vert\vert\). Now suppose we could compute this \(\varphi\). Then we could easily take a linear combination \(\beta_{0}u_{0} + \beta_{1}\varphi\) and recover the true solution – just take \(\beta_{0} = 1\) and \(\beta_{1} = \vert\vert\vert u-u_{0} \vert\vert\vert\).</p>

<p>But typically, we won’t be able to compute \(\varphi\) exactly. This is akin to solving \eqref{eq:variational} exactly, which can only be done in very simple cases. Otherwise, we’ll have to approximate \(\varphi\), and the key to doing this is actually a very simple property of the Riesz representation of the weak residual.</p>

<p><strong>Proposition 1.</strong> Let \(\varphi := (u-u_{0})/\vert\vert\vert u-u_{0} \vert\vert\vert\) denote the normalized Riesz representation of \(r(u_{0})\). Then \(\varphi\) satisfies</p>

\[\begin{align}
  \varphi = \text{argmax}_{v \in X \cap B} \langle r(u_{0}),v \rangle, \;\;\;B := \{v \in X : \vert\vert\vert v \vert\vert\vert = 1\} \label{eq:continuous maximizer}
\end{align}\]

<p>Proposition 1 says that the normalized Riesz representation \(\varphi\) is actually the maximizer of the weak residual over all elements of \(X\) with unity energy norm. In theory, all we need to do to compute \(\varphi\) is solve the maximization problem laid out in \eqref{eq:continuous maximizer}. We already know we can’t solve this maximization problem over all of \(X\) (which is infinite-dimensional). Instead, what if we replace \(X\) with the set of all functions that can be represented by a particular neural network architecture?</p>

<h2 id="ii-learning-a-basis-function"><strong>II. Learning a Basis Function</strong></h2>

<p>Let’s define the set \(V_{n}^{\sigma}\) to be the set of all functions which can be realized by a feedforward neural network with a single hidden layer of \(n\) neurons and activation function \(\sigma\):</p>

\[\begin{align}
  V_{n}^{\sigma} := \left\{ v : v(x) = \sum_{i=1}^{n} c_{i}\sigma(x\cdot W_{i} + b_{i}), \;W \in \mathbb{R}^{d\times n}, \;b \in \mathbb{R}^{1\times n}, \;c \in \mathbb{R}^{n} \right\}.\label{eq:Vn}
\end{align}\]

<p>And let’s replace \(X \cap B\) in \eqref{eq:continuous maximizer} with \(V_{n}^{\sigma} \cap B\). We’ll call the maximizer of this problem \(\varphi_{1}^{NN}\):</p>

\[\begin{align}
  \varphi_{1}^{NN} = \text{argmax}_{v \in V_{n}^{\sigma} \cap B} \langle r(u_{0}),v(\cdot;\theta) \rangle. \label{eq:discrete maximizer}
\end{align}\]

<p>The maximization problem in \eqref{eq:discrete maximizer} is now tractable. In fact, we can approximate it using the typical optimizers we would use for other deep learning problems. Before we go and do this though, we would do well to quantify how far off \(\varphi_{1}^{NN}\) is from the true Riesz representation \(\varphi\).</p>

<p><strong>Theorem 1.</strong> <em>Given \(\varepsilon \in (0,1)\), there exists \(n \in \mathbb{N}\) and \(\varphi_{1}^{NN} \in V_{n}^{\sigma} \cap B\) defined by \eqref{eq:discrete maximizer} such that</em>
\(\begin{align}
  \vert\vert\vert \varphi-\varphi_{1}^{NN} \vert\vert\vert \leqslant \frac{2\varepsilon}{1-\varepsilon}.
\end{align}\)</p>

<p>The proof is somewhat technical and provided in <a href="https://arxiv.org/abs/2105.14094">[3]</a>. It piggybacks off of standard universal approximation results such as those of Cybenko, Hornik, and Leshno. As such, Theorem 1 doesn’t quantify how the error converges with respect to the degrees of freedom in the network (such as in \eqref{eq:linear interp}).</p>

<p>As an aside, it’s worth noting that there have been recent efforts to quantify both the minimum width and depth of a neural network necessary to reach a target accuracy, and how the approximation error in neural networks varies with respect to the width and depth of the network a la \eqref{eq:linear interp}. Some examples of this are the excellent work of De Ryck, Lanthaler, and Mishra <a href="https://doi.org/10.1016/j.neunet.2021.08.015">[5]</a>; Yarotsky <a href="https://doi.org/10.1016/j.neunet.2017.07.002">[6]</a>; and Opschoor, Petersen, and Schwab <a href="https://www.sam.math.ethz.ch/sam_reports/reports_final/reports2019/2019-07_rev2.pdf">[7]</a>. Such results can be integrated with Theorem 1 when appropriate to obtain explicit error bounds for our method.</p>

<p>Once we have \(\varphi_{1}^{NN}\) computed (see \(\S\)4.IV for implementation details), we can use it to “correct” the initial approximation \(u_{0}\). Let’s form the two-dimensional subspace \(S_{1} := \text{span}\{u_{0}, \varphi_{1}^{NN}\}\) and use Galerkin’s method to project \(u\) onto \(S_{1}\):</p>

\[\begin{align}
  u_{1} \in S_{1} \;:\; a(u_{1},v) = L(v) \;\;\;\forall v \in S_{1}.\label{eq:S1}
\end{align}\]

<p>\eqref{eq:S1} is equivalent to solving a \(2\times 2\) linear system to obtain \(u_{1} = \alpha_{0}u_{0} + \alpha_{1}\varphi_{1}^{NN}\). We can derive an error estimate for \(u_{1}\) based on Theorem 1.</p>

<p><strong>Theorem 2.</strong> Given \(\varepsilon \in (0,1)\) and \(\varphi_{1}^{NN}\) as defined in \eqref{eq:discrete maximizer}, there holds</p>

\[\begin{align}
  \vert\vert\vert u-u_{1} \vert\vert\vert \leqslant \frac{2\varepsilon}{1-\varepsilon} \vert\vert\vert u-u_{0} \vert\vert\vert.
\end{align}\]

<p>This result says that if we supplement the initial approximation \(u_{0}\) with the approximate Riesz representation \(\varphi_{1}^{NN}\), we should expected to reduce the error in the initial approximation by a factor of ~\(\varepsilon\).</p>

<p><strong>Remark.</strong> <em>Theorems 1 and 2 only quantify the approximation error accrued when using the set \(V_{n}^{\sigma}\) as a stand-in for the infinite-dimensional space \(X\). Error in the optimization method used to compute \eqref{eq:discrete maximizer} is another significant source of error. Eventually stagnation of these methods is ubiquitous in deep learning. Galerkin neural networks addresses this issue by conceding that we won’t be able to use a single large DNN to capture all features of the solution. Rather, we sequentially compute basis functions as corrections to the error accrued in the previous steps in order to improve accuracy incrementally.</em></p>

<h2 id="iii-adaptive-subspace-generation"><strong>III. Adaptive Subspace Generation</strong></h2>

<p>We now have an improved approximation \(u_{1}\), but what if this still isn’t good enough? We can naturally iterate the procedure thus far to generate more basis functions. This is done by replacing \(r(u_{0})\) with \(r(u_{1})\) and computing its maximizer in \(V_{n}^{\sigma} \cap B\). To generalize, the \(i\)th basis function \(\varphi_{i}^{NN}\) is generated by computing</p>

\[\begin{align}
  \varphi_{i}^{NN} = \text{argmax}_{v \in V_{n_{i}}^{\sigma} \cap B} \langle r(u_{i-1}),v \rangle.\label{eq:basis function i}
\end{align}\]

<p>We’re using \(n_{i}\) here to denote the width of the neural network for \(\varphi_{i}^{NN}\). The \(i\)th approximation \(u_{i}\) is computed by forming the subspace \(S_{i} := \text{span}\{u_{0}, \varphi_{1}^{NN}, \dots, \varphi_{i}^{NN}\}\) and solving</p>

\[\begin{align}
  u_{i} \in S_{i} \;:\; a(u_{i},v) = L(v) \;\;\;\forall v \in S_{i}.\label{eq:ui}
\end{align}\]

<p><strong>Corollary 1.</strong> Given \(0 &lt; \varepsilon_{i} &lt; 1\), \(\varphi_{i} := (u-u_{i-1})/\vert\vert\vert u-u_{i-1} \vert\vert\vert\), \(\varphi_{i}^{NN}\) defined by \eqref{eq:basis function i}, and \(u_{i}\) defined by \eqref{eq:ui}, there holds</p>

\[\begin{align}
  \vert\vert\vert \varphi_{i} - \varphi_{i}^{NN} \vert\vert\vert \leqslant \frac{2\varepsilon_{i}}{1-\varepsilon_{i}}
\end{align}\]

<p>and
\(\begin{align}
  \vert\vert\vert u-u_{i} \vert\vert\vert \leqslant \prod_{j=1}^{i}\frac{2\varepsilon_{j}}{1-\varepsilon_{j}} \vert\vert\vert u-u_{0} \vert\vert\vert.
\end{align}\)</p>

<p>Each basis function \(\varphi_{i}^{NN}\) approximates the Riesz representation \(\varphi_{i}\) of the \(i\)th weak residual, which shouldn’t be too surprising. The approximation \(u_{i}\) is exponentially convergent with respect to the number of basis functions, and in fact each basis function added to the subspace \(S_{i}\) reduces the original error \(\vert\vert\vert u-u_{0} \vert\vert\vert\) by a factor of ~\(\varepsilon_{i}\).</p>

<p>We could go on generating these basis functions indefinitely. For problems where we have access to the true solution, we could compute exact errors of our approximation to determine when to stop adding basis functions to the subspace. For most realistic problems though, we won’t have access to such metrics and will need to estimate the error on the fly. Conveniently, Galerkin neural networks has a very nice way to do this.</p>

<p>We already know that the Riesz representation \(\varphi_{i}\) is the maximizer of \(r(u_{i-1})\). More importantly, the maximum value of \(r(u_{i-1})\) is actually the energy error \(\vert\vert\vert u-u_{i-1} \vert\vert\vert\):</p>

\[\vert\vert\vert u-u_{i-1} \vert\vert\vert = \max_{v\in V_{n_{i}}^{\sigma} \cap B} \langle r(u_{i-1}),v \rangle = \langle r(u_{i-1}),\varphi_{i} \rangle.\]

<p>Thus, when replacing \(\varphi_{i}\) with \(\varphi_{i}^{NN}\), it is reasonable to expect \(\langle r(u_{i-1}),\varphi_{i}^{NN} \rangle \approx \vert\vert\vert u-u_{i-1} \vert\vert\vert\). We can quantify just how well the weak residual approximates the energy norm with the following result.</p>

<p><strong>Theorem 3.</strong> Given \(\varepsilon_{i} \in (0,1/3)\), there holds</p>

\[\begin{align}
  \frac{1-\varepsilon_{i}}{1+\varepsilon_{i}} \langle r(u_{i-1}),\varphi_{i}^{NN} \rangle \leqslant \vert\vert\vert u-u_{i-1} \vert\vert\vert \leqslant \frac{1-\varepsilon_{i}}{1-3\varepsilon_{i}} \langle r(u_{i-1}),\varphi_{i}^{NN} \rangle.
\end{align}\]

<p>This says that if we evaluate the residual \(r(u_{i-1})\) at the computed basis function \(\varphi_{i}^{NN}\), it should be a reasonable <em>a posteriori</em> estimate of the energy error, which is typically a physical quantity of interest!</p>

<p>In effect, this means we can choose a desired tolerance \(\texttt{tol}\) and use \(\langle r(u_{i-1}),\varphi_{i}^{NN} \rangle &lt; \texttt{tol}\) as a stopping ctierion for determining when to stop generating new basis functions.</p>

<h3 id="i-an-illustrative-example"><strong>i. An Illustrative Example</strong></h3>

<p>Before discussing the finer details of implementation and showcasing some broader examples, I want to provide a straightforward one-dimensional example which demonstrates all of the key features of Galerkin neural networks. Let’s consider the Poisson equation:</p>

\[\begin{align}
  \begin{cases}
    -u''(x) = f(x) &amp;\text{in}\;(0,1)\\
    u(0) - \tau u'(0) = 0\\
    u(1) + \tau u'(1) = 0.
  \end{cases}\label{eq:poisson1d}
\end{align}\]

<p>The parameter \(\tau \ll 0\) is designed to approximate a homogeneous Dirichlet boundary condition: as \(\tau \to 0\), the solution of the above BVP converges to the solution when \(\tau = 0\). The reason for having this parameter is due to the fact that weak formulations of this problem typically contain information about the boundary condition in the Hilbert space \(X\), e.g. in \eqref{eq:poisson1d weak} we had \(X=H^{1}_{0}(0,1)\), but in general the Galerkin neural network basis functions \(\varphi_{i}^{NN}\) won’t be in \(H^{1}_{0}(0,1)\), only \(H^{1}(0,1)\).</p>

<p>The variational formulation for \eqref{eq:poisson1d} is</p>

\[\begin{align}
  u \in H^{1}(0,1) \;:\; a(u,v) :&amp;= (u',v')_{L^{2}} + \frac{1}{\tau}u(0)v(0) + \frac{1}{\tau}u(1)v(1)\notag\\
   &amp;= (f,v)_{L^{2}} =: L(v) \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\forall v \in H^{1}(0,1).\notag
\end{align}\]

<p>We’ll take \(f(x) = (2\pi)^{2}\sin(2\pi x) + (4\pi)^{2}\sin(4\pi x) + (6\pi)^{2}\sin(6\pi x)\) in which case the true solution \(u\) is</p>

\[\begin{align}
  u(x) = \sin(2\pi x) + \sin(4\pi x) + \sin(6\pi x) + \frac{-24\tau\pi x + 12\pi\tau}{1+2\tau}.\notag
\end{align}\]

<p>I’ll show first what the Riesz representations and basis functions look like. The different parameters used in this example are provided in Table 2.</p>

<p>\(\small
\begin{array}{c|c|c|c|c|c|c}
\hline
  \textbf{Parameter} &amp; u_{0} &amp; \text{widths } n_{i} &amp; \text{activation} &amp; \text{optimizer} &amp; \text{learn rate} &amp; \text{training set}\\ 
\hline
   \textbf{Value} &amp; \equiv 0 &amp; 5\cdot 2^{i-1} &amp; \tanh((1+i)t) &amp; \text{Adam} &amp; 10^{-2}/1.1^{i-1} &amp; 512 \text{ GL pts} \\
\hline
\end{array}\)</p>
<div class="caption">
  <b>Table 2:</b> Parameters for approximating 1D Poisson equation using Galerkin neural networks. GL = Gauss-Legendre quadrature.
</div>

<div class="container">
  <div class="row mt-0">
      <div class="col-sm mt-0 mt-md-0">
          <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gnn/poisson1d/plot_iter_0_error_1-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gnn/poisson1d/plot_iter_0_error_1-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gnn/poisson1d/plot_iter_0_error_1-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/gnn/poisson1d/plot_iter_0_error_1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

      </div>
      <div class="col-sm mt-0 mt-md-0">
          <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gnn/poisson1d/plot_iter_1_error_1-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gnn/poisson1d/plot_iter_1_error_1-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gnn/poisson1d/plot_iter_1_error_1-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/gnn/poisson1d/plot_iter_1_error_1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

      </div>
  </div>
  <div class="row mt-0">
      <div class="col-sm mt-0 mt-md-0">
          <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gnn/poisson1d/plot_iter_5_error_1-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gnn/poisson1d/plot_iter_5_error_1-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gnn/poisson1d/plot_iter_5_error_1-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/gnn/poisson1d/plot_iter_5_error_1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

      </div>
      <div class="col-sm mt-0 mt-md-0">
          <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gnn/poisson1d/plot_iter_7_error_1-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gnn/poisson1d/plot_iter_7_error_1-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gnn/poisson1d/plot_iter_7_error_1-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/gnn/poisson1d/plot_iter_7_error_1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

      </div>
  </div>
  <div class="caption">
      <b>Figure 4:</b> True Riesz representation $\varphi_{i}$ (blue lines) and basis function approximation $\varphi_{i}^{NN}$ (orange lines).
  </div> 
</div>

<p>One striking observation is that the first couple of Riesz representations consist of mainly low frequency oscillations while the later Riesz representations consist of very high frequency oscillations, and generally speaking each Riesz representation is mono-frequency. Thus, Galerkin neural networks essentially decomposes the problem into a series of subproblems, each of which encodes different frequencies of the solution. We also observe that each neural network is able to very closely approximate the true Riesz representation (granted, this is a very simple problem).</p>

<p>Utilizing all of these basis functions to approximate the solution results in a highly accurate approximation as seenin Figure 5 (\(\eta(u_{i-1}, \varphi_{i}^{NN})\) was the notation we used in [[3]] to denote the weak residual). As predicted by Theorem 3, the weak residual almost perfectly approximates the energy error \(\vert\vert\vert u-u_{i-1} \vert\vert\vert\). Similarly, we can actually take \(\vert\vert \varphi_{i}^{NN} \vert\vert_{L^{2}}\) as an approximation to the \(L^{2}\) error, which is pretty accurate as well. It’s also clear that with each basis function added to the subspace, we reduce the approximation error by a fixed amount (Corollary 1).</p>

<div class="container">
  <div class="row">
    <div class="col">
    </div>
    <div class="col-8">
      <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gnn/poisson1d/errors-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gnn/poisson1d/errors-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gnn/poisson1d/errors-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/gnn/poisson1d/errors.png" class="img rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

      <div class="caption">
        <b>Figure 5:</b> Approximation errors for the 1D Poisson equation.
      </div> 
    </div>
    <div class="col">
    </div>
  </div>
</div>

<h2 id="iv-implementation-details"><strong>IV. Implementation Details</strong></h2>

<p>Before moving on to some more examples, I’ll touch of some implementation details in this section. The first order of business is how to actually compute \eqref{eq:discrete maximizer}. Rather than seeking a normalized maximizer, we divide through by the energy norm:</p>

\[\begin{align}
  \text{argmax}_{v \in V_{n}^{\sigma} \cap B} \langle r(u_{i-1}),v \rangle &amp;= \text{argmax}_{v \in V_{n}^{\sigma}} \frac{\langle r(u_{i-1}),v \rangle}{\vert\vert\vert v \vert\vert\vert}\notag\\
  &amp;= \text{argmax}_{\theta} \frac{\langle r(u_{i-1}),v(\cdot;\theta) \rangle}{\vert\vert\vert v(\cdot;\theta) \vert\vert\vert}\notag
\end{align}\]

<p>Next, we note that \(\langle r(u_{i-1}),v \rangle\) consists of many inner products (integrals) which must be evaluated. We will approximate these integrals with standard Gaussian quadrature rules. To be concrete, let’s consider the 1D Poisson example from the previous section. A quadrature rule consists of a set of nodes \(\{x_{i}\}_{i=1}^{n_{g}} \subset \Omega\) and weights \(\{w_{i}\}_{i=1}^{n_{g}}\) which are used to approximate the integral as a weighted sum of function evaluations over the nodes:</p>

\[\int_{\Omega} g(x)\;dx \approx \sum_{i=1}^{n_{g}} w_{i}g(x_{i}).\]

<p>For the 1D Poisson example, the weak residual is approximated by</p>

\[\begin{align}
  \frac{\langle r(u_{i-1}),v \rangle}{\vert\vert\vert v \vert\vert\vert} &amp;\approx \frac{L_{1} - L_{2} - L_{3}}{(\sum_{j=1}^{n_{g}} w_{j}v'(x_{j};\theta)^{2} + \tau^{-1}[v(0;\theta)^{2} + v(1;\theta)^{2}])^{1/2}}\notag\\
  L_{1} &amp;= \sum_{j=1}^{n_{g}} w_{j}f(x_{j})v(x_{j};\theta)\notag\\
  L_{2} &amp;= \sum_{j=1}^{n_{g}} w_{j}u'_{i-1}(x_{j})v'(x_{j};\theta)\notag\\
  L_{3} &amp;= \tau^{-1}[u_{i-1}(0)v(0;\theta) + u_{i-1}(1)v(1;\theta)].\notag
\end{align}\]

<p>For all examples here, we use a <a href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Legendre_quadrature">Gauss-Legendre</a> quadrature rule in which the nodes and weights are obtained from Legendre polynomials.</p>

<p>To perform the numerical optimization, we use a hybrid first-order least squares optimization approach. This approach considers the weights of the activation layer (i.e. the parameters \(c\) in \eqref{eq:Vn}) to be expansion coefficients of the nonlinear basis \(\sigma\). It’s similar in concept to nonlinear least squares methods such as <a href="https://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm">Levenberg-Marquardt</a> which obtain \(c\) by solving a linear least squares system.</p>

<p>In [[3]] we proposed the following training procedure. We update the hidden parameters using a gradient-based optimizer such as gradient descent:</p>

\[\begin{align}
  W &amp;\leftarrow W + \lambda \nabla_{W}\left[ \frac{\langle r(u_{i-1}),v(\cdot;\theta) \rangle}{\vert\vert\vert v(\cdot;\theta) \vert\vert\vert} \right]\notag\\
  b &amp;\leftarrow b + \lambda \nabla_{b}\left[ \frac{\langle r(u_{i-1}),v(\cdot;\theta) \rangle}{\vert\vert\vert v(\cdot;\theta) \vert\vert\vert} \right].\notag\\
\end{align}\]

<p>The linear parameters are updated according to the least squares solve:</p>

\[\begin{align}
  \begin{cases}
    c \leftarrow \text{argmin}_{c \in \mathbb{R}^{n}} \|Ac-F\|_{\ell^{2}}\notag\\
    A_{k\ell} = a(\sigma_{k}, \sigma_{\ell}), \;\;\;F_{k} = L(\sigma_{k}) - a(u_{i-1},\sigma_{k})\\
    \sigma_{k} := \sigma((\cdot)\cdot W_{k} + b_{k}).
  \end{cases}
\end{align}\]

<p>This linear system is equivalent to taking the orthogonal projection of the Riesz representation \(u-u_{i-1}\) with respect to \(a(\cdot,\cdot)\) onto the space \(\text{span}\{\sigma(x\cdot W_{i}+b_{i})\}_{i=1}^{n}\).</p>

<p>Lastly, there is the matter of how to initialize the parameters \(W\) and \(b\). In computer vision applications, it’s common to draw values of \(W\) and \(b\) from a probability distribution (often uniform or Gaussian with mean and variance depending in some way on the width of the layer). These initializers are sometimes not optimal for function and PDE approximation. In [[8]], the authors propose a new initializer (the “box initialization”) which distributes the hyperplanes of the network more evenly throughout the computational domain \(\Omega\). While this initialization was developed for very deep ResNets, it is sufficient for our very shallow networks.</p>

<div class="container">
  <div class="row">
    <div class="col">
    </div>
    <div class="col-10">
      <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gnn/box-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gnn/box-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gnn/box-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/gnn/box.png" class="img rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

      <div class="caption">
        <b>Figure 6 (from [[8]]):</b> Hyperplanes $x\cdot W_{i} + b_{i}$ for the box initialization vs. several standard initializers in the unit square.
      </div> 
    </div>
    <div class="col">
    </div>
  </div>
</div>

<p>A full implementation (using Google’s Jax framework) for some simple 1D and 2D problems is provided on my <a href="https://github.com/jdongg">GitHub</a>. The rest of this post will be dedicated to showcasing more numerical examples.</p>

<h1 id="5-numerical-examples"><strong>5. Numerical Examples</strong></h1>

<h2 id="i-approximation-of-a-nonsmooth-function"><strong>I. Approximation of a Nonsmooth Function</strong></h2>

<p>The next example we’ll look at is a two-dimensional function approximation problem. Suppose we have target data \(f \in L^{2}(\Omega)\) and wish to fit this function with a neural network. The variational formulation for this problem is</p>

\[\begin{align}
  u \in L^{2} \;:\; a(u,v) := (u,v)_{L^{2}} = (f,v)_{L^{2}} =: L(v) \;\;\;\forall v \in L^{2},
\end{align}\]

<p>which is essentially just an \(L^{2}\) projection. The data \(f\) for this example will be space-time data describing the volume fraction of spherical particles in a suspension flow. Figure 4 shows the target function \(f\) that we’re going to approximate with Galerkin neural networks.</p>

<div class="container">
  <div class="row">
    <div class="col">
    </div>
    <div class="col-6">
      <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gnn/suspension/exact-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gnn/suspension/exact-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gnn/suspension/exact-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/gnn/suspension/exact.png" class="img rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

      <div class="caption">
        <b>Figure 6:</b> Target function $f$.
      </div> 
    </div>
    <div class="col">
    </div>
  </div>
</div>

<p>This data was generated using something called the <a href="https://doi.org/10.1016/j.jcp.2009.11.041">force-coupling method</a> with data provided by <a href="https://www.pnnl.gov/science/staff/staff_info.asp?staff_num=9563">Amanda Howard</a>. The application of Galerkin neural networks to this problem was carried out during my time as an intern at Pacific Northwest National Laboratory. The precise details of the force-coupling method aren’t important for demonstrating the main ideas of Galerkin neural networks.</p>

<p>The different parameters used in this simulation are provided in Table 2.</p>

<p>\(\small
\begin{array}{c|c|c|c|c|c|c}
\hline
  \textbf{Parameter} &amp; u_{0} &amp; \text{widths } n_{i} &amp; \text{activation} &amp; \text{optimizer} &amp; \text{learn rate} &amp; \text{training set}\\ 
\hline
   \textbf{Value} &amp; \equiv 0 &amp; 20\cdot 2^{i-1} &amp; \tanh((1+1.25i)t) &amp; \text{Adam} &amp; 10^{-2}/1.1^{i-1} &amp; 128\times 128 \text{ GL pts} \\
\hline
\end{array}\)</p>
<div class="caption">
  <b>Table 3:</b> Parameters for approximating $f$ using Galerkin neural networks. GL = Gauss-Legendre quadrature.
</div>

<p>I’ll first show the exact Riesz representations for the first two residuals (\(\varphi_{1}\) and \(\varphi_{2}\)) as well as their neural network approximations (\(\varphi_{1}^{NN}\) and \(\varphi_{2}^{NN}\)).</p>

<div>
  <div class="row mt-0">
      <div class="col-sm mt-0 mt-md-0">
          <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gnn/suspension/plot_error_phi_0-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gnn/suspension/plot_error_phi_0-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gnn/suspension/plot_error_phi_0-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/gnn/suspension/plot_error_phi_0.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

      </div>
      <div class="col-sm mt-0 mt-md-0">
          <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gnn/suspension/plot_error_phi_1-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gnn/suspension/plot_error_phi_1-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gnn/suspension/plot_error_phi_1-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/gnn/suspension/plot_error_phi_1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

      </div>
  </div>
  <div class="row mt-0">
      <div class="col-sm mt-0 mt-md-0">
          <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gnn/suspension/plot_basis_phi_0-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gnn/suspension/plot_basis_phi_0-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gnn/suspension/plot_basis_phi_0-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/gnn/suspension/plot_basis_phi_0.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

      </div>
      <div class="col-sm mt-0 mt-md-0">
          <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gnn/suspension/plot_basis_phi_1-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gnn/suspension/plot_basis_phi_1-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gnn/suspension/plot_basis_phi_1-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/gnn/suspension/plot_basis_phi_1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

      </div>
  </div>
  <div class="caption">
    <b>Figure 7:</b> $\varphi_{1}$ and $\varphi_{2}$ (top row) and $\varphi_{1}^{NN}$ and $\varphi_{2}^{NN}$ (bottom row).
  </div> 
</div>

<p>What we observe is that the first basis function \(\varphi_{1}^{NN}\) is a very coarse approximation to the true Riesz representation. It captures the low frequency behavior and “sees” that the Riesz representation is a symmetric function about the \(y\)-centerline but otherwise misses all of the fine-scale behavior. The same is true for \(\varphi_{2}^{NN}\) which essentially looks like a smoothed out version of \(\varphi_{2}\), albeit containing higher frequencies than \(\varphi_{1}^{NN}\).</p>

<p>Let’s take a look at the next couple of Riesz representations and their approximations.</p>

<div>
  <div class="row mt-0">
      <div class="col-sm mt-0 mt-md-0">
          <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gnn/suspension/plot_error_phi_2-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gnn/suspension/plot_error_phi_2-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gnn/suspension/plot_error_phi_2-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/gnn/suspension/plot_error_phi_2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

      </div>
      <div class="col-sm mt-0 mt-md-0">
          <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gnn/suspension/plot_error_phi_3-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gnn/suspension/plot_error_phi_3-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gnn/suspension/plot_error_phi_3-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/gnn/suspension/plot_error_phi_3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

      </div>
  </div>
  <div class="row mt-0">
      <div class="col-sm mt-0 mt-md-0">
          <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gnn/suspension/plot_basis_phi_2-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gnn/suspension/plot_basis_phi_2-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gnn/suspension/plot_basis_phi_2-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/gnn/suspension/plot_basis_phi_2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

      </div>
      <div class="col-sm mt-0 mt-md-0">
          <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gnn/suspension/plot_basis_phi_3-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gnn/suspension/plot_basis_phi_3-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gnn/suspension/plot_basis_phi_3-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/gnn/suspension/plot_basis_phi_3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

      </div>
  </div>
  <div class="caption">
    <b>Figure 8:</b> $\varphi_{3}$ and $\varphi_{4}$ (top row) and $\varphi_{3}^{NN}$ and $\varphi_{4}^{NN}$ (bottom row).
  </div> 
</div>

<p>We observe that \(\varphi_{3}\) and \(\varphi_{4}\) encode higher frequency behavior of the error. In all of these basis functions however, it’s clear that we never approximate the true Riesz representations perfectly. Indeed, the first couple of basis functions don’t look very good at all. Each basis function is only meant to reduce particular frequencies in the error, much like in the first example.</p>

<p>When we take a linear combination of these basis functions, we can see more clearly the effect each one has on the overall approximation.</p>

<div>
  <div class="row mt-0">
      <div class="col-sm mt-0 mt-md-0">
          <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gnn/suspension/plot_solution_phi_0-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gnn/suspension/plot_solution_phi_0-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gnn/suspension/plot_solution_phi_0-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/gnn/suspension/plot_solution_phi_0.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

      </div>
      <div class="col-sm mt-0 mt-md-0">
          <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gnn/suspension/plot_solution_phi_2-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gnn/suspension/plot_solution_phi_2-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gnn/suspension/plot_solution_phi_2-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/gnn/suspension/plot_solution_phi_2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

      </div>
      <div class="col-sm mt-0 mt-md-0">
          <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gnn/suspension/plot_solution_phi_6-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gnn/suspension/plot_solution_phi_6-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gnn/suspension/plot_solution_phi_6-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/gnn/suspension/plot_solution_phi_6.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

      </div>
  </div>
  <div class="row mt-0">
      <div class="col-sm mt-0 mt-md-0">
          <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gnn/suspension/plot_phi_g20_0-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gnn/suspension/plot_phi_g20_0-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gnn/suspension/plot_phi_g20_0-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/gnn/suspension/plot_phi_g20_0.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

      </div>
      <div class="col-sm mt-0 mt-md-0">
          <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gnn/suspension/plot_phi_g20_2-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gnn/suspension/plot_phi_g20_2-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gnn/suspension/plot_phi_g20_2-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/gnn/suspension/plot_phi_g20_2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

      </div>
      <div class="col-sm mt-0 mt-md-0">
          <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gnn/suspension/plot_phi_g20_6-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gnn/suspension/plot_phi_g20_6-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gnn/suspension/plot_phi_g20_6-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/gnn/suspension/plot_phi_g20_6.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

      </div>
  </div>
  <div class="caption">
    <b>Figure 9:</b> $u_{1}$, $u_{3}$, and $u_{7}$ (top row) and corresponding one-dimensional slices along $\gamma=20$ (bottom row).
  </div> 
</div>

<p>A common question I get asked during conference talks (and more recently, job talks) is what happens if we only use the widest neural network in our sequence to approximate the solution. The width of the network for \(\varphi_{i}^{NN}\) is \(n_{i}=20\times 2^{i-1}\) for this example, so \(u_{7}\) which utilizes networks of widths 20, 40, 80, 160, 320, 640, and 1280 to approximate the component basis functions. If we generate only a single basis function coming from a network with width \(n_{1}=1280\), we don’t come close to approximating the target function to the same degree of accuracy obtained from the full ensemble of basis function.</p>

<p>The analogous result using a single wide neural network is shown in Figure 7. While \(\varphi_{1}^{NN} \in V_{1280}^{\sigma}\) is certainly more accurate than \(\varphi_{1}^{NN} \in V_{20}^{\sigma}\) (the initial results shown in Figure 6), it’s clear that the former doesn’t come close to the accuracy obtained by using seven basis functions, the last of which comes from \(\varphi_{7}^{NN} \in V_{1280}^{NN}\). Sure, we can add a second basis function, say \(\varphi_{2}^{NN} \in V_{2560}^{NN}\) if we continue with the same rule for the network widths, but this is computationally inefficient.</p>

<p>I’ll discuss computational costs more in \(\S\)4.IV, but roughly speaking, the computational cost for Galerkin neural networks is on the same order as the computational cost of computing the basis function coming from the widest network. This means that computing \(\varphi_{1}^{NN} \in V_{1280}^{\sigma}\) takes about the same amount of time as \(\varphi_{7}^{NN} \in V_{1280}^{NN}\) and yet is several orders of magnitude less accurate. All this to say that the coarse basis functions we learn initially are important! It might not seem like it since the initial basis functions aren’t great approximations to their respective Riesz representations (Figure 5), but they encode the lower frequencies of the solution and allow the later basis functions to focus on higher frequencies.</p>

<div class="container">
  <div class="row">
    <div class="col-6">
      <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gnn/suspension/plot_oneshot_solution_phi_0-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gnn/suspension/plot_oneshot_solution_phi_0-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gnn/suspension/plot_oneshot_solution_phi_0-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/gnn/suspension/plot_oneshot_solution_phi_0.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-5">
      <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/gnn/suspension/plot_oneshot_phi_g20_0-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/gnn/suspension/plot_oneshot_phi_g20_0-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/gnn/suspension/plot_oneshot_phi_g20_0-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/gnn/suspension/plot_oneshot_phi_g20_0.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
  </div>
  <div class="caption">
    <b>Figure 10:</b> $u_{1}$ (left) and corresponding one-dimensional slice along $\gamma=20$. $u_{1}$ is obtained using a single neural network with width $n_{1}=1280$.
  </div> 
</div>

<h2 id="ii-poisson-equation-with-line-source-data"><strong>II. Poisson Equation with Line Source Data</strong></h2>

<h1 id="6-key-takeaways"><strong>6. Key Takeaways</strong></h1>

<h1 id="7-references"><strong>7. References</strong></h1>
<p>[1] Dissanayake, M. and N. Phan-Thien. “<a href="https://doi.org/10.1002/cnm.1640100303">Neural network-based approximations for solving partial differential equations</a>.” Communications in Numerical Methods in Engineering, 10(3), 195-201.</p>

<p>[2] Raissi, M., Perdikaris, P., and G.E. Karniadakis. “<a href="https://www.brown.edu/research/projects/crunch/sites/brown.edu.research.projects.crunch/files/uploads/Physics-informed%20neural%20networks_A%20deep%20learning%20framwork%20fir%20solving%20forward%20and%20inverse%20probelms%20involving%20nonlinear%20partial%20differential%20equations.pdf">Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations</a>.” Journal of Computational physics, 378, 686-707.</p>

<p>[3] Ainsworth, M. and J. Dong. “<a href="https://arxiv.org/abs/2105.14094">Galerkin neural networks: a framework for approximating variational equations with error control</a>.” SIAM Journal on Scientific Computing, 43(4), A2474-A2501.</p>

<p>[4] Ainsworth, M. and Y. Shin (2021). “<a href="https://epubs.siam.org/doi/pdf/10.1137/20M1353010?casa_token=nNt0YvwU13AAAAAA:ly4stGiO85ibuuoN0wP2fRlC5vXBOk2iqCfOd0qF_q2nvK0Hrat6ZQH58P7WA1VMxmg0HZl9ifRz">Plateau phenomenon in gradient descent training of ReLU networks: Explanation, quantification, and avoidance</a>.” SIAM Journal on Scientific Computing, 43(5), A3438-A3468.</p>

<p>[5] De Ryck, T., Lanthaler, S., and S. Mishra (2021). “<a href="https://doi.org/10.1016/j.neunet.2021.08.015">On the approximation of functions by tanh neural networks</a>.”” Neural Networks, 143, 732-750.</p>

<p>[6] Yarotsky, D. (2017). “<a href="https://doi.org/10.1016/j.neunet.2017.07.002">Error bounds for approximations with deep ReLU networks</a>.” Neural Networks, 94, 103-114.</p>

<p>[7] Opschoor, J. A., Petersen, P. C., and C. Schwab (2020). “<a href="https://www.sam.math.ethz.ch/sam_reports/reports_final/reports2019/2019-07_rev2.pdf">Deep ReLU networks and high-order finite element methods</a>.” Analysis and Applications, 18(05), 715-770.</p>

<h1 id="8-codes--resources"><strong>8. Codes &amp; Resources</strong></h1>
<ul>
  <li><a href="https://github.com/jdongg/numCL">GalerkinNN repository on GitHub</a></li>
  <li>All figures in this post were generated by me unless otherwise noted.</li>
</ul>]]></content><author><name>Justin Dong</name></author><category term="numerical-methods," /><category term="deep-learning" /><category term="neural-networks," /><category term="deep-learning" /><summary type="html"><![CDATA[1. Introduction]]></summary></entry><entry><title type="html">Numerical methods for conservation laws – high-order finite volume methods and slope limiters (part 2)</title><link href="http://localhost:4000/blog/2019/conservationlaws2/" rel="alternate" type="text/html" title="Numerical methods for conservation laws – high-order finite volume methods and slope limiters (part 2)" /><published>2019-05-25T11:12:00-04:00</published><updated>2019-05-25T11:12:00-04:00</updated><id>http://localhost:4000/blog/2019/conservationlaws2</id><content type="html" xml:base="http://localhost:4000/blog/2019/conservationlaws2/"><![CDATA[<h1 id="1-high-order-finite-volume-methods"><strong>1. High Order Finite Volume Methods</strong></h1>

<p>Last time, we established that the explicit first order finite volume scheme is given by
\(\begin{equation}
  \bar{u}_{j}^{n+1} = \bar{u}_{j}^{n} - \frac{\Delta t}{\Delta x}\left( \hat{f}(\bar{u}_{j}^{n}, \bar{u}_{j+1}^{n}) - \hat{f}(\bar{u}_{j-1}^{n}, \bar{u}_{j}^{n}) \right),
  \label{eq:finitevol2}
\end{equation}\)</p>

<p>where \(\bar{u}_{j}^{n}\) denotes the cell averages at time \(t^{n}\) and \(\bar{u}_{j}^{n+1}\) the cell averages at time \(t^{n+1} = t^{n} + \Delta t\). Here, \(\hat{f}\) is the <em>numerical flux function</em> which takes as inputs cell averages and returns an approximation of the flux function at the cell endpoints \(x_{j+1/ 2}\), i.e. \(f(u_{j+1/ 2}) \approx \hat{f}(\bar{u}_{j}, \bar{u}_{j+1})\) and \(f(u_{j-1/ 2}) \approx \hat{f}(\bar{u}_{j-1}, \bar{u}_{j})\).</p>

<p>More generally, we can view the quantity \(\bar{u}_{j}\) as a first order (zero degree polynomial) reconstruction of \(u(x)\) in cell \(I_{j}\) using the cell average. The same can be said of \(\bar{u}_{j+1}\) in cell \(I_{j+1}\). By approximating \(u(x_{j+1/ 2}^{-}) = u_{j+1/ 2}^{-} \approx \bar{u}_{j}\) on the left “side” of \(x_{j+1/ 2}\) and \(u(x_{j+1/ 2}^{+}) = u_{j+1/ 2}^{+} \approx \bar{u}_{j+1}\) on the right side of \(x_{j+1/ 2}\), the numerical flux has a natural interpretation as \(f(u_{j+1/ 2}) \approx \hat{f}(u_{j+1/ 2}^{-}, u_{j+1/ 2}^{+})\). We reconcile the two reconstructions at \(x_{j+1/ 2}\) by utilizing the numerical flux function. Figure 1 illustrates this concept.</p>

<div class="container">
  <div class="row">
    <div class="col">
    </div>
    <div class="col-9">
      <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/conslaw2/reconstruction0-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/conslaw2/reconstruction0-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/conslaw2/reconstruction0-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/conslaw2/reconstruction0.png" class="img rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

      <div class="caption">
        <b>Figure 1:</b> First order reconstruction. We reconstruct $u$ using a zero degree polynomial (i.e. constant functions) in each cell.
      </div> 
    </div>
    <div class="col">
    </div>
  </div>
</div>

<h2 id="i-polynomial-reconstruction"><strong>I. Polynomial Reconstruction</strong></h2>

<p>Viewing the finite volume scheme as a reconstruction procedure using the cell averages is key to developing higher order methods. Using \eqref{eq:finitevol2}, we have \(u_{j+1/ 2}^{-} = \bar{u}_{j} + \mathcal{O}(\Delta x)\). Suppose we want to develop a third-order finite volume scheme now. In particular, we would like to have</p>

\[u_{j+1/ 2}^{-} = \mathcal{F}(\bar{u}_{j-p}, \dots, \bar{u}_{j+q}) + \mathcal{O}(\Delta x^{3}),\]

<p>where \(p\) and \(q\) are integers to be chosen by us. It stands to reason that if we want a third-order reconstruction of $u(x)$ in cell \(I_{j}\), we need to use three “pieces” of information: \(u_{j+1/ 2}^{-} \approx \mathcal{F}(\bar{u}_{j-1}, \bar{u}_{j}, \bar{u}_{j+1})\). In other words, we are seeking a degree two polynomial \(p_{j}(x) = ax^{2} + bx + c\) in \(I_{j}\) such that \(u_{j+1/ 2}^{-} = p_{j}(x_{j+1/ 2}) + \mathcal{O}(\Delta x^{3})\). In order to fully determine this polynomial, i.e. solve for $a,b,c$, we require three equations. A natural condition to impose on \(p_{j}\) is that it preserves cell averages over the stencil \(\{I_{j-1}, I_{j}, I_{j+1}\}\):</p>

\[\begin{equation}
	\frac{1}{\Delta x} \int_{I_{i}} p_{j}(x)\;dx = \bar{u}_{i}, \;\;\;i \in \\{j-1,j,j+1\\}.\notag
\end{equation}\]

<p>We then take \(u_{j+1/ 2}^{-} = p_{j}(x_{j+1/ 2})\). Similarly, we take \(u_{j+1/ 2}^{+} = p_{j+1}(x_{j+1/ 2})\). Figure 2 demonstrates how the third-order reconstruction procedure works.</p>

<div class="container">
  <div class="row">
    <div class="col">
    </div>
    <div class="col-10">
      <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/conslaw2/reconstruction2-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/conslaw2/reconstruction2-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/conslaw2/reconstruction2-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/conslaw2/reconstruction2.png" class="img rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

      <div class="caption">
        <b>Figure 2:</b> Third order reconstruction. We reconstruct $u$ using a quadratic function defined in the stencil $\{I_{j-1}, I_{j}, I_{j+1}\}$ and enforce that the quadratic function maintains cell averages across the stencil. We take $u_{j+1/ 2}^{-} \approx p_{j}(x_{j+1 /2})$ and $u_{j+1/ 2}^{+} \approx p_{j+1}(x_{j+1/ 2})$ (not pictured here).
      </div> 
    </div>
    <div class="col">
    </div>
  </div>
</div>

<p><strong>Definition 1.</strong> <em>The order \(k\) in space finite volume scheme is given by</em>
\(\begin{equation}
	\frac{d\bar{u}_{j}}{dt} + \frac{1}{\Delta x}\left( \hat{f}(u_{j+1/ 2}^{-}, u_{j+1/ 2}^{+}) - \hat{f}(u_{j-1/ 2}^{-}, u_{j-1/ 2}^{+}) \right) = 0
	\label{eq:finitevolk}
\end{equation}\)</p>

<p><em>where \(u_{j+1/ 2}^{-} \approx p_{j}(x_{j+1/ 2})\) and \(u_{j+1/ 2}^{+} \approx p_{j+1}(x_{j+1/ 2})\). The degree \(k-1\) polynomial \(p_{j}\) is reconstructed from the stencil \(\{\bar{u}_{j-p}, \dots, \bar{u}_{j+q}\}\) and satifies the \(q+p+1\) conditions</em>
\(\frac{1}{\Delta x} \int_{I_{i}} p_{j}(x)\;dx = \bar{u}_{i}, \;\;\;i \in \\{j-p, \dots, j+q\\}.\)</p>

<p>For concreteness, we will work with the third-order scheme. It is a straightforward (if tedious) pen and paper exercise to compute the parameters \(a,b,c\) for the polynomial \(p_{j}(x) = ax^{2}+bx+c\). However, notice that the scheme \eqref{eq:finitevolk} only requires the evaluation of the polynomials at the endpoints of the cells. For the third order scheme, this amounts to the following formulae:</p>

\[\begin{align}
	u_{j+1/ 2}^{-} &amp;= p_{j}(x_{j+1/ 2}) = -\frac{1}{6}\bar{u}_{j-1} + \frac{5}{6}\bar{u}_{j} + \frac{1}{3}\bar{u}_{j+1}\label{eq:recon1}\\
	u_{j+1/ 2}^{+} &amp;= p_{j+1}(x_{j+1/ 2}) = \frac{1}{3}\bar{u}_{j} + \frac{5}{6}\bar{u}_{j+1} - \frac{1}{6}\bar{u}_{j+2}.\label{eq:recon2}
\end{align}\]

<p>These formulae can be found in a number of references, e.g. Chapter 4 of <a href="https://doi.org/10.1007/BFb0096351">[3]</a>.</p>

<p>Before moving on, we should make note of two very important issues here. The first is that \eqref{eq:finitevolk} is only a <em>semidiscrete</em> scheme – we haven’t discretized time yet! Recall that for the first order scheme we used an explicit first-order time discretization. But if we use this time discretization here, then we’ll be pairing a third-order in space discretization with a first-order in time discretization. Should we expect such a scheme to be stable? Furthermore, we’ll have to reduce our time step commensurately to ensure that the error in the time discretization does not dominate.</p>

<p>Second, we haven’t said anything about the convergence properties (or any other properties for that matter) of the higher order finite volume schemes. Recall that the Lax-Wendroff flux resulted in a second-order scheme which was neither monotone nor total variation diminishing (TVD). It’s unclear what will happen when we apply \eqref{eq:finitevolk} to actually solve a conservation law.</p>

<h2 id="ii-third-order-runge-kutta-time-integration"><strong>II. Third Order Runge-Kutta Time Integration</strong></h2>
<p>In all of the examples that follow, we will use a third order Runge-Kutta (RK3) discretization in time. For our purposes, this discretization is conditionally stable with some necessary restriction on the time step as with all explicit schemes. If we pair the RK3 discretization with a spatial discretization higher than third order accuracy, we may have to decrease the time step further.</p>

<p>The setting for the RK3 discretization is as follows: consider the semidiscrete scheme</p>

\[\frac{d\bar{u}_{j}}{dt} + \mathcal{L}(\bar{u}_{j}) = 0,\]

<p>where \(\mathcal{L}\) is the spatial operator associated with the flux terms.</p>

<p><strong>Definition 2.</strong> <em>The third order Runge-Kutta time discretization is given by</em></p>

\[\begin{align}
	\bar{u}_{j}^{(1)} &amp;= \bar{u}_{j}^{n} - \Delta t \mathcal{L}(\bar{u}_{j-p}^{n}, \dots, \bar{u}_{j+q}^{n}, t^{n})\notag\\
	\bar{u}_{j}^{(2)} &amp;= \frac{3}{4}\bar{u}_{j}^{n} + \frac{1}{4}\left( \bar{u}_{j}^{(1)} - \Delta t \mathcal{L}(\bar{u}_{j-p}^{(1)}, \dots, \bar{u}_{j+q}^{(1)}, t^{n} + \Delta t) \right)\notag\\
	\bar{u}_{j}^{n+1} &amp;= \frac{1}{3}\bar{u}_{j}^{n} + \frac{2}{3}\left( \bar{u}_{j}^{(2)} - \Delta t \mathcal{L}(\bar{u}_{j-p}^{(2)}, \dots, \bar{u}_{j+q}^{(2)}, t^{n} + \frac{1}{2}\Delta t) \right).\notag
\end{align}\]

<p>The choice of coefficients in the above scheme is deliberate. In fact, with these coefficients, the RK3 time discretization can be shown to be TVD.</p>

<p><strong>Proposition 1.</strong> <em>The third order Runge-Kutta time discretization is TVD when paired with a spatial scheme that is also TVD.</em></p>

<p>Interestingly, there are many examples of time discretizations which are not TVD, even when paired with a TVD spatial scheme. We refer the reader to the seminal work of Sigal Gottlieb and Chi-Wang Shu in <a href="https://doi.org/10.1090/S0025-5718-98-00913-2">[1]</a> for a much more in-depth discussion of TVD Runge-Kutta schemes.</p>

<h2 id="iii-a-first-attempt-at-implementation"><strong>III. A First Attempt at Implementation</strong></h2>
<p>Before covering the theory of higher order spatial discretizations, it is highly illustrative to attempt a first implementation of the third order finite volume scheme with RK3 time discretization. All of the source code can be found on <a href="https://github.com/jdongg/numCL">GitHub</a>. There are actually relatively few changes that must be made to the first order code. The main time-stepping loop is as follows:</p>
<d-code block="" language="python">
t = 0.0
while t &lt; T:
	# alpha for the Lax-Friedrichs flux
	A  = np.amax(np.amax(u0))

	# first RK stage
	um,up = polynomial_reconstruction(u0)
	fR = lf_flux(um,up,A)
	fL = np.roll(fR,1)
	u = u0 - dt/dx*(fR - fL)

	# second RK stage
	um,up = polynomial_reconstruction(u)
	fR = lf_flux(um,up,A)  
	fL = np.roll(fR,1)
	u = 3.0/4.0*u0 + 1.0/4.0*(u - dt/dx*(fR - fL))

	# third RK stage
	um,up = polynomial_reconstruction(u)
	fR = lf_flux(um,up,A)   
	fL = np.roll(fR,1)
	u = 1.0/3.0*u0 + 2.0/3.0*(u - dt/dx*(fR - fL))

	# increment time step
	u0 = u
	t = t+dt
</d-code>

<p>Aside from having the three stages for the Runge-Kutta time-stepping, the only real addition to the code is the polynomial reconstruction using the cell averages. Using \eqref{eq:recon1} and \eqref{eq:recon2}, the polynomial reconstruction is easily implemented with the following lines:</p>
<d-code block="" language="python">
def polynomial_reconstruction(u):

	# compute u_{j+1/2}^{-} and u_{j+1/2}^{+}
	um = -1.0/6.0*np.roll(u,1) + 5.0/6.0*u + 1.0/3.0*np.roll(u,-1)
	up = 1.0/3.0*u + 5.0/6.0*np.roll(u,-1) - 1.0/6.0*np.roll(u,-2)

	return um, up
</d-code>

<p>Figure 3 shows a complete simulation of Burgers equation up to \(T=2\) using this scheme.</p>

<div class="container">
  <div class="row">
    <div class="col">
    </div>
    <div class="col-10">
      <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/conslaw2/fv3bad.gif-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/conslaw2/fv3bad.gif-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/conslaw2/fv3bad.gif-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/conslaw2/fv3bad.gif" class="img rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

      <div class="caption">
        <b>Figure 3:</b> The third order finite volume scheme applied to Burgers equation.
      </div> 
    </div>
    <div class="col">
    </div>
  </div>
</div>

<p>Yikes. On first glance, it appears our scheme performs reasonably well before the appearance of the shock but develops spurious oscillations after the shock. We can also compute the errors in the \(\ell^{1}\) norm:</p>

\[||\bar{u} - \bar{u}_{e}||_{\ell^{1}} = \frac{1}{N} \sum_{j=1}^{N} |\bar{u}_{j} - \bar{u}_{e,j}|.\]

<p>Figure 4 describes how the \(\ell^{1}\) error decreases as we increase the number of grid points. Before the appearance of the shock, our scheme displays third order accuracy as expected. But after the appearance of the shock, the accuracy is reduced to first order.</p>

<div class="container">
  <div class="row">
    <div class="col">
    </div>
    <div class="col-10">
      <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/conslaw2/convergencefv3bad-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/conslaw2/convergencefv3bad-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/conslaw2/convergencefv3bad-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/conslaw2/convergencefv3bad.png" class="img rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

      <div class="caption">
        <b>Figure 4:</b> Errors in the $\ell^{1}$ norm of the third order finite volume scheme before and after the formation of the shock in Burgers equation.
      </div> 
    </div>
    <div class="col">
    </div>
  </div>
</div>

<p>So, what on earth is going on here? The driving idea behind higher order schemes is that we hope to converge to the exact (entropy) solution faster than the original first order scheme. In the presence of shocks and discontinuities, though, it seems as if our third-order scheme hasn’t given us any benefits. Indeed, it actually introduces oscillations, which is arguably worse than the first order scheme which we recall was provably monotone and TVD.</p>

<h1 id="2-godunovs-theorem"><strong>2. Godunov’s Theorem</strong></h1>

<p>Our initial results are actually not altogether unexpected and are actually a result of Godunov’s Theorem.</p>

<p><strong>Definition 3.</strong> <em>A linear scheme is one that can be written in the form</em></p>

\[\begin{align}
	\bar{u}_{j}^{n+1} = \sum_{\ell=-k}^{k} c_{\ell}(\lambda) \bar{u}_{j+\ell}^{n}, \;\;\;\lambda = \frac{\Delta t}{\Delta x}\notag
\end{align}\]

<p><em>when applied to a linear conservation law.</em></p>

<p>It’s worth noting that all of the finite volume schemes we have considered thus far are linear schemes (a fact which can easily be verified with pen and paper).</p>

<p><strong>Theorem 1. (Godunov)</strong> <em>A linear scheme is monotone if and only if it is total variation diminishing (TVD). Moreover, linear TVD schemes are at most first-order accurate.</em></p>

<p>There’s quite a bit to digest here, but we’ll start with the first statement. Recall from last time that monotone schemes converge to the entropy solution and are also TVD. Godunov’s Theorem states that for <em>linear</em> schemes, the notion of being monotone is equivalent to being TVD. The second statement is much deeper: it tells us that if we want to construct a linear monotone scheme, then it must be first-order accurate. In particular, we posit that the third-order finite volume scheme is neither monotone nor TVD.</p>

<p>In fact, monotone schemes are at most first-order accurate regardless of whether the scheme is linear or not. This suggests that we simply can’t develop high order monotone schemes. However, having only a TVD scheme is still desirable as it essentially guarantees our scheme will not generate spurious oscillations. And yet, Godunov’s Theorem tells us that if we stick to linear schemes, we can’t develop high order TVD schemes either.</p>

<p>Nevertheless, it is still possible to enforce the TVD property on the third-order finite volume scheme. To do so, we employ a post-processing step known as <em>slope limiting</em>. This results in a nonlinear TVD scheme which is does not produce spurious oscillations.</p>

<h1 id="3-slope-limiting"><strong>3. Slope Limiting</strong></h1>

<h2 id="i-generalized-muscl-limiter"><strong>I. Generalized MUSCL Limiter</strong></h2>
<p>The key observation is that when nonphysical oscillations appear in the numerical solution, the gradients of the cell averages between successive cells rapidly change sign. Slope limiting procedures identify where these sign changes occur and reduce the gradient to zero in these regions. The procedure is as follows.</p>

<ol>
  <li>Given \(\{u_{j+1/ 2}^{\pm}\}\) and the cell averages \(\{\bar{u}_{j}\}\), define</li>
</ol>

\[\begin{align}
	\tilde{u}_{j} = u_{j+1/ 2}^{-} - \bar{u}_{j}, \;\;\;\;\;\tilde{\tilde{u}}_{j} = \bar{u}_{j} - u_{j-1/ 2}^{+}.\notag
\end{align}\]

<ol>
  <li>Compute the modified quantities \(\tilde{u}_{j}^{\text{mod}}\) and \(\tilde{\tilde{u}}_{j}^{\text{mod}}\) according to
\(\begin{align}
 \tilde{u}_{j}^{\text{mod}} &amp;= \text{minmod}\{\tilde{u}_{j}, \bar{u}_{j+1} - \bar{u}_{j}, \bar{u}_{j} - \bar{u}_{j-1}\}\\
 \tilde{\tilde{u}}_{j}^{\text{mod}} &amp;= \text{minmod}\{\tilde{\tilde{u}}_{j}, \bar{u}_{j+1} - \bar{u}_{j}, \bar{u}_{j} - \bar{u}_{j-1}\},
\end{align}\)</li>
</ol>

<p>where the \(\text{minmod}\) function is defined as</p>

\[\begin{equation}
	\text{minmod}(a_{1},\dots,a_{n}) = \begin{cases}
		s \cdot \min_{j} |a_{j}|, &amp;s = \text{sgn}(a_{1}) = \dots = \text{sgn}(a_{n})\\
		0, &amp;\text{else}.
	\end{cases}\notag
\end{equation}\]

<ol>
  <li>Compute the modified reconstructed values \(u_{j+1/ 2}^{\pm,\text{mod}}\) according to</li>
</ol>

\[\begin{align}
	u_{j+1/ 2}^{-,\text{mod}} = \bar{u}_{j} + \tilde{u}_{j}^{\text{mod}}, \;\;\;\;\;u_{j-1/ 2}^{+,\text{mod}} = \bar{u}_{j} - \tilde{\tilde{u}}_{j}^{\text{mod}}.\notag
\end{align}\]

<p>The procedure outlined above is known as the <strong>generalized MUSCL (monotone upwind scheme for conservation laws) limiter.</strong> Note that the quantities \(\bar{u}_{j+1} - \bar{u}_{j}\) and \(\bar{u}_{j} - \bar{u}_{j-1}\) are crude approximations to the gradient over stencil \(\{I_{j}, I_{j+1}\}\) and \(\{I_{j-1}, I_{j}\}\). In the case of oscillations in our numerical solution, these two quantities will have opposite signs, in which case the \(\text{minmod}\) function returns $0$ and the modified quantities \(u_{j+1/ 2}^{\pm,\text{mod}}\) are set to the cell average. This is equivalent to the original first order finite volume scheme we studied last time! Hence, in the presence of shocks and discontinuities, we might expect the generalized MUSCL scheme to be first order accurate.</p>

<p>Now, let’s consider the case when the $\text{minmod}$ function returns the first argument. We carefully note that this situation occurs when \(\{\bar{u}_{j}\}\) is monotone over the stencils \(\{I_{j-1}, I_{j}, I_{j+1}\}\). Then we have \(u_{j+1/ 2}^{\pm,\text{mod}} = u_{j+1/ 2}^{\pm}\) and the high order finite volume scheme is unchanged. Thus, in <em>smooth, monotone regions</em> the generalized MUSCL limiter maintains original high order accuracy of the scheme.</p>

<p>Lastly, we must consider what happens near smooth extrema. For instance, \(u(x,0) = \sin{x}\) has smooth extrema at \(x=\pi/2\) (maximum) and \(x=3\pi/2\) (minimum). As consecutive gradients have opposite sign at local extrema, the $\text{minmod}$ function will return \(0\) and the scheme is again reduced to first order. Stanley Osher proved the following result:</p>

<p><strong>Theorem 2.</strong> <em>TVD schemes are at most first-order accurate near smooth extrema.</em></p>

<p>This seems like a problem since plenty of smooth solutions have smooth extrema. We’ll discuss shortly how to develop slope limiters which do not have this drawback, but first, it would be prudent to discuss to discuss the properties of the generalized MUSCL limiter. When paired with this limiter, the finite volume scheme is actually TVD. The proof is due to Ami Harten [<a href="http://arrow.utias.utoronto.ca/~groth/aer1319/Handouts/Additional_Reading_Material/JCP-1983-harten.pdf">2</a>].</p>

<p><strong>Lemma 1. (Harten)</strong> <em>If a scheme can be written in the form</em></p>

\[\begin{equation}
	\bar{u}_{j}^{n+1} = \bar{u}_{j}^{n} + C_{j+1/ 2}(\bar{u}_{j+1}^{n} - \bar{u}_{j}^{n}) - D_{j-1/ 2}(\bar{u}_{j}^{n} - \bar{u}_{j-1}^{n})
\end{equation}\]

<p><em>where \(C_{j+1/ 2}, D_{j+1/ 2} \geqslant 0\) and \(C_{j+1/ 2} + D_{j+1/ 2} \leqslant 1\), then the scheme is TVD.</em></p>

<p>Harten’s Lemma is the key to proving that the finite volume MUSCL scheme is TVD.</p>

<p><strong>Proposition 1.</strong> <em>The finite volume scheme with generalized MUSCL limiter is TVD.</em></p>

<p>Without further ado, we present the results using the MUSCL limiter. The only change from the previous third-order code is to add the MUSCL limiter:</p>
<d-code block="" language="python">
def muscl_limiter(um, up, u):

	# gradients to be adjusted
	ut  = um - u
	utt = u - np.roll(up,1)

	ut_m = minmod(ut, np.roll(u,-1)-u, u-np.roll(u,1))
	utt_m = minmod(utt, np.roll(u,-1)-u, u-np.roll(u,1))

	# modify the cell reconstructions using ut and utt
	um_mod = u + ut_m
	up_mod = u - utt_m
	up_mod = np.roll(up_mod,-1)

	return um_mod, up_mod
</d-code>

<p>The \(\text{minmod}\) function is easily implemented as follows:</p>
<d-code block="" language="python">
def minmod(a, b, c):
	
	# check whether a, b, and c are the same sign
	signs = ((np.sign(a)==np.sign(b)) &amp; (np.sign(b)==np.sign(c)) &amp; (np.sign(c)==np.sign(a)))

	# compute minimum magnitudes of {a,b,c}
	vals = np.concatenate((a,b,c), axis=1)
	mins = np.amin(np.abs(vals), axis=1)

	# compute the minmod
	m = signs*np.sign(a)*np.reshape(mins,(len(vals),1))

	return m
</d-code>

<p>Figure 5 contains the results. Voila! At first glance, all appears to be well. The nonphysical oscillations have been removed entirely from our numerical solution.</p>

<div class="container">
  <div class="row">
    <div class="col">
    </div>
    <div class="col-10">
      <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/conslaw2/fv3muscl.gif-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/conslaw2/fv3muscl.gif-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/conslaw2/fv3muscl.gif-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/conslaw2/fv3muscl.gif" class="img rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

      <div class="caption">
        <b>Figure 5:</b> The third order finite volume scheme with generalized MUSCL limiter applied to Burgers equation.
      </div> 
    </div>
    <div class="col">
    </div>
  </div>
</div>

<p>The next step is to verify that we are indeed getting the expected rates of convergence. There are three scenarios to consider: (i) the solution is smooth everywhere (we’ll consider the case when \(T=0.3\)) (ii) the solution develops a shock and we compute the \(\ell^{1}\) error over the entire computational grid (we’ll consider the case \(T=1.5\) here) (iii) the solution develops a shock and we compute the \(\ell^{1}\) error excluding a small region around the shock.</p>

<p>In the first scenario, we expect a reduced rate of convergence due to the presence of smooth extrema in the solution. In the second scenario, the order of convergence should be reduced to first order. Finally, in the third scenario, the solution is actually piecewise monotone and so we might expect to maintain the original third order accuracy if compute errors away from the shock.</p>

<p>Indeed, Figure 6 shows these three scenarios exactly. Before the shock, the overall order of accuracy is somewhere between 2 and 3; at the two extrema, the order is reduced to 1 but everywhere else, it is close to 3, leading to a suboptimal order of accuracy. After the shock, the order of accuracy is 1 overall, but when considering only cells at a distance 0.5 away from the shock at \(x=\pi\), the order of accuracy is 3.</p>

<p>This isn’t such a bad place to be, then. Near shocks, the mathematical theory tells us that we can <em>never</em> do better than first order accuracy, but we’ve managed to construct a scheme which is TVD and capable of giving higher order accuracy everywhere the solution is smooth. In the end, this is all we can really ask for. In the remaining posts, we will consider the different types of high order schemes which can be used to numerically solve nonlinear conservation laws.</p>

<div class="container">
  <div class="row">
    <div class="col">
    </div>
    <div class="col-10">
      <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/conslaw2/convergencefv3muscl-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/conslaw2/convergencefv3muscl-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/conslaw2/convergencefv3muscl-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/conslaw2/convergencefv3muscl.png" class="img rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

      <div class="caption">
        <b>Figure 6:</b> Error in the $\ell^{1}$ norm of the third order finite volume MUSCL scheme before the shock (blue line with squares), after the shock over the entire domain (red line with circles), and after the shock but excluding a small region around $x=\pi$ (red line with x's).
      </div> 
    </div>
    <div class="col">
    </div>
  </div>
</div>

<h2 id="ii-total-variation-bounded-tvb-limiter"><strong>II. Total-Variation Bounded (TVB) Limiter</strong></h2>

<p>But first, we consider how to remedy the issue of suboptimal convergence near <em>smooth</em> extrema. Recall that the issue here is that consecutive gradients have different signs and the \(\text{minmod}\) function return zero. In fact, recall that Theorem 2 tells us we can have at most first order accuracy near smooth extrema. If we relax the TVD constraint even further, it is possible to obtain uniformly high-order accurate schemes near smooth extrema.</p>

<p><strong>Definition 4.</strong> If \(TV(\bar{u}^{n}) \leqslant B\) for some fixed \(B&gt;0\) depending only on \(TV(\bar{u}^{0})\) and \(n\) and \(\Delta t\) such that \(n\Delta t &lt; T\), then the scheme is said to be <strong>total variation bounded</strong> (TVB) in \(0 \leqslant t \leqslant T\).</p>

<p>It is clear that all TVD schemes are TVB. For TVB schemes, one can show that there exists a subsequence in \(L^{1}_{\text{loc}}\) that converges to a weak solution of the conservation law.</p>

<p>In order to turn the generalized MUSCL limiter into a scheme that is TVB, we need only make a small adjustment. Namely, we define</p>

\[\begin{align}
	\tilde{u}_{j}^{\text{mod}} &amp;= \overline{\text{minmod}}\{\tilde{u}_{j}, \bar{u}_{j+1} - \bar{u}_{j}, \bar{u}_{j} - \bar{u}_{j-1}\}\notag\\
	\tilde{\tilde{u}}_{j}^{\text{mod}} &amp;= \overline{\text{minmod}}\{\tilde{\tilde{u}}_{j}, \bar{u}_{j+1} - \bar{u}_{j}, \bar{u}_{j} - \bar{u}_{j-1}\},\notag
\end{align}\]

<p>where the modified minmod function \(\overline{\text{minmod}}\) is defined by</p>

\[\begin{equation}
	\overline{\text{minmod}}(a_{1}, \dots, a_{n}) = \begin{cases}
		a_{1}, &amp;|a_{1}| \leqslant M\Delta x^{2}\\
		\text{minmod}(a_{1}, \dots, a_{n}) &amp;\text{else}.
	\end{cases}\notag
\end{equation}\]

<p>Here, \(M&gt;0\) is a parameter to be chosen by us. Essentially, the TVB limiter first checks whether \(u_{j+1/ 2}^{-}\) is sufficiently close to \(\bar{u}_{j}\). If so, then the modified $\text{minmod}$ function returns the first argument even if consecutive gradients have different signs, thus ensuring that the scheme retains high-order accuracy around smooth extrema [<a href="https://www.ams.org/journals/mcom/1987-49-179/S0025-5718-1987-0890256-5/S0025-5718-1987-0890256-5.pdf">4</a>]. The implementation is quite straightforward:</p>
<d-code block="" language="python">
def minmod2(a, b, c, dx, M):
	
	# check whether a, b, and c are the same sign
	signs = ((np.sign(a)==np.sign(b)) &amp; (np.sign(b)==np.sign(c)) &amp; (np.sign(c)==np.sign(a)))

	# compute minimum magnitudes of {a,b,c}
	vals = np.concatenate((a,b,c), axis=1)
	mins = np.amin(np.abs(vals), axis=1)

	# check whether first argument is sufficiently small
	a1 = (np.abs(a) &lt;= M*dx**2)
	a2 = (np.abs(a) &gt; M*dx**2)

	# compute the minmod
	m = a1*a + a2*signs*np.sign(a)*np.reshape(mins,(len(vals),1))

	return m
</d-code>

<p>We won’t show plots of the numerical solution here as they wouldn’t be discernible from the plots of the MUSCL limiter to the naked eye. Instead, Figure 7 shows the convergence rates for the TVB limiter. We note that before the shock forms, the order of accuracy is now 3 as expected, even in the presence of smooth extrema.</p>

<div class="container">
  <div class="row">
    <div class="col">
    </div>
    <div class="col-10">
      <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/conslaw2/convergencefv3tvb-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/conslaw2/convergencefv3tvb-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/conslaw2/convergencefv3tvb-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/conslaw2/convergencefv3tvb.png" class="img rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

      <div class="caption">
        <b>Figure 7:</b> Error in the $\ell^{1}$ norm of the third order finite volume TVB scheme before the shock (blue line with squares), after the shock over the entire domain (red line with circles), and after the shock but excluding a small region around $x=\pi$ (red line with x's).
      </div> 
    </div>
    <div class="col">
    </div>
  </div>
</div>

<h1 id="4-key-takeaways"><strong>4. Key Takeaways</strong></h1>
<p>To summarize, we implemented a third order finite volume scheme for a one-dimensional scalar conservation law. Without any modifications, this scheme was not TVD and introduced new extrema in our numerical solution that were not seen in the first order finite volume scheme. We touched on Godunov’s Theorem, which describes the fundamental limits of accuracy of linear monotone schemes.</p>

<p>We were able to develop a TVD slope limiter to remove the spurious oscillations in our high order numerical solution but noted that the limiter degraded the order of accuracy of our scheme near smooth extrema. In fact, we learned that <em>all</em> TVD schemes are at most first order near smooth extrema. To develop a truly high order scheme in the presence of smooth extrema, we turned to the TVB limiter.</p>

<p>It is worth emphasizing once more than when we talk about “high order” schemes for nonlinear conservation laws, we generally mean that the scheme maintains high order accuracy in smooth regions, is first order accurate in the vicinity of shocks and discontinuities, and is total variation diminishing (does not introduce any new extrema/spurious oscillations). Next time, we will cover essentially non-oscillatory finite volume schemes which essentially remove spurious oscillations by performing the polynomial reconstruction step in a clever way rather than post-processing the reconstruction as with the slope limiters we have seen in this post.</p>

<h1 id="5-references"><strong>5. References</strong></h1>
<p>[1] Gottlieb, Sigal, and Chi-Wang Shu. “<a href="https://doi.org/10.1090/S0025-5718-98-00913-2">Total variation diminishing Runge-Kutta schemes</a>.” Mathematics of computation of the American Mathematical Society 67.221 (1998): 73-85.</p>

<p>[2] Harten, Ami. “<a href="http://arrow.utias.utoronto.ca/~groth/aer1319/Handouts/Additional_Reading_Material/JCP-1983-harten.pdf">High resolution schemes for hyperbolic conservation laws</a>.” Journal of computational physics 49.3 (1983): 357-393.</p>

<p>[3] Johnson, B. Cockburn C., and C-W. Shu E. Tadmor. “<a href="https://doi.org/10.1007/BFb0096351">Advanced numerical approximation of nonlinear hyperbolic equations</a>.” (1997).</p>

<p>[4] Shu, Chi-Wang. “TVB uniformly high-order schemes for conservation laws.” Mathematics of Computation 49.179 (1987): 105-121.</p>

<h1 id="6-codes--resources"><strong>6. Codes &amp; Resources</strong></h1>
<ul>
  <li><a href="https://github.com/jdongg/numCL">NumCL repository on GitHub</a></li>
  <li>Figures 1 and 2 were created using <a href="https://www.mathcha.io/">Mathcha</a>, an online math editor with a convenient GUI that even allows you to export figures to TikZ.</li>
</ul>]]></content><author><name>Justin Dong</name></author><category term="numerical-methods" /><category term="finite-volume-method," /><category term="hyperbolic-conservation-laws" /><summary type="html"><![CDATA[1. High Order Finite Volume Methods]]></summary></entry><entry><title type="html">Numerical methods for conservation laws – introduction to finite volume methods (part 1)</title><link href="http://localhost:4000/blog/2019/conservationlaws1/" rel="alternate" type="text/html" title="Numerical methods for conservation laws – introduction to finite volume methods (part 1)" /><published>2019-05-15T11:12:00-04:00</published><updated>2019-05-15T11:12:00-04:00</updated><id>http://localhost:4000/blog/2019/conservationlaws1</id><content type="html" xml:base="http://localhost:4000/blog/2019/conservationlaws1/"><![CDATA[<h1 id="1-conservation-laws"><strong>1. Conservation Laws</strong></h1>

<p>In this post, we’ll take a look at conservation laws, the contexts in which they arise in nature, and some of the numerical methods used for solving them. In one spatial-dimension, conservation laws take the general form</p>

\[\begin{equation}
  \begin{cases}
    u_{t} + f(u)_{x} = 0 &amp;\text{in}\;\mathbb{R} \times [0,\infty)\\
    u(x,0) = u_{0}(x) &amp;\text{on}\;\mathbb{R} \times \{t=0\}.
  \end{cases}
  \label{eq:conslaw}
\end{equation}\]

<p>Here, we consider the spatial domain to be the entire real line (thus, we have no boundary conditions). The function \(f(u)\) – typically called the <em>flux function</em> – is sufficiently smooth. As their name suggests, conservation laws preserve mass. We often think of the quantity \(u\) as the density of some fluid, in which case \(\int_{\mathbb{R}} u(x,t)dx\) may be viewed as the mass of fluid. If we assume that \(u\) is compactly supported in \(x\) (i.e. it is zero outside of some compact set in \(x\), a reasonable assumption as the fluid should have finite mass) and \(f(0) = 0\), then taking derivatives over time yields</p>

\[\begin{align}
  \frac{\partial}{\partial t} \int_{-\infty}^{\infty} u(x,t)\;dx &amp;= \int_{-\infty}^{\infty} \frac{\partial u}{\partial t}\;dx = -\int_{-\infty}^{\infty} \frac{\partial f(u)}{\partial x}\;dx\notag\\
  &amp;= f(u(-\infty)) - f(u(\infty)) = 0.\notag
\end{align}\]

<p>The above computation implies that \(\int_{\mathbb{R}} u(x,t)\;dx = \int_{\mathbb{R}} u(x,0)\;dx\): the mass of \(u\) is conserved over time.</p>

<p>To gain some intuition for how solutions of this PDE might behave, we’ll first consider the simplest conservation law there is: the linear advection equation. Taking \(f(u) = u\), it is evident that our solution is given by \(u(x,t) = u_{0}(x-t)\). The PDE “transports” the initial profile in the \(x\) direction. In particular, we note that the solution to the <em>linear</em> conservation law is exactly as smooth as our initial data. That is, if \(u_{0} \in C^{1}_{x}(\mathbb{R})\) then \(u \in C_{x}^{1}(\mathbb{R})\) as well. We haven’t said anything particularly illuminating thus far and indeed, the linear advection equation isn’t particularly interesting to begin with.</p>

<p>Next, let’s consider a simple nonlinear conservation law:</p>

\[\begin{align}
\begin{cases}
  u_{t} + uu_{x} = 0 &amp;\text{in}\;\mathbb{R} \times [0,\infty)\\
  u(x,0) = \sin{x} &amp;\text{on}\;\mathbb{R} \times \{t=0\}.
\end{cases}
\end{align}\]

<p>The initial condition is as smooth as possible now: \(\sin{x} \in C_{x}^{\infty}(\mathbb{R})\). But what can we say about \(u(x,t)\)? Should we still expect it to be smooth in \(x\)? It turns out that the answer is a resounding <em>no</em>. In fact, solutions to the above nonlinear PDE (known as <em>Burgers equation</em>) are not even \(C_{x}^{0}(\mathbb{R})\)! The exact mathematical cause of this phenomena is due to the crossing of something called characteristic curves, but we will first consider a more intuitive explanation.</p>

<p>We may view Burgers equation as an advection equation in which the speed of propagation is equal to \(u\) itself. Our initial sine profile thus moves with variable speed in \(x\). For \(0 \leqslant x \leqslant \pi\), the sine wave is positive and the solution travels forward. For \(\pi &lt; x \leqslant 2\pi\), the sine wave is negative and the solution travels backwards. The animation below shows how the solution of Burgers equation evolves over time, and we see the development of a genuine discontinuity at \(x=\pi\).</p>

<div class="container">
  <div class="row">
    <div class="col">
    </div>
    <div class="col-10">
      <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/conslaw1/burgers.gif-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/conslaw1/burgers.gif-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/conslaw1/burgers.gif-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/conslaw1/burgers.gif" class="img rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

      <div class="caption">
        <b>Figure 1:</b> Solution of Burgers equation with sine initial data.
      </div> 
    </div>
    <div class="col">
    </div>
  </div>
</div>

<p>Shocks such as the one above occur naturally in many settings, for instance high-speed compressible flows and aeroacoustics. However, a natural question we might ask next is how discontinuities and shocks fit with our notion of the classical solution of a PDE. For instance, the linear advection equation contains a first-order time derivative and a first-order spatial derivative. Naturally, we should require that \(u \in C_{x,t}^{1}(\mathbb{R} \times [0,\infty))\) – our solution should be once differentiable in space and time. Consider, though, the linear advection equation with \(u(x,0) = 𝟙_{\\{x \leqslant 0\\}}\). You’ll probably agree with me that the only possible solution to this problem is \(u(x,t) = 𝟙_{\\{x \leqslant t\\}}\), and yet this “solution” is not even continuous let alone differentiable. Before moving on to numerical solvers, we will spend some time developing alternative notions of PDE solutions that are capable of admitting discontinuities and shocks.</p>

<h2 id="i-weak-solutions-of-conservation-laws">I. Weak Solutions of Conservation Laws</h2>
<p><strong>Definition 1.</strong> <em>We call \(u(x,t)\) a</em> <strong>weak solution</strong> <em>of the conservation law \eqref{eq:conslaw} if</em></p>

\[\begin{equation} 
  \int_{0}^{\infty}\int_{-\infty}^{\infty} u\varphi_{t} + f(u)\varphi_{x}\;dxdt = -\int_{-\infty}^{\infty} u(x,0)\varphi(x,0)\;dx
  \label{eq:weaksol}
\end{equation}\]

<p><em>for all \(\varphi \in C_{c}^{\infty}(\mathbb{R} \times \mathbb{R})\).</em></p>

<p>We refer to \(\varphi\) as a test function. It is smooth in both space and time and compactly supported, i.e. \(\varphi\) is zero outside of some compact set in space and time. Let’s break down what’s actually going on in the definition of the weak solution. The most notable feature is that if $u$ is a weak solution, it does <em>not</em> have to be differentiable in space or time since the definition only contains derivatives of \(\varphi\)!</p>

<p>So how do we arrive at this definition? First, we multiply the entire conservation law by \(\varphi\):
\begin{align}
  u_{t}\varphi + f(u)_{x}\varphi = 0.\notag
\end{align}</p>

<p>Then, we integrate in time over $[0,\infty)$ and in space over the entire real line:
\begin{align}
  \int_{0}^{\infty}\int_{-\infty}^{\infty} u_{t}\varphi + f(u)_{x}\varphi\;dxdt = 0.\notag
\end{align}</p>

<p>The last step is to integrate the first term by parts in time and the second term by parts in space:</p>

\[\begin{align}
  \int_{0}^{\infty} \int_{-\infty}^{\infty} u_{t}\varphi dxdt &amp;= -\int_{0}^{\infty} \int_{-\infty}^{\infty} u\varphi_{t}dxdt + \int_{-\infty}^{\infty} u(x,0)\varphi(x,0)dx\notag\\
  \int_{0}^{\infty} \int_{-\infty}^{\infty} f(u)_{x}\varphi\;dxdt &amp;= -\int_{0}^{\infty} \int_{-\infty}^{\infty} f(u)\varphi_{x}\;dxdt
\end{align}\]

<p>Most of the boundary terms vanish because \(\varphi\) has compact support: we have \(\varphi(\pm \infty,t) = \varphi(x,\pm\infty) = 0\). Note that in integrating by parts, we pass the derivatives from \(u\) onto the test function. Altogether, we obtain
\begin{align}
\int_{0}^{\infty}\int_{-\infty}^{\infty} u\varphi_{t} + f(u)\varphi_{x}\;dxdt = -\int_{-\infty}^{\infty} u(x,0)\varphi(x,0)\;dx.\notag
\end{align}</p>

<p>But <em>why</em> do we do this? The idea is that rather than consider pointwise values of \(u(x,t)\), we consider averaged values of \(u(x,t)\) against test functions. If you’ve taken a functional analysis course, you can view the integration against a test function as the evaluation of a distribution induced by \(u_{t}\) and \(f(u)_{x}\). We note that all classical solutions (\(C_{x,t}^{1}\) solutions that satisfy the PDE in a pointwise sense) are weak solutions but the converse is certainly not true.</p>

<p>It is straightforward to verify that \(𝟙_{\\{x\leqslant t\\}}\) is a weak solution of the linear advection equation with \(u(x,0) = 𝟙_{\\{x \leqslant 0\\}}\). However, the definition \eqref{eq:weaksol} of quite cumbersome to work with and we would like to develop a more convenient way to verify whether we have a weak solution or not. Notice that \(u(x,t) = 𝟙_{\\{x\leqslant t\\}}\) as well as the solution of Burgers equation in Figure 1 are piecewise smooth and only discontinuous at a single point that may (in the case of linear advection) or may not (in the case of Burgers equation) change with time. In this case, we can say much more about the structure of the weak solution.</p>

<p><strong>Theorem 1. (Rankine-Hugoniot)</strong> <em>Suppose the solution of \eqref{eq:conslaw} is piecewise smooth and contains a discontinuity along the curve \(x(t)\). Then \(u\) is a weak solution of \eqref{eq:conslaw} if and only if</em>
\begin{equation}
  x’(t) = \frac{f(u^{-}) - f(u^{+})}{u^{-} - u^{+}},
  \label{eq:RH}
\end{equation}</p>

<p><em>where \(x'(t)\) is the speed of the discontinuity and \(u^{\pm}\) are the values of \(u\) along each side of the shock.</em></p>

<p>Condition \eqref{eq:RH} is referred to as the <strong>Rankine-Hugoniot jump condition</strong>.</p>

<p><strong>Example 1.</strong> <em>Let’s return to the linear advection equation and use the jump condition to verify that \(𝟙_{\\{x\leqslant t\\}}\) is a weak solution. We have \(x'(t) = 1\), \(u^{-} = 1\), and \(u^{+} = 0\). Then</em>
\begin{align}
  \frac{f(u^{-}) - f(u^{+})}{u^{-} - u^{+}} = \frac{u^{-} - u^{+}}{u^{-} - u^{+}} = 1\notag
\end{align}
<em>and the jump condition is satisfied.</em></p>

<p>Thus, the Rankine-Hugoniot condition is a handy way for us to verify whether something is a weak solution. It turns out that solutions to conservation laws often have the structure required by Theorem 1 – that is, they are piecewise smooth except along finitely many curves of discontinuity.</p>

<p>By relaxing the definition of the solution, we are able to admit much less smooth solutions to \eqref{eq:conslaw}. However, we will see weak solutions need not be unique! Consider Burgers equation with the initial condition \(u(x,0) = -𝟙_{\\{x\leqslant 0\\}} + 𝟙_{\\{x&gt;0\\}}\). It is straightforward to verify that \(u(x,t) = -𝟙_{\\{x\leqslant 0\\}} + 𝟙_{\\{x&gt;0\\}}\) as well as</p>

\[\begin{align}
u(x,t) = \begin{cases}
  -1 &amp;x \leqslant -t\\
  \frac{x}{t} &amp;-t &lt; x \leqslant t\\
  1 &amp;x &gt; t
  \end{cases}
\end{align}\]

<p>are both weak solutions. The first weak solution does not change at all with time, so our intuition might tell us that this solution does not make much physical sense. But what about the second weak solution? At the very least, we need to establish more stringent criteria for our weak solutions in order to pick out the physically relevant solution.</p>

<h2 id="ii-entropy-solutions-of-conservation-laws">II. Entropy Solutions of Conservation Laws</h2>

<p>Consider the slightly modified PDE given by</p>

\[\begin{equation}
  \begin{cases}
    u^{(\varepsilon)}_{t} + f(u^{(\varepsilon)})_{x} = \varepsilon u^{(\varepsilon)}_{xx} &amp;\text{in}\;\mathbb{R} \times (0,\infty)\\
    u^{(\varepsilon)}(x,0) = u_{0}(x) &amp;\text{on}\;\mathbb{R} \times \{t=0\}.
    \label{eq:viscosityPDE}
  \end{cases}
\end{equation}\]

<p>This PDE is <em>parabolic</em> and it turns out that the solution \(u^{(\varepsilon)}\), if it exists, is smooth and unique.</p>

<p><strong>Definition 2.</strong> <em>The entropy solution of \eqref{eq:conslaw} is defined as</em>
\begin{align}
u(x,t) = \lim_{\varepsilon \to 0} u^{(\varepsilon)}(x,t).\notag
\end{align}
<em>This limit, if it exists, is unique and is a weak solution of  \eqref{eq:conslaw}.</em></p>

<p>Again, this definition is quite difficult to work with, even moreso than the original definition of the weak solution, and we need to establish a more efficient way of verifying whether we have the entropy solution or not. There are alternative definitions based on the notions of <em>entropy flux</em> and <em>entropy flux functions</em>, but we’ll skip right to the chase here. Thanks to the work of Oleinik and Lax, we have a quick way to identify entropy solutions.</p>

<p><strong>Theorem 2. (Oleinik Entropy Condition)</strong> <em>Suppose the solution of \eqref{eq:conslaw} is piecewise smooth and contains a discontinuity along the curve \(x(t)\). Then \(u\) is the entropy solution if and only if</em>
\begin{equation}
  \frac{f(u) - f(u^{-})}{u - u^{-}} \geqslant \frac{f(u^{+}) - f(u^{-})}{u^{+} - u^{-}} \geqslant \frac{f(u^{+}) - f(u)}{u^{+} - u}
  \label{eq:oleinik}
\end{equation}
for all \(u\) between \(u^{-}\) and \(u^{+}\).*</p>

<p>Again, \(u^{-}\) and \(u^{+}\) denote the values of $u$ on each side of the shock \(x(t)\). However, we can simplify \eqref{eq:oleinik} even further if we have a <em>convex</em> conservation law, which is simply the case when \(f(u)\) is convex. For Burgers equation, we have \(f(u) = u^{2}/2\), which is indeed convex. The Lax Entropy condition tells us the following.</p>

<p><strong>Theorem 3. (Lax Entropy Condition)</strong> <em>Suppose the solution of \eqref{eq:conslaw} is piecewise smooth and contains a discontinuity along the curve \(x(t)\). Suppose also that \(f\) is convex. Then \(u\) is the entropy solution if and only if
\begin{equation}
  f’(u^{-}) \geqslant \frac{f(u^{+}) - f(u^{-})}{u^{+} - u^{-}} \geqslant f’(u^{+}).
  \label{eq:lax}
\end{equation}
In particular, \eqref{eq:lax} is equivalent to requiring that \(u^{-} &gt; u^{+}\).</em></p>

<p>The Lax Entropy condition tells us that for convex conservation laws, the value of the weak solution on the left side of the shock ($u^{-}$) must be larger than the value of the solution on the right side of the shock ($u^{+}$) in order for the weak solution to be an entropy solution. The physical intuition behind the entropy solution is related to the entropy of a fluid in gas dynamics: in smooth flows, the entropy remains constant along particle paths, and if the particle crosses a shock, the entropy may only jump to a <em>higher</em> value. Entropy is inversely proportional to density, and so the density $u$ can only jump to lower values along a shock.</p>

<p>We’ve spent a lot of time now developing the main ideas of weak solutions for conservation laws. We can concretely define what it means to have discontinuous solutions of \eqref{eq:conslaw} and impose conditions to guarantee the uniqueness of this solution. Great. We can finally move on to numerical methods for solving conservation laws. In developing such schemes, our goal will be to ensure that our scheme converges (in some sense) to the entropy solution.</p>

<h1 id="2-monotone-schemes"><strong>2. Monotone Schemes</strong></h1>
<p>First, a cautionary tale in constructing numerical methods for conservation laws. We consider Burgers equation with the initial condition \(u(x,0) = 𝟙_{\\{x \geqslant 0\\}}\) and the following finite difference scheme:
\begin{align}
  u_{j}^{n+1} = u_{j}^{n} - \frac{\Delta t}{\Delta x} u_{j}^{n}(u_{j}^{n} - u_{j-1}^{n}).\notag
\end{align}</p>

<p>Initially, we have \(u_{j}^{0} = 𝟙_{\\{x_{j} \geqslant 0\\}}\). However, this scheme returns \(u_{j}^{n} = u_{j}^{0}\) for all \(n\) and $j$, so the scheme converges to \(u(x,t) = u(x,0)\). But in the preceding sections, we established that this can’t even be a weak solution (indeed, the Rankine-Hugoniot condition is not satisfied).</p>

<p>Clearly, we must exercise caution in constructing schemes to solve \eqref{eq:conslaw}. We have just seen that a perfectly reasonable-looking finite difference scheme (at least, at first glance) fails to converge to a weak solution, let alone the correct entropy solution. In other cases, it may be possible that a scheme converges to a weak solution but not an entropy solution. We will circle back to this idea shortly.</p>

<h2 id="i-the-finite-volume-method">I. The Finite Volume Method</h2>
<p>We begin by introducing the <strong>finite volume method</strong>, which discretizes the spatial domain into cells (intervals in 1D, rectangles or other polygons in 2D), and computes an approximation to the average of the solution in each cell. The precise formulation is as follows: consider a bounded, open, connected domain \(\Omega = (a,b) \subset \mathbb{R}\) and a finite stopping time \(T\), and consider the conservation law given by
\(\begin{equation}
  \begin{cases}
    u_{t} + f(u)_{x} = 0 &amp;\text{in}\;\Omega \times (0,T)\\
    u(x,0) = u_{0}(x) &amp;\text{on}\;\Omega \times \{t=0\}.
  \end{cases}
  \label{eq:conslaw2}
\end{equation}\)</p>

<p>We discretize \(\Omega\) into equally-sized cells \(I_{j} = (x_{j-1/ 2}, x_{j+1/ 2})\):
\begin{align}
  a = x_{1/ 2} &lt; x_{3/ 2} &lt; \dots &lt; x_{N+1/ 2} = b, \;\;\;\Delta x := x_{j+1/ 2} - x_{j-1/ 2}.\notag
\end{align}</p>

<p>Next, we integrate \eqref{eq:conslaw2} over each cell \(I_{j}\):</p>

\[\begin{align}
  \int_{x_{j-1/ 2}}^{x_{j+1/ 2}} (u_{t} + f(u)_{x})\;dx &amp;= 0\notag\\
  \Delta x \frac{d\bar{u}_{j}}{dt} + f(u_{j+1/ 2}^{-}) - f(u_{j-1/ 2}^{+}) &amp;= 0\notag\\
  \frac{d\bar{u}_{j}}{dt} + \frac{f(u_{j+1/ 2}) - f(u_{j-1/ 2})}{\Delta x} &amp;= 0\notag
\end{align}\]

<p>Here, \(\bar{u}_{j}\) denotes the cell average of \(u\) in cell \(I_{j}\): \(\bar{u}_{j} := \frac{1}{\Delta x}\int_{I_{j}} u\;dx\). If our goal is to solve for the cell averages \(\bar{u}_{j}\), you might notice that the second term poses a problem as it is formulated in terms of pointwise values of \(u\) at \(x_{j-1/ 2}\) and \(x_{j+1/ 2}\) rather than \(\bar{u}_{j}\). In the first finite volume scheme we consider, we will approximate \(u\) in \(I_{j} = (x_{j-1/ 2}, x_{j+1/ 2})\) by the cell average \(\bar{u}_{j}\) in \(I_{j}\).</p>

<p>This approximation is not so well defined as the points \(x_{j+1/ 2}\), which lie at the intersection between the two cells \(I_{j}\) and \(I_{j+1}\). The finite volume scheme remedies this by introducing a <em>numerical flux function</em> which takes as inputs cell averages and returns an approximation of the flux function at the cell endpoints \(x_{j+1/ 2}\), i.e. \(f(u_{j+1/ 2}) \approx \hat{f}(\bar{u}_{j}, \bar{u}_{j+1})\) and \(f(u_{j-1/ 2}) \approx \hat{f}(\bar{u}_{j-1}, \bar{u}_{j})\):</p>

\[\begin{equation}
  \frac{d\bar{u}_{j}}{dt} + \frac{1}{\Delta x}\left(\hat{f}(\bar{u}_{j}, \bar{u}_{j+1}) - \hat{f}(\bar{u}_{j-1}, \bar{u}_{j})\right) = 0.
  \label{eq:finitevol}
\end{equation}\]

<p>Thus, \(\hat{f}\) can be viewed in this context as a function which takes in the dual values at the cell interfaces and returns a single, physically relevant value. This will be important when we try to develop higher order finite volume schemes later.</p>

<p>For now, the only thing left to do to turn \eqref{eq:finitevol} into a usable scheme is to discretize the time derivative. The simplest option is to use a first-order finite difference. In doing so, we arrive at the explicit first-order finite volume scheme.</p>

<p><strong>Definition 3.</strong> <em>The explicit first-order finite volume scheme in space and time is given by</em>
\(\begin{equation}
  \bar{u}_{j}^{n+1} = \bar{u}_{j}^{n} - \frac{\Delta t}{\Delta x}\left( \hat{f}(\bar{u}_{j}^{n}, \bar{u}_{j+1}^{n}) - \hat{f}(\bar{u}_{j-1}^{n}, \bar{u}_{j}^{n}) \right),
  \label{eq:finitevol2}
\end{equation}\)</p>

<p><em>where \(\bar{u}_{j}^{n}\) denotes the cell averages at time \(t^{n}\) and \(\bar{u}_{j}^{n+1}\) the cell averages at time \(t^{n+1} = t^{n} + \Delta t\).</em></p>

<p>Notice that we haven’t specified what, exactly, the flux function \(\hat{f}\) is yet. The choice of \(\hat{f}\) is immensely important as it will determine many properties of our scheme, such as convergence to the entropy solution. Before stating some possible choices of \(\hat{f}\), we will first cover the properties \(\hat{f}\) must satisfy in order to converge to the entropy solution.</p>

<p><strong>Definition 4.</strong> <em>A</em> <strong>monotone scheme</strong> <em>is one that can be written in the form</em>
\(\begin{equation}
  \bar{u}_{j}^{n+1} = \mathcal{H}(\bar{u}_{j-p}^{n}, \dots, \bar{u}_{j+q}^{n}),
\end{equation}\)</p>

<p><em>where \(\mathcal{H}\) is a nondecreasing function in all arguments. In particular, the monotone scheme on the three-point stencil \(\{\bar{u}_{j-1}^{n}, \bar{u}_{j}^{n}, \bar{u}_{j+1}^{n}\}\) is given by \(\bar{u}_{j}^{n+1} = \mathcal{H}(\bar{u}_{j-1}^{n}, \bar{u}_{j}^{n}, \bar{u}_{j+1}^{n})\).</em></p>

<p>This is all well and good, but why should we care about monotone schemes? To begin with, they have a number of remarkable properties that guarantee our numerical solution behaves “nicely.”</p>

<p><strong>Theorem 4.</strong> <em>Monotone schemes satisfy the following properties:</em></p>

<ol>
  <li>
    <p><strong>Local maximum principle.</strong>
\(\begin{align}
  \min_{j-p \leqslant i \leqslant j+q} \bar{u}_{i}^{n} \leqslant \mathcal{H}(\bar{u})_{j} \leqslant \max_{j-p \leqslant i \leqslant j+q} \bar{u}_{i}^{n} \;\;\;\forall j.\notag
\end{align}\)</p>
  </li>
  <li>
    <p><strong>Total variation diminishing property.</strong>
\(\begin{align}
  ||\mathcal{H}(\bar{u})||_{BV} \leqslant ||\bar{u}||_{BV},\notag
\end{align}\)
<em>where the bounded variation seminorm is given by \(||u||_{BV} = \sum_{j} |u_{j} - u_{j-1}|\).</em></p>
  </li>
  <li>
    <p><strong>Entropy solution.</strong> <em>Monotone schemes converge to the entropy solution with a rate in \(\ell^{1}\) of half-order. This bound is sharp.</em></p>
  </li>
</ol>

<p>Roughly speaking, the local maximum principle tells us that our cell averages at \(t^{n+1}\) cannot be larger or smaller than our averages at \(t^{n}\) around the stencil \(\{\bar{u}_{j-p}^{n}, \dots, \bar{u}_{j+q}^{n}\}\). The total variation diminishing property tells us that our cell averages can only decrease in bounded variation. In particular, this means that our cell averages at \(t^{n+1}\) cannot be more oscillatory than our cell averages at \(t^{n}\). We will see soon some schemes which violate this property and tend to produce spurious oscillations in the numerical solution. The last property is of course self-explanatory: by using a monotone scheme, we are guaranteed to converge to the entropy solution!</p>

<p>We would like for the finite volume scheme in \eqref{eq:finitevol2} to be a monotone scheme, and our goal now is to choose a function \(\hat{f}\) in order to achieve this. We have</p>

\[\begin{align}
  \mathcal{H}(\bar{u}_{j-1}^{n}, \bar{u}_{j}^{n}, \bar{u}_{j+1}^{n}) = \bar{u}_{j}^{n} - \frac{\Delta t}{\Delta x}\left( \hat{f}(\bar{u}_{j}^{n}, \bar{u}_{j+1}^{n}) - \hat{f}(\bar{u}_{j-1}^{n}, \bar{u}_{j}^{n}) \right).\notag
\end{align}\]

<p>\(\mathcal{H}\) must be nondecreasing in each argument, so we compute its partial derivatives:
\(\begin{align}
  \mathcal{H}_{1} &amp;= \frac{\Delta t}{\Delta x} \hat{f}_{1}(\bar{u}_{j-1}^{n}, \bar{u}_{j}^{n})\label{eq:monotone1}\\
  \mathcal{H}_{2} &amp;= 1 - \frac{\Delta t}{\Delta x} \left( \hat{f}_{1}(\bar{u}_{j}^{n}, \bar{u}_{j+1}^{n}) - \hat{f}_{2}(\bar{u}_{j-1}^{n}, \bar{u}_{j}^{n}) \right)\label{eq:monotone2}\\
  \mathcal{H}_{3} &amp;= -\frac{\Delta t}{\Delta x} \hat{f}_{2}(\bar{u}_{j}^{n}, \bar{u}_{j+1}^{n})\label{eq:monotone3}.
\end{align}\)</p>

<p>Here, \(\hat{f}_{1}\) denotes the partial derivative of \(\hat{f}\) with respect to the first argument, and so on. If \(\hat{f}\) is increasing in the first argument and decreasing in the second argument, then \eqref{eq:monotone1} and \eqref{eq:monotone3} will be nonnegative. Furthermore, if</p>

\[\begin{align}
  \frac{\Delta t}{\Delta x} \left( \hat{f}_{1}(\bar{u}_{j}^{n}, \bar{u}_{j+1}^{n}) - \hat{f}_{2}(\bar{u}_{j-1}^{n}, \bar{u}_{j}^{n}) \right) \leqslant 1,\notag
\end{align}\]

<p>then \eqref{eq:monotone2} will also be nonnegative. This last condition is merely a restriction on our time step. By choosing \(\Delta t\) small enough, we can always satisfy this condition. The first two conditions tell us that we merely need to choose our numerical flux function \(\hat{f}\) such that it is increasing in its first argument and decreasing in its second argument.</p>

<p><strong>Proposition 1.</strong> <em>If \(\hat{f}\) satifies the following conditions:</em></p>

<ol>
  <li>
    <p><em>\(\hat{f}\) is Lipschitz continuous in all arguments.</em></p>
  </li>
  <li>
    <p><em>\(\hat{f}(u,u) = f(u)\). This is known as the consistency condition.</em></p>
  </li>
  <li>
    <p><em>\(\hat{f}\) is increasing in the first argument and decreasing in the second argument,</em></p>
  </li>
</ol>

<p><em>then the scheme \eqref{eq:finitevol2} is monotone.</em></p>

<p>We will not discuss the first two conditions here, but suffice it to say that these properties are very easy to check.</p>

<h3 id="i-numerical-flux-functions">i. Numerical Flux Functions</h3>
<p>We will briefly state some common choices of \(\hat{f}\) that satisfy the conditions of Proposition 1 and lead to monotone schemes.</p>

<p><strong>Lax-Friedrichs flux.</strong></p>

\[\begin{align}
  \hat{f}(\bar{u}_{j}, \bar{u}_{j+1}) = \frac{1}{2}\left( f(u_{j}) + f(u_{j+1}) - \alpha(u_{j+1} - u_{j}) \right),\notag\\
\end{align}\]

<p>where 
\(\alpha = \max_{u} |f'(u)|\). We can take \(\alpha = \max_{[\bar{u}_{j}, \bar{u}_{j+1}]} |f'(u)|\).</p>

<p><strong>Godunov flux.</strong></p>

\[\begin{align}
  \hat{f}(\bar{u}_{j}, \bar{u}_{j+1}) = \begin{cases}
    \min_{\bar{u}_{j} \leqslant u \leqslant \bar{u}_{j+1}} f(u), &amp;\bar{u}_{j} \leqslant \bar{u}_{j+1}\\
    \max_{\bar{u}_{j} \geqslant u \geqslant \bar{u}_{j+1}} f(u), &amp;\bar{u}_{j} &gt; \bar{u}_{j+1}.
  \end{cases}\notag
\end{align}\]

<p><strong>Engquist-Osher flux.</strong></p>

\[\begin{align}
  \hat{f}(\bar{u}_{j}, \bar{u}_{j+1}) = &amp;\int_{0}^{\bar{u}_{j}} \max\{f'(u),0\}\;du + \int_{0}^{\bar{u}_{j+1}} \min\{f'(u),0\}\;du.\notag
\end{align}\]

<p>There are two more popular choices of flux that do not lead to monotone schemes but are nevertheless still popular.</p>

<p><strong>Roe flux.</strong></p>

\[\begin{align}
  \hat{f}(\bar{u}_{j}, \bar{u}_{j+1}) = \begin{cases}
    f(\bar{u}_{j}), &amp;\frac{f(\bar{u}_{j+1}) - f(\bar{u}_{j})}{\bar{u}_{j+1} - \bar{u}_{j}} \geqslant 0\\
    f(\bar{u}_{j+1}), &amp;\frac{f(\bar{u}_{j+1}) - f(\bar{u}_{j})}{\bar{u}_{j+1} - \bar{u}_{j}} &lt; 0.
  \end{cases}\notag
\end{align}\]

<p><strong>Lax-Wendroff flux.</strong></p>

\[\begin{align}
  \hat{f}(\bar{u}_{j}, \bar{u}_{j+1}) = &amp;\frac{1}{2}(f(\bar{u}_{j}) + f(\bar{u}_{j+1})) -\notag\\
  &amp;\frac{\Delta t}{2\Delta x}f'\left( \frac{\bar{u}_{j} + \bar{u}_{j+1}}{2}\right) \left( f(\bar{u}_{j+1}) - f(\bar{u}_{j}) \right)\notag
\end{align}\]

<h2 id="ii-implementation">II. Implementation</h2>
<p>Without further ado, we will implement the first-order finite volume method using all five of the aforementioned numerical fluxes for Burgers equation with \(u(x,0) = \sin{x}\). All of the source code is publicly available on <a href="https://github.com/jdongg/numCL">GitHub</a> in both a MATLAB and Python implementation. For convenience, we will explain the Python implementation here. First, we initialize the parameters of the scheme.</p>

<d-code block="" language="python">
# specify domain
a = 0
b = 2*np.pi

# initial condition: u(x,0) = alpha + beta*sin(x)
alpha = 0.0
beta  = 1.0

# number of grid points in spatial discretization
N  = 80

# setup grid points
x = np.linspace(a,b,N)     
dx = (b-a)/(N-1);  

# stopping time
T = 1.5
</d-code>

<p>Next, we compute the cell averages of the initial condition, \(\bar{u}_{j}^{0}\). This is accomplished by integrating the initial condiiton over each cell \(I_{j}\):</p>
<d-code block="" language="python">
# setup array to store cell averages; due to periodicity, we omit the last cell
u = np.zeros((len(x)-1,1)); 

# returns the initial condition of the PDE
def initial_condition(z, alpha, beta):
    return alpha + beta*np.sin(z)

# compute cell averages at t=0
for i in range(0,N-1):
    u[i] = (1.0/dx)*integrate.quad(initial_condition, x[i], x[i+1], args=(alpha,beta))[0]
</d-code>

<p>Next, we must choose an appropriate time step for the scheme. Since we are using an explicit time-stepping scheme, we must choose an appropriately small time step (recall that explicit time-stepping is conditionally stable). For the purposes of example, we choose</p>

\[\frac{\Delta t}{\Delta x} \max_{u} |f'(u)| = \frac{1}{2}.\]

<p>The last step is to perform the time-stepping, whereby for each time step we must compute the numerical fluxes \(\hat{f}(\bar{u}_{j}^{n}, \bar{u}_{j+1}^{n})\) and \(\hat{f}(\bar{u}_{j-1}^{n}, \bar{u}_{j}^{n})\):</p>
<d-code block="" language="python">
t = 0.0
while t &lt; T:
    # alpha for the Lax-Friedrichs flux
    A  = np.amax(np.amax(u));

    # compute numerical fluxes fhat_{j+1/2}
    um = u
    up = np.roll(u,-1)
    fR = lf_flux(um, up, A)

    # compute numerical fluxes fhat_{j-1/2} (assuming periodic BCs)
    fL = np.roll(fR,1)

    # first order explicit time-stepping
    u -= dt/dx*(fR - fL)

    # increment time step
    t = t+dt
</d-code>

<p>Each of the numerical fluxes in Section 2.1.1 have been implemented and can be viewed on <a href="https://github.com/jdongg/numCL">GitHub</a>. Figure 2 shows the finite volume solutions at \(T=1.5\).</p>

<div class="container">
  <div class="row">
    <div class="col">
    </div>
    <div class="col-15">
      <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/conslaw1/finiteVolumeBurgers-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/conslaw1/finiteVolumeBurgers-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/conslaw1/finiteVolumeBurgers-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/conslaw1/finiteVolumeBurgers.png" class="img rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

      <div class="caption">
        <b>Figure 2:</b> Solution of Burgers equation with sine initial data using the first-order finite volume method.
      </div> 
    </div>
    <div class="col">
    </div>
  </div>
</div>

<p>The first observation we make is that there is little difference between the Roe, Godunov, and Engquist-Osher fluxes, at least for this example problem. The Godunov and Engquist-Osher fluxes lead to monotone schemes which converge to the entropy solution. The Roe flux does not lead to a monotone scheme, and in very rare cases may provide solutions with the incorrect shock speed. Nevertheless, in most cases the Roe scheme works well and is a popular choice for the numerical flux.</p>

<p>Next, the Lax-Friedrichs flux is noticeably more dissipative than any of the other results. It is known to “smear” discontinuities and add numerical diffusion. However, the Lax-Friedrichs flux is a very popular choice for the numerical flux since it leads to a monotone scheme and is straightforward to implement.</p>

<p>Lastly, the Lax-Wendroff flux is…problematic, to say the least. Notice that the numerical solution develops spurious (nonphysical) oscillations in the vicinity of the shock at \(x=\pi\). If the Lax-Wendroff scheme converges, it converges to a weak solution, but the scheme is not monotone and does not exhibit the total variation-diminishing (TVD) property. Lax-Wendroff is actually a <em>second-order method</em> in disguise, but we’ll discuss this much later and how the order of a numerical method is related to the TVD property for conservation laws.</p>

<h2 id="3-key-takeaways"><strong>3. Key Takeaways</strong></h2>
<p>If I haven’t bored you to death by this point, you should have a basic understanding of weak solutions for conservation laws and the mathematical foundations for discontinuous solutions. The study of such solutions is important because they occur frequently in fluid dynamics and other related fields. You should also be able to implement a basic first-order finite volume scheme for a variety of one-dimensional conservation laws.</p>

<p>We’ve only covered first-order schemes in this post, and you might have already guessed that these methods converge too slowly to be of much use for practical problems. Next time, we’ll cover higher order finite volume methods for conservation laws and touch on some of the theory regarding higher-order methods (remember how I said the Lax-Wendroff flux actually yields a second-order method? We’ll circle back to that soon). Unfortunately, there’s no free lunch and higher order methods come with their own set of issues.</p>

<p>Below are two texts by Randy LeVeque which are helpful in fleshing out much of the details of the Theorems given in this post. I’ve found them to be immensely helpful in my Ph.D. studies.</p>

<h2 id="4-references"><strong>4. References</strong></h2>
<p>[1] LeVeque, Randall J. <a href="http://staff.washington.edu/rjl/book2/sample.pdf">Finite volume methods for hyperbolic problems</a>. Vol. 31. Cambridge university press, 2002.</p>

<p>[2] LeVeque, Randall J. <a href="https://pdfs.semanticscholar.org/1470/c6f43c769572c4cfc94ffc9c5710484ff1e5.pdf">Numerical methods for conservation laws</a>. Vol. 132. Basel: Birkhäuser, 1992.</p>

<h2 id="5-codes"><strong>5. Codes</strong></h2>
<p><a href="https://github.com/jdongg/numCL">NumCL repository on GitHub</a></p>]]></content><author><name>Justin Dong</name></author><category term="numerical-methods" /><category term="finite-volume-method," /><category term="hyperbolic-conservation-laws" /><summary type="html"><![CDATA[1. Conservation Laws]]></summary></entry></feed>