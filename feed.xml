<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-03-23T04:11:20-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">Numerical methods for conservation laws (part 2)</title><link href="http://localhost:4000/blog/2019/conservationlaws2/" rel="alternate" type="text/html" title="Numerical methods for conservation laws (part 2)" /><published>2019-05-25T11:12:00-04:00</published><updated>2019-05-25T11:12:00-04:00</updated><id>http://localhost:4000/blog/2019/conservationlaws2</id><content type="html" xml:base="http://localhost:4000/blog/2019/conservationlaws2/"><![CDATA[<h1 id="1-high-order-finite-volume-methods"><strong>1. High Order Finite Volume Methods</strong></h1>

<p>Last time, we established that the explicit first order finite volume scheme is given by
\(\begin{equation}
  \bar{u}_{j}^{n+1} = \bar{u}_{j}^{n} - \frac{\Delta t}{\Delta x}\left( \hat{f}(\bar{u}_{j}^{n}, \bar{u}_{j+1}^{n}) - \hat{f}(\bar{u}_{j-1}^{n}, \bar{u}_{j}^{n}) \right),
  \label{eq:finitevol2}
\end{equation}\)</p>

<p>where \(\bar{u}_{j}^{n}\) denotes the cell averages at time \(t^{n}\) and \(\bar{u}_{j}^{n+1}\) the cell averages at time \(t^{n+1} = t^{n} + \Delta t\). Here, \(\hat{f}\) is the <em>numerical flux function</em> which takes as inputs cell averages and returns an approximation of the flux function at the cell endpoints \(x_{j+1/ 2}\), i.e. \(f(u_{j+1/ 2}) \approx \hat{f}(\bar{u}_{j}, \bar{u}_{j+1})\) and \(f(u_{j-1/ 2}) \approx \hat{f}(\bar{u}_{j-1}, \bar{u}_{j})\).</p>

<p>More generally, we can view the quantity \(\bar{u}_{j}\) as a first order (zero degree polynomial) reconstruction of \(u(x)\) in cell \(I_{j}\) using the cell average. The same can be said of \(\bar{u}_{j+1}\) in cell \(I_{j+1}\). By approximating \(u(x_{j+1/ 2}^{-}) = u_{j+1/ 2}^{-} \approx \bar{u}_{j}\) on the left “side” of \(x_{j+1/ 2}\) and \(u(x_{j+1/ 2}^{+}) = u_{j+1/ 2}^{+} \approx \bar{u}_{j+1}\) on the right side of \(x_{j+1/ 2}\), the numerical flux has a natural interpretation as \(f(u_{j+1/ 2}) \approx \hat{f}(u_{j+1/ 2}^{-}, u_{j+1/ 2}^{+})\). We reconcile the two reconstructions at \(x_{j+1/ 2}\) by utilizing the numerical flux function. Figure 1 illustrates this concept.</p>

<div class="container">
  <div class="row">
    <div class="col">
    </div>
    <div class="col-9">
      <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/conslaw2/reconstruction0-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/conslaw2/reconstruction0-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/conslaw2/reconstruction0-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/conslaw2/reconstruction0.png" class="img rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

      <div class="caption">
        <b>Figure 1:</b> First order reconstruction. We reconstruct $u$ using a zero degree polynomial (i.e. constant functions) in each cell.
      </div> 
    </div>
    <div class="col">
    </div>
  </div>
</div>

<h2 id="i-polynomial-reconstruction"><strong>I. Polynomial Reconstruction</strong></h2>

<p>Viewing the finite volume scheme as a reconstruction procedure using the cell averages is key to developing higher order methods. Using \eqref{eq:finitevol2}, we have \(u_{j+1/ 2}^{-} = \bar{u}_{j} + \mathcal{O}(\Delta x)\). Suppose we want to develop a third-order finite volume scheme now. In particular, we would like to have</p>

\[u_{j+1/ 2}^{-} = \mathcal{F}(\bar{u}_{j-p}, \dots, \bar{u}_{j+q}) + \mathcal{O}(\Delta x^{3}),\]

<p>where \(p\) and \(q\) are integers to be chosen by us. It stands to reason that if we want a third-order reconstruction of $u(x)$ in cell \(I_{j}\), we need to use three “pieces” of information: \(u_{j+1/ 2}^{-} \approx \mathcal{F}(\bar{u}_{j-1}, \bar{u}_{j}, \bar{u}_{j+1})\). In other words, we are seeking a degree two polynomial \(p_{j}(x) = ax^{2} + bx + c\) in \(I_{j}\) such that \(u_{j+1/ 2}^{-} = p_{j}(x_{j+1/ 2}) + \mathcal{O}(\Delta x^{3})\). In order to fully determine this polynomial, i.e. solve for $a,b,c$, we require three equations. A natural condition to impose on \(p_{j}\) is that it preserves cell averages over the stencil \(\{I_{j-1}, I_{j}, I_{j+1}\}\):</p>

\[\begin{equation}
	\frac{1}{\Delta x} \int_{I_{i}} p_{j}(x)\;dx = \bar{u}_{i}, \;\;\;i \in \\{j-1,j,j+1\\}.\notag
\end{equation}\]

<p>We then take \(u_{j+1/ 2}^{-} = p_{j}(x_{j+1/ 2})\). Similarly, we take \(u_{j+1/ 2}^{+} = p_{j+1}(x_{j+1/ 2})\). Figure 2 demonstrates how the third-order reconstruction procedure works.</p>

<div class="container">
  <div class="row">
    <div class="col">
    </div>
    <div class="col-10">
      <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/conslaw2/reconstruction2-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/conslaw2/reconstruction2-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/conslaw2/reconstruction2-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/conslaw2/reconstruction2.png" class="img rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

      <div class="caption">
        <b>Figure 2:</b> Third order reconstruction. We reconstruct $u$ using a quadratic function defined in the stencil $\{I_{j-1}, I_{j}, I_{j+1}\}$ and enforce that the quadratic function maintains cell averages across the stencil. We take $u_{j+1/ 2}^{-} \approx p_{j}(x_{j+1 /2})$ and $u_{j+1/ 2}^{+} \approx p_{j+1}(x_{j+1/ 2})$ (not pictured here).
      </div> 
    </div>
    <div class="col">
    </div>
  </div>
</div>

<p><strong>Definition 1.</strong> <em>The order \(k\) in space finite volume scheme is given by</em>
\(\begin{equation}
	\frac{d\bar{u}_{j}}{dt} + \frac{1}{\Delta x}\left( \hat{f}(u_{j+1/ 2}^{-}, u_{j+1/ 2}^{+}) - \hat{f}(u_{j-1/ 2}^{-}, u_{j-1/ 2}^{+}) \right) = 0
	\label{eq:finitevolk}
\end{equation}\)</p>

<p><em>where \(u_{j+1/ 2}^{-} \approx p_{j}(x_{j+1/ 2})\) and \(u_{j+1/ 2}^{+} \approx p_{j+1}(x_{j+1/ 2})\). The degree \(k-1\) polynomial \(p_{j}\) is reconstructed from the stencil \(\{\bar{u}_{j-p}, \dots, \bar{u}_{j+q}\}\) and satifies the \(q+p+1\) conditions</em>
\(\frac{1}{\Delta x} \int_{I_{i}} p_{j}(x)\;dx = \bar{u}_{i}, \;\;\;i \in \\{j-p, \dots, j+q\\}.\)</p>

<p>For concreteness, we will work with the third-order scheme. It is a straightforward (if tedious) pen and paper exercise to compute the parameters \(a,b,c\) for the polynomial \(p_{j}(x) = ax^{2}+bx+c\). However, notice that the scheme \eqref{eq:finitevolk} only requires the evaluation of the polynomials at the endpoints of the cells. For the third order scheme, this amounts to the following formulae:</p>

\[\begin{align}
	u_{j+1/ 2}^{-} &amp;= p_{j}(x_{j+1/ 2}) = -\frac{1}{6}\bar{u}_{j-1} + \frac{5}{6}\bar{u}_{j} + \frac{1}{3}\bar{u}_{j+1}\label{eq:recon1}\\
	u_{j+1/ 2}^{+} &amp;= p_{j+1}(x_{j+1/ 2}) = \frac{1}{3}\bar{u}_{j} + \frac{5}{6}\bar{u}_{j+1} - \frac{1}{6}\bar{u}_{j+2}.\label{eq:recon2}
\end{align}\]

<p>These formulae can be found in a number of references, e.g. Chapter 4 of <a href="https://doi.org/10.1007/BFb0096351">[3]</a>.</p>

<p>Before moving on, we should make note of two very important issues here. The first is that \eqref{eq:finitevolk} is only a <em>semidiscrete</em> scheme – we haven’t discretized time yet! Recall that for the first order scheme we used an explicit first-order time discretization. But if we use this time discretization here, then we’ll be pairing a third-order in space discretization with a first-order in time discretization. Should we expect such a scheme to be stable? Furthermore, we’ll have to reduce our time step commensurately to ensure that the error in the time discretization does not dominate.</p>

<p>Second, we haven’t said anything about the convergence properties (or any other properties for that matter) of the higher order finite volume schemes. Recall that the Lax-Wendroff flux resulted in a second-order scheme which was neither monotone nor total variation diminishing (TVD). It’s unclear what will happen when we apply \eqref{eq:finitevolk} to actually solve a conservation law.</p>

<h2 id="ii-third-order-runge-kutta-time-integration"><strong>II. Third Order Runge-Kutta Time Integration</strong></h2>
<p>In all of the examples that follow, we will use a third order Runge-Kutta (RK3) discretization in time. For our purposes, this discretization is conditionally stable with some necessary restriction on the time step as with all explicit schemes. If we pair the RK3 discretization with a spatial discretization higher than third order accuracy, we may have to decrease the time step further.</p>

<p>The setting for the RK3 discretization is as follows: consider the semidiscrete scheme</p>

\[\frac{d\bar{u}_{j}}{dt} + \mathcal{L}(\bar{u}_{j}) = 0,\]

<p>where \(\mathcal{L}\) is the spatial operator associated with the flux terms.</p>

<p><strong>Definition 2.</strong> <em>The third order Runge-Kutta time discretization is given by</em></p>

\[\begin{align}
	\bar{u}_{j}^{(1)} &amp;= \bar{u}_{j}^{n} - \Delta t \mathcal{L}(\bar{u}_{j-p}^{n}, \dots, \bar{u}_{j+q}^{n}, t^{n})\notag\\
	\bar{u}_{j}^{(2)} &amp;= \frac{3}{4}\bar{u}_{j}^{n} + \frac{1}{4}\left( \bar{u}_{j}^{(1)} - \Delta t \mathcal{L}(\bar{u}_{j-p}^{(1)}, \dots, \bar{u}_{j+q}^{(1)}, t^{n} + \Delta t) \right)\notag\\
	\bar{u}_{j}^{n+1} &amp;= \frac{1}{3}\bar{u}_{j}^{n} + \frac{2}{3}\left( \bar{u}_{j}^{(2)} - \Delta t \mathcal{L}(\bar{u}_{j-p}^{(2)}, \dots, \bar{u}_{j+q}^{(2)}, t^{n} + \frac{1}{2}\Delta t) \right).\notag
\end{align}\]

<p>The choice of coefficients in the above scheme is deliberate. In fact, with these coefficients, the RK3 time discretization can be shown to be TVD.</p>

<p><strong>Proposition 1.</strong> <em>The third order Runge-Kutta time discretization is TVD when paired with a spatial scheme that is also TVD.</em></p>

<p>Interestingly, there are many examples of time discretizations which are not TVD, even when paired with a TVD spatial scheme. We refer the reader to the seminal work of Sigal Gottlieb and Chi-Wang Shu in <a href="https://doi.org/10.1090/S0025-5718-98-00913-2">[1]</a> for a much more in-depth discussion of TVD Runge-Kutta schemes.</p>

<h2 id="iii-a-first-attempt-at-implementation"><strong>III. A First Attempt at Implementation</strong></h2>
<p>Before covering the theory of higher order spatial discretizations, it is highly illustrative to attempt a first implementation of the third order finite volume scheme with RK3 time discretization. All of the source code can be found on <a href="https://github.com/jdongg/numCL">GitHub</a>. There are actually relatively few changes that must be made to the first order code. The main time-stepping loop is as follows:</p>
<d-code block="" language="python">
t = 0.0
while t &lt; T:
	# alpha for the Lax-Friedrichs flux
	A  = np.amax(np.amax(u0))

	# first RK stage
	um,up = polynomial_reconstruction(u0)
	fR = lf_flux(um,up,A)
	fL = np.roll(fR,1)
	u = u0 - dt/dx*(fR - fL)

	# second RK stage
	um,up = polynomial_reconstruction(u)
	fR = lf_flux(um,up,A)  
	fL = np.roll(fR,1)
	u = 3.0/4.0*u0 + 1.0/4.0*(u - dt/dx*(fR - fL))

	# third RK stage
	um,up = polynomial_reconstruction(u)
	fR = lf_flux(um,up,A)   
	fL = np.roll(fR,1)
	u = 1.0/3.0*u0 + 2.0/3.0*(u - dt/dx*(fR - fL))

	# increment time step
	u0 = u
	t = t+dt
</d-code>

<p>Aside from having the three stages for the Runge-Kutta time-stepping, the only real addition to the code is the polynomial reconstruction using the cell averages. Using \eqref{eq:recon1} and \eqref{eq:recon2}, the polynomial reconstruction is easily implemented with the following lines:</p>
<d-code block="" language="python">
def polynomial_reconstruction(u):

	# compute u_{j+1/2}^{-} and u_{j+1/2}^{+}
	um = -1.0/6.0*np.roll(u,1) + 5.0/6.0*u + 1.0/3.0*np.roll(u,-1)
	up = 1.0/3.0*u + 5.0/6.0*np.roll(u,-1) - 1.0/6.0*np.roll(u,-2)

	return um, up
</d-code>

<p>Figure 3 shows a complete simulation of Burgers equation up to \(T=2\) using this scheme.</p>

<div class="container">
  <div class="row">
    <div class="col">
    </div>
    <div class="col-10">
      <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/conslaw2/fv3bad.gif-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/conslaw2/fv3bad.gif-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/conslaw2/fv3bad.gif-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/conslaw2/fv3bad.gif" class="img rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

      <div class="caption">
        <b>Figure 3:</b> The third order finite volume scheme applied to Burgers equation.
      </div> 
    </div>
    <div class="col">
    </div>
  </div>
</div>

<p>Yikes. On first glance, it appears our scheme performs reasonably well before the appearance of the shock but develops spurious oscillations after the shock. We can also compute the errors in the \(\ell^{1}\) norm:</p>

\[||\bar{u} - \bar{u}_{e}||_{\ell^{1}} = \frac{1}{N} \sum_{j=1}^{N} |\bar{u}_{j} - \bar{u}_{e,j}|.\]

<p>Figure 4 describes how the \(\ell^{1}\) error decreases as we increase the number of grid points. Before the appearance of the shock, our scheme displays third order accuracy as expected. But after the appearance of the shock, the accuracy is reduced to first order.</p>

<div class="container">
  <div class="row">
    <div class="col">
    </div>
    <div class="col-10">
      <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/conslaw2/convergencefv3bad-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/conslaw2/convergencefv3bad-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/conslaw2/convergencefv3bad-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/conslaw2/convergencefv3bad.png" class="img rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

      <div class="caption">
        <b>Figure 4:</b> Errors in the $\ell^{1}$ norm of the third order finite volume scheme before and after the formation of the shock in Burgers equation.
      </div> 
    </div>
    <div class="col">
    </div>
  </div>
</div>

<p>So, what on earth is going on here? The driving idea behind higher order schemes is that we hope to converge to the exact (entropy) solution faster than the original first order scheme. In the presence of shocks and discontinuities, though, it seems as if our third-order scheme hasn’t given us any benefits. Indeed, it actually introduces oscillations, which is arguably worse than the first order scheme which we recall was provably monotone and TVD.</p>

<h1 id="2-godunovs-theorem"><strong>2. Godunov’s Theorem</strong></h1>

<p>Our initial results are actually not altogether unexpected and are actually a result of Godunov’s Theorem.</p>

<p><strong>Definition 3.</strong> <em>A linear scheme is one that can be written in the form</em></p>

\[\begin{align}
	\bar{u}_{j}^{n+1} = \sum_{\ell=-k}^{k} c_{\ell}(\lambda) \bar{u}_{j+\ell}^{n}, \;\;\;\lambda = \frac{\Delta t}{\Delta x}\notag
\end{align}\]

<p><em>when applied to a linear conservation law.</em></p>

<p>It’s worth noting that all of the finite volume schemes we have considered thus far are linear schemes (a fact which can easily be verified with pen and paper).</p>

<p><strong>Theorem 1. (Godunov)</strong> <em>A linear scheme is monotone if and only if it is total variation diminishing (TVD). Moreover, linear TVD schemes are at most first-order accurate.</em></p>

<p>There’s quite a bit to digest here, but we’ll start with the first statement. Recall from last time that monotone schemes converge to the entropy solution and are also TVD. Godunov’s Theorem states that for <em>linear</em> schemes, the notion of being monotone is equivalent to being TVD. The second statement is much deeper: it tells us that if we want to construct a linear monotone scheme, then it must be first-order accurate. In particular, we posit that the third-order finite volume scheme is neither monotone nor TVD.</p>

<p>In fact, monotone schemes are at most first-order accurate regardless of whether the scheme is linear or not. This suggests that we simply can’t develop high order monotone schemes. However, having only a TVD scheme is still desirable as it essentially guarantees our scheme will not generate spurious oscillations. And yet, Godunov’s Theorem tells us that if we stick to linear schemes, we can’t develop high order TVD schemes either.</p>

<p>Nevertheless, it is still possible to enforce the TVD property on the third-order finite volume scheme. To do so, we employ a post-processing step known as <em>slope limiting</em>. This results in a nonlinear TVD scheme which is does not produce spurious oscillations.</p>

<h1 id="3-slope-limiting"><strong>3. Slope Limiting</strong></h1>

<h2 id="i-generalized-muscl-limiter"><strong>I. Generalized MUSCL Limiter</strong></h2>
<p>The key observation is that when nonphysical oscillations appear in the numerical solution, the gradients of the cell averages between successive cells rapidly change sign. Slope limiting procedures identify where these sign changes occur and reduce the gradient to zero in these regions. The procedure is as follows.</p>

<ol>
  <li>Given \(\{u_{j+1/ 2}^{\pm}\}\) and the cell averages \(\{\bar{u}_{j}\}\), define</li>
</ol>

\[\begin{align}
	\tilde{u}_{j} = u_{j+1/ 2}^{-} - \bar{u}_{j}, \;\;\;\;\;\tilde{\tilde{u}}_{j} = \bar{u}_{j} - u_{j-1/ 2}^{+}.\notag
\end{align}\]

<ol>
  <li>Compute the modified quantities \(\tilde{u}_{j}^{\text{mod}}\) and \(\tilde{\tilde{u}}_{j}^{\text{mod}}\) according to
\(\begin{align}
 \tilde{u}_{j}^{\text{mod}} &amp;= \text{minmod}\{\tilde{u}_{j}, \bar{u}_{j+1} - \bar{u}_{j}, \bar{u}_{j} - \bar{u}_{j-1}\}\\
 \tilde{\tilde{u}}_{j}^{\text{mod}} &amp;= \text{minmod}\{\tilde{\tilde{u}}_{j}, \bar{u}_{j+1} - \bar{u}_{j}, \bar{u}_{j} - \bar{u}_{j-1}\},
\end{align}\)</li>
</ol>

<p>where the \(\text{minmod}\) function is defined as</p>

\[\begin{equation}
	\text{minmod}(a_{1},\dots,a_{n}) = \begin{cases}
		s \cdot \min_{j} |a_{j}|, &amp;s = \text{sgn}(a_{1}) = \dots = \text{sgn}(a_{n})\\
		0, &amp;\text{else}.
	\end{cases}\notag
\end{equation}\]

<ol>
  <li>Compute the modified reconstructed values \(u_{j+1/ 2}^{\pm,\text{mod}}\) according to</li>
</ol>

\[\begin{align}
	u_{j+1/ 2}^{-,\text{mod}} = \bar{u}_{j} + \tilde{u}_{j}^{\text{mod}}, \;\;\;\;\;u_{j-1/ 2}^{+,\text{mod}} = \bar{u}_{j} - \tilde{\tilde{u}}_{j}^{\text{mod}}.\notag
\end{align}\]

<p>The procedure outlined above is known as the <strong>generalized MUSCL (monotone upwind scheme for conservation laws) limiter.</strong> Note that the quantities \(\bar{u}_{j+1} - \bar{u}_{j}\) and \(\bar{u}_{j} - \bar{u}_{j-1}\) are crude approximations to the gradient over stencil \(\{I_{j}, I_{j+1}\}\) and \(\{I_{j-1}, I_{j}\}\). In the case of oscillations in our numerical solution, these two quantities will have opposite signs, in which case the \(\text{minmod}\) function returns $0$ and the modified quantities \(u_{j+1/ 2}^{\pm,\text{mod}}\) are set to the cell average. This is equivalent to the original first order finite volume scheme we studied last time! Hence, in the presence of shocks and discontinuities, we might expect the generalized MUSCL scheme to be first order accurate.</p>

<p>Now, let’s consider the case when the $\text{minmod}$ function returns the first argument. We carefully note that this situation occurs when \(\{\bar{u}_{j}\}\) is monotone over the stencils \(\{I_{j-1}, I_{j}, I_{j+1}\}\). Then we have \(u_{j+1/ 2}^{\pm,\text{mod}} = u_{j+1/ 2}^{\pm}\) and the high order finite volume scheme is unchanged. Thus, in <em>smooth, monotone regions</em> the generalized MUSCL limiter maintains original high order accuracy of the scheme.</p>

<p>Lastly, we must consider what happens near smooth extrema. For instance, \(u(x,0) = \sin{x}\) has smooth extrema at \(x=\pi/2\) (maximum) and \(x=3\pi/2\) (minimum). As consecutive gradients have opposite sign at local extrema, the $\text{minmod}$ function will return \(0\) and the scheme is again reduced to first order. Stanley Osher proved the following result:</p>

<p><strong>Theorem 2.</strong> <em>TVD schemes are at most first-order accurate near smooth extrema.</em></p>

<p>This seems like a problem since plenty of smooth solutions have smooth extrema. We’ll discuss shortly how to develop slope limiters which do not have this drawback, but first, it would be prudent to discuss to discuss the properties of the generalized MUSCL limiter. When paired with this limiter, the finite volume scheme is actually TVD. The proof is due to Ami Harten [<a href="http://arrow.utias.utoronto.ca/~groth/aer1319/Handouts/Additional_Reading_Material/JCP-1983-harten.pdf">2</a>].</p>

<p><strong>Lemma 1. (Harten)</strong> <em>If a scheme can be written in the form</em></p>

\[\begin{equation}
	\bar{u}_{j}^{n+1} = \bar{u}_{j}^{n} + C_{j+1/ 2}(\bar{u}_{j+1}^{n} - \bar{u}_{j}^{n}) - D_{j-1/ 2}(\bar{u}_{j}^{n} - \bar{u}_{j-1}^{n})
\end{equation}\]

<p><em>where \(C_{j+1/ 2}, D_{j+1/ 2} \geqslant 0\) and \(C_{j+1/ 2} + D_{j+1/ 2} \leqslant 1\), then the scheme is TVD.</em></p>

<p>Harten’s Lemma is the key to proving that the finite volume MUSCL scheme is TVD.</p>

<p><strong>Proposition 1.</strong> <em>The finite volume scheme with generalized MUSCL limiter is TVD.</em></p>

<p>Without further ado, we present the results using the MUSCL limiter. The only change from the previous third-order code is to add the MUSCL limiter:</p>
<d-code block="" language="python">
def muscl_limiter(um, up, u):

	# gradients to be adjusted
	ut  = um - u
	utt = u - np.roll(up,1)

	ut_m = minmod(ut, np.roll(u,-1)-u, u-np.roll(u,1))
	utt_m = minmod(utt, np.roll(u,-1)-u, u-np.roll(u,1))

	# modify the cell reconstructions using ut and utt
	um_mod = u + ut_m
	up_mod = u - utt_m
	up_mod = np.roll(up_mod,-1)

	return um_mod, up_mod
</d-code>

<p>The \(\text{minmod}\) function is easily implemented as follows:</p>
<d-code block="" language="python">
def minmod(a, b, c):
	
	# check whether a, b, and c are the same sign
	signs = ((np.sign(a)==np.sign(b)) &amp; (np.sign(b)==np.sign(c)) &amp; (np.sign(c)==np.sign(a)))

	# compute minimum magnitudes of {a,b,c}
	vals = np.concatenate((a,b,c), axis=1)
	mins = np.amin(np.abs(vals), axis=1)

	# compute the minmod
	m = signs*np.sign(a)*np.reshape(mins,(len(vals),1))

	return m
</d-code>

<p>Figure 5 contains the results. Voila! At first glance, all appears to be well. The nonphysical oscillations have been removed entirely from our numerical solution.</p>

<div class="container">
  <div class="row">
    <div class="col">
    </div>
    <div class="col-10">
      <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/conslaw2/fv3muscl.gif-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/conslaw2/fv3muscl.gif-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/conslaw2/fv3muscl.gif-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/conslaw2/fv3muscl.gif" class="img rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

      <div class="caption">
        <b>Figure 5:</b> The third order finite volume scheme with generalized MUSCL limiter applied to Burgers equation.
      </div> 
    </div>
    <div class="col">
    </div>
  </div>
</div>

<p>The next step is to verify that we are indeed getting the expected rates of convergence. There are three scenarios to consider: (i) the solution is smooth everywhere (we’ll consider the case when \(T=0.3\)) (ii) the solution develops a shock and we compute the \(\ell^{1}\) error over the entire computational grid (we’ll consider the case \(T=1.5\) here) (iii) the solution develops a shock and we compute the \(\ell^{1}\) error excluding a small region around the shock.</p>

<p>In the first scenario, we expect a reduced rate of convergence due to the presence of smooth extrema in the solution. In the second scenario, the order of convergence should be reduced to first order. Finally, in the third scenario, the solution is actually piecewise monotone and so we might expect to maintain the original third order accuracy if compute errors away from the shock.</p>

<p>Indeed, Figure 6 shows these three scenarios exactly. Before the shock, the overall order of accuracy is somewhere between 2 and 3; at the two extrema, the order is reduced to 1 but everywhere else, it is close to 3, leading to a suboptimal order of accuracy. After the shock, the order of accuracy is 1 overall, but when considering only cells at a distance 0.5 away from the shock at \(x=\pi\), the order of accuracy is 3.</p>

<p>This isn’t such a bad place to be, then. Near shocks, the mathematical theory tells us that we can <em>never</em> do better than first order accuracy, but we’ve managed to construct a scheme which is TVD and capable of giving higher order accuracy everywhere the solution is smooth. In the end, this is all we can really ask for. In the remaining posts, we will consider the different types of high order schemes which can be used to numerically solve nonlinear conservation laws.</p>

<div class="container">
  <div class="row">
    <div class="col">
    </div>
    <div class="col-10">
      <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/conslaw2/convergencefv3muscl-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/conslaw2/convergencefv3muscl-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/conslaw2/convergencefv3muscl-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/conslaw2/convergencefv3muscl.png" class="img rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

      <div class="caption">
        <b>Figure 6:</b> Error in the $\ell^{1}$ norm of the third order finite volume MUSCL scheme before the shock (blue line with squares), after the shock over the entire domain (red line with circles), and after the shock but excluding a small region around $x=\pi$ (red line with x's).
      </div> 
    </div>
    <div class="col">
    </div>
  </div>
</div>

<h2 id="ii-total-variation-bounded-tvb-limiter"><strong>II. Total-Variation Bounded (TVB) Limiter</strong></h2>

<p>But first, we consider how to remedy the issue of suboptimal convergence near <em>smooth</em> extrema. Recall that the issue here is that consecutive gradients have different signs and the \(\text{minmod}\) function return zero. In fact, recall that Theorem 2 tells us we can have at most first order accuracy near smooth extrema. If we relax the TVD constraint even further, it is possible to obtain uniformly high-order accurate schemes near smooth extrema.</p>

<p><strong>Definition 4.</strong> If \(TV(\bar{u}^{n}) \leqslant B\) for some fixed \(B&gt;0\) depending only on \(TV(\bar{u}^{0})\) and \(n\) and \(\Delta t\) such that \(n\Delta t &lt; T\), then the scheme is said to be <strong>total variation bounded</strong> (TVB) in \(0 \leqslant t \leqslant T\).</p>

<p>It is clear that all TVD schemes are TVB. For TVB schemes, one can show that there exists a subsequence in \(L^{1}_{\text{loc}}\) that converges to a weak solution of the conservation law.</p>

<p>In order to turn the generalized MUSCL limiter into a scheme that is TVB, we need only make a small adjustment. Namely, we define</p>

\[\begin{align}
	\tilde{u}_{j}^{\text{mod}} &amp;= \overline{\text{minmod}}\{\tilde{u}_{j}, \bar{u}_{j+1} - \bar{u}_{j}, \bar{u}_{j} - \bar{u}_{j-1}\}\notag\\
	\tilde{\tilde{u}}_{j}^{\text{mod}} &amp;= \overline{\text{minmod}}\{\tilde{\tilde{u}}_{j}, \bar{u}_{j+1} - \bar{u}_{j}, \bar{u}_{j} - \bar{u}_{j-1}\},\notag
\end{align}\]

<p>where the modified minmod function \(\overline{\text{minmod}}\) is defined by</p>

\[\begin{equation}
	\overline{\text{minmod}}(a_{1}, \dots, a_{n}) = \begin{cases}
		a_{1}, &amp;|a_{1}| \leqslant M\Delta x^{2}\\
		\text{minmod}(a_{1}, \dots, a_{n}) &amp;\text{else}.
	\end{cases}\notag
\end{equation}\]

<p>Here, \(M&gt;0\) is a parameter to be chosen by us. Essentially, the TVB limiter first checks whether \(u_{j+1/ 2}^{-}\) is sufficiently close to \(\bar{u}_{j}\). If so, then the modified $\text{minmod}$ function returns the first argument even if consecutive gradients have different signs, thus ensuring that the scheme retains high-order accuracy around smooth extrema [<a href="https://www.ams.org/journals/mcom/1987-49-179/S0025-5718-1987-0890256-5/S0025-5718-1987-0890256-5.pdf">4</a>]. The implementation is quite straightforward:</p>
<d-code block="" language="python">
def minmod2(a, b, c, dx, M):
	
	# check whether a, b, and c are the same sign
	signs = ((np.sign(a)==np.sign(b)) &amp; (np.sign(b)==np.sign(c)) &amp; (np.sign(c)==np.sign(a)))

	# compute minimum magnitudes of {a,b,c}
	vals = np.concatenate((a,b,c), axis=1)
	mins = np.amin(np.abs(vals), axis=1)

	# check whether first argument is sufficiently small
	a1 = (np.abs(a) &lt;= M*dx**2)
	a2 = (np.abs(a) &gt; M*dx**2)

	# compute the minmod
	m = a1*a + a2*signs*np.sign(a)*np.reshape(mins,(len(vals),1))

	return m
</d-code>

<p>We won’t show plots of the numerical solution here as they wouldn’t be discernible from the plots of the MUSCL limiter to the naked eye. Instead, Figure 7 shows the convergence rates for the TVB limiter. We note that before the shock forms, the order of accuracy is now 3 as expected, even in the presence of smooth extrema.</p>

<div class="container">
  <div class="row">
    <div class="col">
    </div>
    <div class="col-10">
      <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/conslaw2/convergencefv3tvb-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/conslaw2/convergencefv3tvb-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/conslaw2/convergencefv3tvb-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/conslaw2/convergencefv3tvb.png" class="img rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

      <div class="caption">
        <b>Figure 7:</b> Error in the $\ell^{1}$ norm of the third order finite volume TVB scheme before the shock (blue line with squares), after the shock over the entire domain (red line with circles), and after the shock but excluding a small region around $x=\pi$ (red line with x's).
      </div> 
    </div>
    <div class="col">
    </div>
  </div>
</div>

<h1 id="4-key-takeaways"><strong>4. Key Takeaways</strong></h1>
<p>To summarize, we implemented a third order finite volume scheme for a one-dimensional scalar conservation law. Without any modifications, this scheme was not TVD and introduced new extrema in our numerical solution that were not seen in the first order finite volume scheme. We touched on Godunov’s Theorem, which describes the fundamental limits of accuracy of linear monotone schemes.</p>

<p>We were able to develop a TVD slope limiter to remove the spurious oscillations in our high order numerical solution but noted that the limiter degraded the order of accuracy of our scheme near smooth extrema. In fact, we learned that <em>all</em> TVD schemes are at most first order near smooth extrema. To develop a truly high order scheme in the presence of smooth extrema, we turned to the TVB limiter.</p>

<p>It is worth emphasizing once more than when we talk about “high order” schemes for nonlinear conservation laws, we generally mean that the scheme maintains high order accuracy in smooth regions, is first order accurate in the vicinity of shocks and discontinuities, and is total variation diminishing (does not introduce any new extrema/spurious oscillations). Next time, we will cover essentially non-oscillatory finite volume schemes which essentially remove spurious oscillations by performing the polynomial reconstruction step in a clever way rather than post-processing the reconstruction as with the slope limiters we have seen in this post.</p>

<h1 id="5-references"><strong>5. References</strong></h1>
<p>[1] Gottlieb, Sigal, and Chi-Wang Shu. “<a href="https://doi.org/10.1090/S0025-5718-98-00913-2">Total variation diminishing Runge-Kutta schemes</a>.” Mathematics of computation of the American Mathematical Society 67.221 (1998): 73-85.</p>

<p>[2] Harten, Ami. “<a href="http://arrow.utias.utoronto.ca/~groth/aer1319/Handouts/Additional_Reading_Material/JCP-1983-harten.pdf">High resolution schemes for hyperbolic conservation laws</a>.” Journal of computational physics 49.3 (1983): 357-393.</p>

<p>[3] Johnson, B. Cockburn C., and C-W. Shu E. Tadmor. “<a href="https://doi.org/10.1007/BFb0096351">Advanced numerical approximation of nonlinear hyperbolic equations</a>.” (1997).</p>

<p>[4] Shu, Chi-Wang. “TVB uniformly high-order schemes for conservation laws.” Mathematics of Computation 49.179 (1987): 105-121.</p>

<h1 id="6-codes--resources"><strong>6. Codes &amp; Resources</strong></h1>
<ul>
  <li><a href="https://github.com/jdongg/numCL">NumCL repository on GitHub</a></li>
  <li>Figures 1 and 2 were created using <a href="https://www.mathcha.io/">Mathcha</a>, an online math editor with a convenient GUI that even allows you to export figures to TikZ.</li>
</ul>]]></content><author><name>Justin Dong</name></author><category term="numerical-methods" /><category term="finite-volume-method," /><category term="hyperbolic-conservation-laws" /><summary type="html"><![CDATA[1. High Order Finite Volume Methods]]></summary></entry><entry><title type="html">Numerical methods for conservation laws (part 1)</title><link href="http://localhost:4000/blog/2019/conservationlaws1/" rel="alternate" type="text/html" title="Numerical methods for conservation laws (part 1)" /><published>2019-05-15T11:12:00-04:00</published><updated>2019-05-15T11:12:00-04:00</updated><id>http://localhost:4000/blog/2019/conservationlaws1</id><content type="html" xml:base="http://localhost:4000/blog/2019/conservationlaws1/"><![CDATA[<h1 id="1-conservation-laws"><strong>1. Conservation Laws</strong></h1>

<p>In this post, we’ll take a look at conservation laws, the contexts in which they arise in nature, and some of the numerical methods used for solving them. In one spatial-dimension, conservation laws take the general form</p>

\[\begin{equation}
  \begin{cases}
    u_{t} + f(u)_{x} = 0 &amp;\text{in}\;\mathbb{R} \times [0,\infty)\\
    u(x,0) = u_{0}(x) &amp;\text{on}\;\mathbb{R} \times \{t=0\}.
  \end{cases}
  \label{eq:conslaw}
\end{equation}\]

<p>Here, we consider the spatial domain to be the entire real line (thus, we have no boundary conditions). The function \(f(u)\) – typically called the <em>flux function</em> – is sufficiently smooth. As their name suggests, conservation laws preserve mass. We often think of the quantity \(u\) as the density of some fluid, in which case \(\int_{\mathbb{R}} u(x,t)dx\) may be viewed as the mass of fluid. If we assume that \(u\) is compactly supported in \(x\) (i.e. it is zero outside of some compact set in \(x\), a reasonable assumption as the fluid should have finite mass) and \(f(0) = 0\), then taking derivatives over time yields</p>

\[\begin{align}
  \frac{\partial}{\partial t} \int_{-\infty}^{\infty} u(x,t)\;dx &amp;= \int_{-\infty}^{\infty} \frac{\partial u}{\partial t}\;dx = -\int_{-\infty}^{\infty} \frac{\partial f(u)}{\partial x}\;dx\notag\\
  &amp;= f(u(-\infty)) - f(u(\infty)) = 0.\notag
\end{align}\]

<p>The above computation implies that \(\int_{\mathbb{R}} u(x,t)\;dx = \int_{\mathbb{R}} u(x,0)\;dx\): the mass of \(u\) is conserved over time.</p>

<p>To gain some intuition for how solutions of this PDE might behave, we’ll first consider the simplest conservation law there is: the linear advection equation. Taking \(f(u) = u\), it is evident that our solution is given by \(u(x,t) = u_{0}(x-t)\). The PDE “transports” the initial profile in the \(x\) direction. In particular, we note that the solution to the <em>linear</em> conservation law is exactly as smooth as our initial data. That is, if \(u_{0} \in C^{1}_{x}(\mathbb{R})\) then \(u \in C_{x}^{1}(\mathbb{R})\) as well. We haven’t said anything particularly illuminating thus far and indeed, the linear advection equation isn’t particularly interesting to begin with.</p>

<p>Next, let’s consider a simple nonlinear conservation law:</p>

\[\begin{align}
\begin{cases}
  u_{t} + uu_{x} = 0 &amp;\text{in}\;\mathbb{R} \times [0,\infty)\\
  u(x,0) = \sin{x} &amp;\text{on}\;\mathbb{R} \times \{t=0\}.
\end{cases}
\end{align}\]

<p>The initial condition is as smooth as possible now: \(\sin{x} \in C_{x}^{\infty}(\mathbb{R})\). But what can we say about \(u(x,t)\)? Should we still expect it to be smooth in \(x\)? It turns out that the answer is a resounding <em>no</em>. In fact, solutions to the above nonlinear PDE (known as <em>Burgers equation</em>) are not even \(C_{x}^{0}(\mathbb{R})\)! The exact mathematical cause of this phenomena is due to the crossing of something called characteristic curves, but we will first consider a more intuitive explanation.</p>

<p>We may view Burgers equation as an advection equation in which the speed of propagation is equal to \(u\) itself. Our initial sine profile thus moves with variable speed in \(x\). For \(0 \leqslant x \leqslant \pi\), the sine wave is positive and the solution travels forward. For \(\pi &lt; x \leqslant 2\pi\), the sine wave is negative and the solution travels backwards. The animation below shows how the solution of Burgers equation evolves over time, and we see the development of a genuine discontinuity at \(x=\pi\).</p>

<div class="container">
  <div class="row">
    <div class="col">
    </div>
    <div class="col-10">
      <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/conslaw1/burgers.gif-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/conslaw1/burgers.gif-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/conslaw1/burgers.gif-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/conslaw1/burgers.gif" class="img rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

      <div class="caption">
        <b>Figure 1:</b> Solution of Burgers equation with sine initial data.
      </div> 
    </div>
    <div class="col">
    </div>
  </div>
</div>

<p>Shocks such as the one above occur naturally in many settings, for instance high-speed compressible flows and aeroacoustics. However, a natural question we might ask next is how discontinuities and shocks fit with our notion of the classical solution of a PDE. For instance, the linear advection equation contains a first-order time derivative and a first-order spatial derivative. Naturally, we should require that \(u \in C_{x,t}^{1}(\mathbb{R} \times [0,\infty))\) – our solution should be once differentiable in space and time. Consider, though, the linear advection equation with \(u(x,0) = 𝟙_{\\{x \leqslant 0\\}}\). You’ll probably agree with me that the only possible solution to this problem is \(u(x,t) = 𝟙_{\\{x \leqslant t\\}}\), and yet this “solution” is not even continuous let alone differentiable. Before moving on to numerical solvers, we will spend some time developing alternative notions of PDE solutions that are capable of admitting discontinuities and shocks.</p>

<h2 id="i-weak-solutions-of-conservation-laws">I. Weak Solutions of Conservation Laws</h2>
<p><strong>Definition 1.</strong> <em>We call \(u(x,t)\) a</em> <strong>weak solution</strong> <em>of the conservation law \eqref{eq:conslaw} if</em></p>

\[\begin{equation} 
  \int_{0}^{\infty}\int_{-\infty}^{\infty} u\varphi_{t} + f(u)\varphi_{x}\;dxdt = -\int_{-\infty}^{\infty} u(x,0)\varphi(x,0)\;dx
  \label{eq:weaksol}
\end{equation}\]

<p><em>for all \(\varphi \in C_{c}^{\infty}(\mathbb{R} \times \mathbb{R})\).</em></p>

<p>We refer to \(\varphi\) as a test function. It is smooth in both space and time and compactly supported, i.e. \(\varphi\) is zero outside of some compact set in space and time. Let’s break down what’s actually going on in the definition of the weak solution. The most notable feature is that if $u$ is a weak solution, it does <em>not</em> have to be differentiable in space or time since the definition only contains derivatives of \(\varphi\)!</p>

<p>So how do we arrive at this definition? First, we multiply the entire conservation law by \(\varphi\):
\begin{align}
  u_{t}\varphi + f(u)_{x}\varphi = 0.\notag
\end{align}</p>

<p>Then, we integrate in time over $[0,\infty)$ and in space over the entire real line:
\begin{align}
  \int_{0}^{\infty}\int_{-\infty}^{\infty} u_{t}\varphi + f(u)_{x}\varphi\;dxdt = 0.\notag
\end{align}</p>

<p>The last step is to integrate the first term by parts in time and the second term by parts in space:</p>

\[\begin{align}
  \int_{0}^{\infty} \int_{-\infty}^{\infty} u_{t}\varphi dxdt &amp;= -\int_{0}^{\infty} \int_{-\infty}^{\infty} u\varphi_{t}dxdt + \int_{-\infty}^{\infty} u(x,0)\varphi(x,0)dx\notag\\
  \int_{0}^{\infty} \int_{-\infty}^{\infty} f(u)_{x}\varphi\;dxdt &amp;= -\int_{0}^{\infty} \int_{-\infty}^{\infty} f(u)\varphi_{x}\;dxdt
\end{align}\]

<p>Most of the boundary terms vanish because \(\varphi\) has compact support: we have \(\varphi(\pm \infty,t) = \varphi(x,\pm\infty) = 0\). Note that in integrating by parts, we pass the derivatives from \(u\) onto the test function. Altogether, we obtain
\begin{align}
\int_{0}^{\infty}\int_{-\infty}^{\infty} u\varphi_{t} + f(u)\varphi_{x}\;dxdt = -\int_{-\infty}^{\infty} u(x,0)\varphi(x,0)\;dx.\notag
\end{align}</p>

<p>But <em>why</em> do we do this? The idea is that rather than consider pointwise values of \(u(x,t)\), we consider averaged values of \(u(x,t)\) against test functions. If you’ve taken a functional analysis course, you can view the integration against a test function as the evaluation of a distribution induced by \(u_{t}\) and \(f(u)_{x}\). We note that all classical solutions (\(C_{x,t}^{1}\) solutions that satisfy the PDE in a pointwise sense) are weak solutions but the converse is certainly not true.</p>

<p>It is straightforward to verify that \(𝟙_{\\{x\leqslant t\\}}\) is a weak solution of the linear advection equation with \(u(x,0) = 𝟙_{\\{x \leqslant 0\\}}\). However, the definition \eqref{eq:weaksol} of quite cumbersome to work with and we would like to develop a more convenient way to verify whether we have a weak solution or not. Notice that \(u(x,t) = 𝟙_{\\{x\leqslant t\\}}\) as well as the solution of Burgers equation in Figure 1 are piecewise smooth and only discontinuous at a single point that may (in the case of linear advection) or may not (in the case of Burgers equation) change with time. In this case, we can say much more about the structure of the weak solution.</p>

<p><strong>Theorem 1. (Rankine-Hugoniot)</strong> <em>Suppose the solution of \eqref{eq:conslaw} is piecewise smooth and contains a discontinuity along the curve \(x(t)\). Then \(u\) is a weak solution of \eqref{eq:conslaw} if and only if</em>
\begin{equation}
  x’(t) = \frac{f(u^{-}) - f(u^{+})}{u^{-} - u^{+}},
  \label{eq:RH}
\end{equation}</p>

<p><em>where \(x'(t)\) is the speed of the discontinuity and \(u^{\pm}\) are the values of \(u\) along each side of the shock.</em></p>

<p>Condition \eqref{eq:RH} is referred to as the <strong>Rankine-Hugoniot jump condition</strong>.</p>

<p><strong>Example 1.</strong> <em>Let’s return to the linear advection equation and use the jump condition to verify that \(𝟙_{\\{x\leqslant t\\}}\) is a weak solution. We have \(x'(t) = 1\), \(u^{-} = 1\), and \(u^{+} = 0\). Then</em>
\begin{align}
  \frac{f(u^{-}) - f(u^{+})}{u^{-} - u^{+}} = \frac{u^{-} - u^{+}}{u^{-} - u^{+}} = 1\notag
\end{align}
<em>and the jump condition is satisfied.</em></p>

<p>Thus, the Rankine-Hugoniot condition is a handy way for us to verify whether something is a weak solution. It turns out that solutions to conservation laws often have the structure required by Theorem 1 – that is, they are piecewise smooth except along finitely many curves of discontinuity.</p>

<p>By relaxing the definition of the solution, we are able to admit much less smooth solutions to \eqref{eq:conslaw}. However, we will see weak solutions need not be unique! Consider Burgers equation with the initial condition \(u(x,0) = -𝟙_{\\{x\leqslant 0\\}} + 𝟙_{\\{x&gt;0\\}}\). It is straightforward to verify that \(u(x,t) = -𝟙_{\\{x\leqslant 0\\}} + 𝟙_{\\{x&gt;0\\}}\) as well as</p>

\[\begin{align}
u(x,t) = \begin{cases}
  -1 &amp;x \leqslant -t\\
  \frac{x}{t} &amp;-t &lt; x \leqslant t\\
  1 &amp;x &gt; t
  \end{cases}
\end{align}\]

<p>are both weak solutions. The first weak solution does not change at all with time, so our intuition might tell us that this solution does not make much physical sense. But what about the second weak solution? At the very least, we need to establish more stringent criteria for our weak solutions in order to pick out the physically relevant solution.</p>

<h2 id="ii-entropy-solutions-of-conservation-laws">II. Entropy Solutions of Conservation Laws</h2>

<p>Consider the slightly modified PDE given by</p>

\[\begin{equation}
  \begin{cases}
    u^{(\varepsilon)}_{t} + f(u^{(\varepsilon)})_{x} = \varepsilon u^{(\varepsilon)}_{xx} &amp;\text{in}\;\mathbb{R} \times (0,\infty)\\
    u^{(\varepsilon)}(x,0) = u_{0}(x) &amp;\text{on}\;\mathbb{R} \times \{t=0\}.
    \label{eq:viscosityPDE}
  \end{cases}
\end{equation}\]

<p>This PDE is <em>parabolic</em> and it turns out that the solution \(u^{(\varepsilon)}\), if it exists, is smooth and unique.</p>

<p><strong>Definition 2.</strong> <em>The entropy solution of \eqref{eq:conslaw} is defined as</em>
\begin{align}
u(x,t) = \lim_{\varepsilon \to 0} u^{(\varepsilon)}(x,t).\notag
\end{align}
<em>This limit, if it exists, is unique and is a weak solution of  \eqref{eq:conslaw}.</em></p>

<p>Again, this definition is quite difficult to work with, even moreso than the original definition of the weak solution, and we need to establish a more efficient way of verifying whether we have the entropy solution or not. There are alternative definitions based on the notions of <em>entropy flux</em> and <em>entropy flux functions</em>, but we’ll skip right to the chase here. Thanks to the work of Oleinik and Lax, we have a quick way to identify entropy solutions.</p>

<p><strong>Theorem 2. (Oleinik Entropy Condition)</strong> <em>Suppose the solution of \eqref{eq:conslaw} is piecewise smooth and contains a discontinuity along the curve \(x(t)\). Then \(u\) is the entropy solution if and only if</em>
\begin{equation}
  \frac{f(u) - f(u^{-})}{u - u^{-}} \geqslant \frac{f(u^{+}) - f(u^{-})}{u^{+} - u^{-}} \geqslant \frac{f(u^{+}) - f(u)}{u^{+} - u}
  \label{eq:oleinik}
\end{equation}
for all \(u\) between \(u^{-}\) and \(u^{+}\).*</p>

<p>Again, \(u^{-}\) and \(u^{+}\) denote the values of $u$ on each side of the shock \(x(t)\). However, we can simplify \eqref{eq:oleinik} even further if we have a <em>convex</em> conservation law, which is simply the case when \(f(u)\) is convex. For Burgers equation, we have \(f(u) = u^{2}/2\), which is indeed convex. The Lax Entropy condition tells us the following.</p>

<p><strong>Theorem 3. (Lax Entropy Condition)</strong> <em>Suppose the solution of \eqref{eq:conslaw} is piecewise smooth and contains a discontinuity along the curve \(x(t)\). Suppose also that \(f\) is convex. Then \(u\) is the entropy solution if and only if
\begin{equation}
  f’(u^{-}) \geqslant \frac{f(u^{+}) - f(u^{-})}{u^{+} - u^{-}} \geqslant f’(u^{+}).
  \label{eq:lax}
\end{equation}
In particular, \eqref{eq:lax} is equivalent to requiring that \(u^{-} &gt; u^{+}\).</em></p>

<p>The Lax Entropy condition tells us that for convex conservation laws, the value of the weak solution on the left side of the shock ($u^{-}$) must be larger than the value of the solution on the right side of the shock ($u^{+}$) in order for the weak solution to be an entropy solution. The physical intuition behind the entropy solution is related to the entropy of a fluid in gas dynamics: in smooth flows, the entropy remains constant along particle paths, and if the particle crosses a shock, the entropy may only jump to a <em>higher</em> value. Entropy is inversely proportional to density, and so the density $u$ can only jump to lower values along a shock.</p>

<p>We’ve spent a lot of time now developing the main ideas of weak solutions for conservation laws. We can concretely define what it means to have discontinuous solutions of \eqref{eq:conslaw} and impose conditions to guarantee the uniqueness of this solution. Great. We can finally move on to numerical methods for solving conservation laws. In developing such schemes, our goal will be to ensure that our scheme converges (in some sense) to the entropy solution.</p>

<h1 id="2-monotone-schemes"><strong>2. Monotone Schemes</strong></h1>
<p>First, a cautionary tale in constructing numerical methods for conservation laws. We consider Burgers equation with the initial condition \(u(x,0) = 𝟙_{\\{x \geqslant 0\\}}\) and the following finite difference scheme:
\begin{align}
  u_{j}^{n+1} = u_{j}^{n} - \frac{\Delta t}{\Delta x} u_{j}^{n}(u_{j}^{n} - u_{j-1}^{n}).\notag
\end{align}</p>

<p>Initially, we have \(u_{j}^{0} = 𝟙_{\\{x_{j} \geqslant 0\\}}\). However, this scheme returns \(u_{j}^{n} = u_{j}^{0}\) for all \(n\) and $j$, so the scheme converges to \(u(x,t) = u(x,0)\). But in the preceding sections, we established that this can’t even be a weak solution (indeed, the Rankine-Hugoniot condition is not satisfied).</p>

<p>Clearly, we must exercise caution in constructing schemes to solve \eqref{eq:conslaw}. We have just seen that a perfectly reasonable-looking finite difference scheme (at least, at first glance) fails to converge to a weak solution, let alone the correct entropy solution. In other cases, it may be possible that a scheme converges to a weak solution but not an entropy solution. We will circle back to this idea shortly.</p>

<h2 id="i-the-finite-volume-method">I. The Finite Volume Method</h2>
<p>We begin by introducing the <strong>finite volume method</strong>, which discretizes the spatial domain into cells (intervals in 1D, rectangles or other polygons in 2D), and computes an approximation to the average of the solution in each cell. The precise formulation is as follows: consider a bounded, open, connected domain \(\Omega = (a,b) \subset \mathbb{R}\) and a finite stopping time \(T\), and consider the conservation law given by
\(\begin{equation}
  \begin{cases}
    u_{t} + f(u)_{x} = 0 &amp;\text{in}\;\Omega \times (0,T)\\
    u(x,0) = u_{0}(x) &amp;\text{on}\;\Omega \times \{t=0\}.
  \end{cases}
  \label{eq:conslaw2}
\end{equation}\)</p>

<p>We discretize \(\Omega\) into equally-sized cells \(I_{j} = (x_{j-1/ 2}, x_{j+1/ 2})\):
\begin{align}
  a = x_{1/ 2} &lt; x_{3/ 2} &lt; \dots &lt; x_{N+1/ 2} = b, \;\;\;\Delta x := x_{j+1/ 2} - x_{j-1/ 2}.\notag
\end{align}</p>

<p>Next, we integrate \eqref{eq:conslaw2} over each cell \(I_{j}\):</p>

\[\begin{align}
  \int_{x_{j-1/ 2}}^{x_{j+1/ 2}} (u_{t} + f(u)_{x})\;dx &amp;= 0\notag\\
  \Delta x \frac{d\bar{u}_{j}}{dt} + f(u_{j+1/ 2}^{-}) - f(u_{j-1/ 2}^{+}) &amp;= 0\notag\\
  \frac{d\bar{u}_{j}}{dt} + \frac{f(u_{j+1/ 2}) - f(u_{j-1/ 2})}{\Delta x} &amp;= 0\notag
\end{align}\]

<p>Here, \(\bar{u}_{j}\) denotes the cell average of \(u\) in cell \(I_{j}\): \(\bar{u}_{j} := \frac{1}{\Delta x}\int_{I_{j}} u\;dx\). If our goal is to solve for the cell averages \(\bar{u}_{j}\), you might notice that the second term poses a problem as it is formulated in terms of pointwise values of \(u\) at \(x_{j-1/ 2}\) and \(x_{j+1/ 2}\) rather than \(\bar{u}_{j}\). In the first finite volume scheme we consider, we will approximate \(u\) in \(I_{j} = (x_{j-1/ 2}, x_{j+1/ 2})\) by the cell average \(\bar{u}_{j}\) in \(I_{j}\).</p>

<p>This approximation is not so well defined as the points \(x_{j+1/ 2}\), which lie at the intersection between the two cells \(I_{j}\) and \(I_{j+1}\). The finite volume scheme remedies this by introducing a <em>numerical flux function</em> which takes as inputs cell averages and returns an approximation of the flux function at the cell endpoints \(x_{j+1/ 2}\), i.e. \(f(u_{j+1/ 2}) \approx \hat{f}(\bar{u}_{j}, \bar{u}_{j+1})\) and \(f(u_{j-1/ 2}) \approx \hat{f}(\bar{u}_{j-1}, \bar{u}_{j})\):</p>

\[\begin{equation}
  \frac{d\bar{u}_{j}}{dt} + \frac{1}{\Delta x}\left(\hat{f}(\bar{u}_{j}, \bar{u}_{j+1}) - \hat{f}(\bar{u}_{j-1}, \bar{u}_{j})\right) = 0.
  \label{eq:finitevol}
\end{equation}\]

<p>Thus, \(\hat{f}\) can be viewed in this context as a function which takes in the dual values at the cell interfaces and returns a single, physically relevant value. This will be important when we try to develop higher order finite volume schemes later.</p>

<p>For now, the only thing left to do to turn \eqref{eq:finitevol} into a usable scheme is to discretize the time derivative. The simplest option is to use a first-order finite difference. In doing so, we arrive at the explicit first-order finite volume scheme.</p>

<p><strong>Definition 3.</strong> <em>The explicit first-order finite volume scheme in space and time is given by</em>
\(\begin{equation}
  \bar{u}_{j}^{n+1} = \bar{u}_{j}^{n} - \frac{\Delta t}{\Delta x}\left( \hat{f}(\bar{u}_{j}^{n}, \bar{u}_{j+1}^{n}) - \hat{f}(\bar{u}_{j-1}^{n}, \bar{u}_{j}^{n}) \right),
  \label{eq:finitevol2}
\end{equation}\)</p>

<p><em>where \(\bar{u}_{j}^{n}\) denotes the cell averages at time \(t^{n}\) and \(\bar{u}_{j}^{n+1}\) the cell averages at time \(t^{n+1} = t^{n} + \Delta t\).</em></p>

<p>Notice that we haven’t specified what, exactly, the flux function \(\hat{f}\) is yet. The choice of \(\hat{f}\) is immensely important as it will determine many properties of our scheme, such as convergence to the entropy solution. Before stating some possible choices of \(\hat{f}\), we will first cover the properties \(\hat{f}\) must satisfy in order to converge to the entropy solution.</p>

<p><strong>Definition 4.</strong> <em>A</em> <strong>monotone scheme</strong> <em>is one that can be written in the form</em>
\(\begin{equation}
  \bar{u}_{j}^{n+1} = \mathcal{H}(\bar{u}_{j-p}^{n}, \dots, \bar{u}_{j+q}^{n}),
\end{equation}\)</p>

<p><em>where \(\mathcal{H}\) is a nondecreasing function in all arguments. In particular, the monotone scheme on the three-point stencil \(\{\bar{u}_{j-1}^{n}, \bar{u}_{j}^{n}, \bar{u}_{j+1}^{n}\}\) is given by \(\bar{u}_{j}^{n+1} = \mathcal{H}(\bar{u}_{j-1}^{n}, \bar{u}_{j}^{n}, \bar{u}_{j+1}^{n})\).</em></p>

<p>This is all well and good, but why should we care about monotone schemes? To begin with, they have a number of remarkable properties that guarantee our numerical solution behaves “nicely.”</p>

<p><strong>Theorem 4.</strong> <em>Monotone schemes satisfy the following properties:</em></p>

<ol>
  <li>
    <p><strong>Local maximum principle.</strong>
\(\begin{align}
  \min_{j-p \leqslant i \leqslant j+q} \bar{u}_{i}^{n} \leqslant \mathcal{H}(\bar{u})_{j} \leqslant \max_{j-p \leqslant i \leqslant j+q} \bar{u}_{i}^{n} \;\;\;\forall j.\notag
\end{align}\)</p>
  </li>
  <li>
    <p><strong>Total variation diminishing property.</strong>
\(\begin{align}
  ||\mathcal{H}(\bar{u})||_{BV} \leqslant ||\bar{u}||_{BV},\notag
\end{align}\)
<em>where the bounded variation seminorm is given by \(||u||_{BV} = \sum_{j} |u_{j} - u_{j-1}|\).</em></p>
  </li>
  <li>
    <p><strong>Entropy solution.</strong> <em>Monotone schemes converge to the entropy solution with a rate in \(\ell^{1}\) of half-order. This bound is sharp.</em></p>
  </li>
</ol>

<p>Roughly speaking, the local maximum principle tells us that our cell averages at \(t^{n+1}\) cannot be larger or smaller than our averages at \(t^{n}\) around the stencil \(\{\bar{u}_{j-p}^{n}, \dots, \bar{u}_{j+q}^{n}\}\). The total variation diminishing property tells us that our cell averages can only decrease in bounded variation. In particular, this means that our cell averages at \(t^{n+1}\) cannot be more oscillatory than our cell averages at \(t^{n}\). We will see soon some schemes which violate this property and tend to produce spurious oscillations in the numerical solution. The last property is of course self-explanatory: by using a monotone scheme, we are guaranteed to converge to the entropy solution!</p>

<p>We would like for the finite volume scheme in \eqref{eq:finitevol2} to be a monotone scheme, and our goal now is to choose a function \(\hat{f}\) in order to achieve this. We have</p>

\[\begin{align}
  \mathcal{H}(\bar{u}_{j-1}^{n}, \bar{u}_{j}^{n}, \bar{u}_{j+1}^{n}) = \bar{u}_{j}^{n} - \frac{\Delta t}{\Delta x}\left( \hat{f}(\bar{u}_{j}^{n}, \bar{u}_{j+1}^{n}) - \hat{f}(\bar{u}_{j-1}^{n}, \bar{u}_{j}^{n}) \right).\notag
\end{align}\]

<p>\(\mathcal{H}\) must be nondecreasing in each argument, so we compute its partial derivatives:
\(\begin{align}
  \mathcal{H}_{1} &amp;= \frac{\Delta t}{\Delta x} \hat{f}_{1}(\bar{u}_{j-1}^{n}, \bar{u}_{j}^{n})\label{eq:monotone1}\\
  \mathcal{H}_{2} &amp;= 1 - \frac{\Delta t}{\Delta x} \left( \hat{f}_{1}(\bar{u}_{j}^{n}, \bar{u}_{j+1}^{n}) - \hat{f}_{2}(\bar{u}_{j-1}^{n}, \bar{u}_{j}^{n}) \right)\label{eq:monotone2}\\
  \mathcal{H}_{3} &amp;= -\frac{\Delta t}{\Delta x} \hat{f}_{2}(\bar{u}_{j}^{n}, \bar{u}_{j+1}^{n})\label{eq:monotone3}.
\end{align}\)</p>

<p>Here, \(\hat{f}_{1}\) denotes the partial derivative of \(\hat{f}\) with respect to the first argument, and so on. If \(\hat{f}\) is increasing in the first argument and decreasing in the second argument, then \eqref{eq:monotone1} and \eqref{eq:monotone3} will be nonnegative. Furthermore, if</p>

\[\begin{align}
  \frac{\Delta t}{\Delta x} \left( \hat{f}_{1}(\bar{u}_{j}^{n}, \bar{u}_{j+1}^{n}) - \hat{f}_{2}(\bar{u}_{j-1}^{n}, \bar{u}_{j}^{n}) \right) \leqslant 1,\notag
\end{align}\]

<p>then \eqref{eq:monotone2} will also be nonnegative. This last condition is merely a restriction on our time step. By choosing \(\Delta t\) small enough, we can always satisfy this condition. The first two conditions tell us that we merely need to choose our numerical flux function \(\hat{f}\) such that it is increasing in its first argument and decreasing in its second argument.</p>

<p><strong>Proposition 1.</strong> <em>If \(\hat{f}\) satifies the following conditions:</em></p>

<ol>
  <li>
    <p><em>\(\hat{f}\) is Lipschitz continuous in all arguments.</em></p>
  </li>
  <li>
    <p><em>\(\hat{f}(u,u) = f(u)\). This is known as the consistency condition.</em></p>
  </li>
  <li>
    <p><em>\(\hat{f}\) is increasing in the first argument and decreasing in the second argument,</em></p>
  </li>
</ol>

<p><em>then the scheme \eqref{eq:finitevol2} is monotone.</em></p>

<p>We will not discuss the first two conditions here, but suffice it to say that these properties are very easy to check.</p>

<h3 id="i-numerical-flux-functions">i. Numerical Flux Functions</h3>
<p>We will briefly state some common choices of \(\hat{f}\) that satisfy the conditions of Proposition 1 and lead to monotone schemes.</p>

<p><strong>Lax-Friedrichs flux.</strong></p>

\[\begin{align}
  \hat{f}(\bar{u}_{j}, \bar{u}_{j+1}) = \frac{1}{2}\left( f(u_{j}) + f(u_{j+1}) - \alpha(u_{j+1} - u_{j}) \right),\notag\\
\end{align}\]

<p>where 
\(\alpha = \max_{u} |f'(u)|\). We can take \(\alpha = \max_{[\bar{u}_{j}, \bar{u}_{j+1}]} |f'(u)|\).</p>

<p><strong>Godunov flux.</strong></p>

\[\begin{align}
  \hat{f}(\bar{u}_{j}, \bar{u}_{j+1}) = \begin{cases}
    \min_{\bar{u}_{j} \leqslant u \leqslant \bar{u}_{j+1}} f(u), &amp;\bar{u}_{j} \leqslant \bar{u}_{j+1}\\
    \max_{\bar{u}_{j} \geqslant u \geqslant \bar{u}_{j+1}} f(u), &amp;\bar{u}_{j} &gt; \bar{u}_{j+1}.
  \end{cases}\notag
\end{align}\]

<p><strong>Engquist-Osher flux.</strong></p>

\[\begin{align}
  \hat{f}(\bar{u}_{j}, \bar{u}_{j+1}) = &amp;\int_{0}^{\bar{u}_{j}} \max\{f'(u),0\}\;du + \int_{0}^{\bar{u}_{j+1}} \min\{f'(u),0\}\;du.\notag
\end{align}\]

<p>There are two more popular choices of flux that do not lead to monotone schemes but are nevertheless still popular.</p>

<p><strong>Roe flux.</strong></p>

\[\begin{align}
  \hat{f}(\bar{u}_{j}, \bar{u}_{j+1}) = \begin{cases}
    f(\bar{u}_{j}), &amp;\frac{f(\bar{u}_{j+1}) - f(\bar{u}_{j})}{\bar{u}_{j+1} - \bar{u}_{j}} \geqslant 0\\
    f(\bar{u}_{j+1}), &amp;\frac{f(\bar{u}_{j+1}) - f(\bar{u}_{j})}{\bar{u}_{j+1} - \bar{u}_{j}} &lt; 0.
  \end{cases}\notag
\end{align}\]

<p><strong>Lax-Wendroff flux.</strong></p>

\[\begin{align}
  \hat{f}(\bar{u}_{j}, \bar{u}_{j+1}) = &amp;\frac{1}{2}(f(\bar{u}_{j}) + f(\bar{u}_{j+1})) -\notag\\
  &amp;\frac{\Delta t}{2\Delta x}f'\left( \frac{\bar{u}_{j} + \bar{u}_{j+1}}{2}\right) \left( f(\bar{u}_{j+1}) - f(\bar{u}_{j}) \right)\notag
\end{align}\]

<h2 id="ii-implementation">II. Implementation</h2>
<p>Without further ado, we will implement the first-order finite volume method using all five of the aforementioned numerical fluxes for Burgers equation with \(u(x,0) = \sin{x}\). All of the source code is publicly available on <a href="https://github.com/jdongg/numCL">GitHub</a> in both a MATLAB and Python implementation. For convenience, we will explain the Python implementation here. First, we initialize the parameters of the scheme.</p>

<d-code block="" language="python">
# specify domain
a = 0
b = 2*np.pi

# initial condition: u(x,0) = alpha + beta*sin(x)
alpha = 0.0
beta  = 1.0

# number of grid points in spatial discretization
N  = 80

# setup grid points
x = np.linspace(a,b,N)     
dx = (b-a)/(N-1);  

# stopping time
T = 1.5
</d-code>

<p>Next, we compute the cell averages of the initial condition, \(\bar{u}_{j}^{0}\). This is accomplished by integrating the initial condiiton over each cell \(I_{j}\):</p>
<d-code block="" language="python">
# setup array to store cell averages; due to periodicity, we omit the last cell
u = np.zeros((len(x)-1,1)); 

# returns the initial condition of the PDE
def initial_condition(z, alpha, beta):
    return alpha + beta*np.sin(z)

# compute cell averages at t=0
for i in range(0,N-1):
    u[i] = (1.0/dx)*integrate.quad(initial_condition, x[i], x[i+1], args=(alpha,beta))[0]
</d-code>

<p>Next, we must choose an appropriate time step for the scheme. Since we are using an explicit time-stepping scheme, we must choose an appropriately small time step (recall that explicit time-stepping is conditionally stable). For the purposes of example, we choose</p>

\[\frac{\Delta t}{\Delta x} \max_{u} |f'(u)| = \frac{1}{2}.\]

<p>The last step is to perform the time-stepping, whereby for each time step we must compute the numerical fluxes \(\hat{f}(\bar{u}_{j}^{n}, \bar{u}_{j+1}^{n})\) and \(\hat{f}(\bar{u}_{j-1}^{n}, \bar{u}_{j}^{n})\):</p>
<d-code block="" language="python">
t = 0.0
while t &lt; T:
    # alpha for the Lax-Friedrichs flux
    A  = np.amax(np.amax(u));

    # compute numerical fluxes fhat_{j+1/2}
    um = u
    up = np.roll(u,-1)
    fR = lf_flux(um, up, A)

    # compute numerical fluxes fhat_{j-1/2} (assuming periodic BCs)
    fL = np.roll(fR,1)

    # first order explicit time-stepping
    u -= dt/dx*(fR - fL)

    # increment time step
    t = t+dt
</d-code>

<p>Each of the numerical fluxes in Section 2.1.1 have been implemented and can be viewed on <a href="https://github.com/jdongg/numCL">GitHub</a>. Figure 2 shows the finite volume solutions at \(T=1.5\).</p>

<div class="container">
  <div class="row">
    <div class="col">
    </div>
    <div class="col-15">
      <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/conslaw1/finiteVolumeBurgers-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/conslaw1/finiteVolumeBurgers-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/conslaw1/finiteVolumeBurgers-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/conslaw1/finiteVolumeBurgers.png" class="img rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

      <div class="caption">
        <b>Figure 2:</b> Solution of Burgers equation with sine initial data using the first-order finite volume method.
      </div> 
    </div>
    <div class="col">
    </div>
  </div>
</div>

<p>The first observation we make is that there is little difference between the Roe, Godunov, and Engquist-Osher fluxes, at least for this example problem. The Godunov and Engquist-Osher fluxes lead to monotone schemes which converge to the entropy solution. The Roe flux does not lead to a monotone scheme, and in very rare cases may provide solutions with the incorrect shock speed. Nevertheless, in most cases the Roe scheme works well and is a popular choice for the numerical flux.</p>

<p>Next, the Lax-Friedrichs flux is noticeably more dissipative than any of the other results. It is known to “smear” discontinuities and add numerical diffusion. However, the Lax-Friedrichs flux is a very popular choice for the numerical flux since it leads to a monotone scheme and is straightforward to implement.</p>

<p>Lastly, the Lax-Wendroff flux is…problematic, to say the least. Notice that the numerical solution develops spurious (nonphysical) oscillations in the vicinity of the shock at \(x=\pi\). If the Lax-Wendroff scheme converges, it converges to a weak solution, but the scheme is not monotone and does not exhibit the total variation-diminishing (TVD) property. Lax-Wendroff is actually a <em>second-order method</em> in disguise, but we’ll discuss this much later and how the order of a numerical method is related to the TVD property for conservation laws.</p>

<h2 id="3-key-takeaways"><strong>3. Key Takeaways</strong></h2>
<p>If I haven’t bored you to death by this point, you should have a basic understanding of weak solutions for conservation laws and the mathematical foundations for discontinuous solutions. The study of such solutions is important because they occur frequently in fluid dynamics and other related fields. You should also be able to implement a basic first-order finite volume scheme for a variety of one-dimensional conservation laws.</p>

<p>We’ve only covered first-order schemes in this post, and you might have already guessed that these methods converge too slowly to be of much use for practical problems. Next time, we’ll cover higher order finite volume methods for conservation laws and touch on some of the theory regarding higher-order methods (remember how I said the Lax-Wendroff flux actually yields a second-order method? We’ll circle back to that soon). Unfortunately, there’s no free lunch and higher order methods come with their own set of issues.</p>

<p>Below are two texts by Randy LeVeque which are helpful in fleshing out much of the details of the Theorems given in this post. I’ve found them to be immensely helpful in my Ph.D. studies.</p>

<h2 id="4-references"><strong>4. References</strong></h2>
<p>[1] LeVeque, Randall J. <a href="http://staff.washington.edu/rjl/book2/sample.pdf">Finite volume methods for hyperbolic problems</a>. Vol. 31. Cambridge university press, 2002.</p>

<p>[2] LeVeque, Randall J. <a href="https://pdfs.semanticscholar.org/1470/c6f43c769572c4cfc94ffc9c5710484ff1e5.pdf">Numerical methods for conservation laws</a>. Vol. 132. Basel: Birkhäuser, 1992.</p>

<h2 id="5-codes"><strong>5. Codes</strong></h2>
<p><a href="https://github.com/jdongg/numCL">NumCL repository on GitHub</a></p>]]></content><author><name>Justin Dong</name></author><category term="numerical-methods" /><category term="finite-volume-method," /><category term="hyperbolic-conservation-laws" /><summary type="html"><![CDATA[1. Conservation Laws]]></summary></entry></feed>